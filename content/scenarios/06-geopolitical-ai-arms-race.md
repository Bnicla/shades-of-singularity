---
number: 6
title: "The Geopolitical AI Arms Race"
slug: "geopolitical-ai-arms-race"
tier: 2
tierLabel: "Highly Probable"
likelihood: "~80%"
unmanaged: -4
governed: 1
dividend: 5
summary: "The competition that drives AI advancement is the same competition that makes coordinated safety governance nearly impossible."
---

The arms race is no longer theoretical. It is being fought with real weapons on a real battlefield. In Ukraine, AI-enabled drones have transformed the war into a laboratory for autonomous combat. A 2025 CSIS report found that drones equipped with autonomous terminal guidance raised target engagement success rates from 10-20% to 70-80% by removing the need for constant manual control and stable communications. Ukraine aimed to equip half its drone production with AI guidance in 2025, up from 0.5% in 2024, representing roughly a million AI-assisted drones ([Bondar, CSIS/Breaking Defense, 2025](https://breakingdefense.com/2025/03/trained-on-classified-battlefield-data-ai-multiplies-effectiveness-of-ukraines-drones-report/)). By summer 2025, Ukraine had employed drone swarming technology in over 100 operations, with groups of 8-25 coordinating strikes autonomously ([Ukraine's Arms Monitor, 2025](https://ukrainesarmsmonitor.substack.com/p/drone-warfare-in-ukraine-the-interplay)). Russia matched the escalation: by mid-2025, it was producing approximately 1.5 million FPV drones annually, and its largest single strike involved 818 drones and missiles. Both sides are racing toward the point where AI selects its own targets. In June 2025, Ukrainian forces launched what they claim was the first confirmed assault operation entirely carried out by unmanned platforms, with AI-driven drones autonomously scanning for and engaging targets without human piloting.

The superpower competition sits behind this battlefield. According to Stanford's Human-Centered Artificial Intelligence Institute, US private AI investment reached $109.1 billion in 2024, nearly twelve times China's $9.3 billion. But China plans to deploy $98 billion in AI investment in 2025, including $56 billion from government sources ([Stanford HAI/Modern Diplomacy, 2025](https://moderndiplomacy.eu/2025/10/06/great-power-competition-in-ai-led-driven-warfare-between-the-us-and-china/)). China's state-owned defense giant Norinco unveiled a military vehicle powered by DeepSeek in February 2025, and a Reuters review of hundreds of research papers, patents, and procurement records documented the systematic effort to harness AI for military advantage ([Reuters/Calcalist, 2025](https://www.calcalistech.com/ctechnews/article/sktsoohcex)). The most dangerous dimension remains nuclear. In November 2024, Biden and Xi jointly affirmed the need to maintain human control over decisions to use nuclear weapons, the first time the US and China made this statement together. National Security Adviser Jake Sullivan described it as addressing a "long-term strategic risk" of two significant nuclear powers being unable to reach agreement on anything in the AI-nuclear space ([White House Press Briefing, November 2024](https://bidenwhitehouse.archives.gov/briefing-room/press-briefings/2024/11/17/on-the-record-press-gaggle-by-apnsa-jake-sullivan-on-president-bidens-meeting-with-president-xi-jinping/)). Two months later, China declined to sign a multilateral declaration at the REAIM conference endorsing the same principle it had just agreed to bilaterally, illustrating how rivalry undermines even minimal commitments ([South China Morning Post, September 2024](https://www.scmp.com/news/china/diplomacy/article/3279368/why-us-china-rivalry-impedes-global-efforts-regulate-artificial-intelligence)).

The US-China framing, however, obscures a larger structural problem: most of the world has no seat in this race and enormous stakes in its outcome. The India AI Impact Summit in February 2026, the first major AI summit hosted in the Global South, made that gap visible. Over 100 countries sent delegations. Prime Minister Modi framed the central question as developmental: AI must serve humanity in all its diversity, and any model that succeeds in India can be deployed globally ([PM India, 2026](https://www.pmindia.gov.in/en/news_updates/pm-inaugurates-india-ai-impact-summit-2026/)). As NBC News reported, the summit's pitch was that the future of AI should not be written only in Washington and Beijing ([NBC News](https://www.nbcnews.com/world/asia/indias-ai-summit-draws-global-leaders-big-pledges-chaos-rcna259855)). India is testing what the Observer Research Foundation calls a "third way" in AI governance: strategic autonomy without isolation, remaining engaged with multiple power centers while amplifying voices from Africa, Southeast Asia, and Latin America ([ORF](https://www.orfonline.org/research/the-ai-impact-summit-2026-shifting-the-global-discourse)).

India's position in this landscape is structurally unique. The country accounts for 16% of the world's AI workforce and leads globally in AI skill penetration at 2.8 times the world average ([India Skills Report 2026](https://indiaai.gov.in/article/india-leads-global-ai-talent-and-skill-penetration); [Stanford HAI](https://indiaai.gov.in/news/india-tops-global-ai-skill-penetration-and-talent-concentration-rates)). Indian engineers contributed 19% of GitHub AI projects in 2023, second only to the United States at 22.9%. Over 1,700 Global Capability Centers employ 1.9 million people, and more than 60% of GCCs established in the last two years focus on AI, data, and product development ([Carnegie Endowment](https://carnegieendowment.org/research/2025/02/the-missing-pieces-in-indias-ai-puzzle-talent-data-and-randd?lang=en)). Carnegie also flags the structural tension: top-tier AI research talent trained at Indian institutions ends up working in the US and Europe. The CEOs running Microsoft, Alphabet, and IBM were all trained in India's engineering pipeline. The workforce building the models, the executives directing the companies building them, and the largest potential deployment market all trace back to the same talent ecosystem. This gives India leverage that pure compute investment figures understate. Yet the infrastructure gap remains real. Africa accounts for less than 1% of global data center capacity despite being home to 18% of the world's population. India itself would need to nearly double its compute capacity to meet domestic demand ([CSIS](https://www.csis.org/analysis/divide-delivery-how-ai-can-serve-global-south)). The risk is that the arms race between Washington and Beijing produces parallel AI ecosystems, one relatively open, the other centralized and surveillance-driven, while the Global South becomes a market for both and a designer of neither.

The arms race poisons coordinated safety governance through a straightforward mechanism: neither side will slow development for fear the other gains advantage. When the UN General Assembly's First Committee passed a historic resolution in November 2025 calling for a legally binding agreement on lethal autonomous weapons systems, 156 nations voted in favor. The United States and Russia were among five that opposed it ([Usanas Foundation, 2026](https://usanasfoundation.com/regulating-lethal-autonomous-weapons-systems-laws-in-a-fractured-multipolar-order)). The leading military powers have concluded that strategic advantage outweighs legal or ethical constraints. This is a prisoner's dilemma at civilizational scale: mutual restraint produces the best collective outcome, but each actor's dominant strategy is to defect.

The honest counterargument is that arms control has worked before. The Nuclear Non-Proliferation Treaty, the Chemical Weapons Convention, and the various strategic arms limitation agreements all constrained powerful technologies despite intense geopolitical competition. As former NSA Jake Sullivan argued in January 2026, there are meaningful parallels to the decades-long process of nuclear arms control, which produced export controls, verification protocols, and guardrails even at the height of the Cold War ([Sullivan, CHINA US Focus, 2026](https://www.chinausfocus.com/peace-security/china-and-america-must-get-serious-about-ai-risk)). Sullivan himself identified why the analogy breaks down. Verification is harder: you can count missiles and warheads, which have detectable signatures, but counting algorithms or discerning all the capabilities of a given model is a different problem entirely. The dual-use challenge is more severe: there is a relatively clear line between peaceful nuclear power and nuclear weapons, while the same AI model can power a medical diagnosis system and an autonomous weapons targeting system. And the uncertainty about capability trajectories has no nuclear equivalent. The evolution and impact of AI capabilities is far less predictable than the physics of nuclear weapons. Arms control frameworks took decades to develop. AI capabilities are advancing on timescales of months. The governed outcome is modest (+1) because the structural incentives favor escalation and the verification problem may be genuinely unsolvable with current tools. What governance can realistically achieve is a floor: bilateral hotlines to prevent AI-triggered escalation (modeled on Cold War nuclear communication channels), multilateral compute tracking regimes that make large training runs visible, export controls on the most dangerous capabilities, and binding commitments to maintain human control over nuclear launch decisions. The India AI Summit's "Delhi Declaration," with at least 70 signatories, represents an early attempt at consensus, even if its language remains aspirational ([TIME](https://time.com/7379949/india-ai-impact-summit-us-china-middle-powers/)). None of this prevents the arms race. It manages the risk that the arms race produces an accidental catastrophe. The +1 reflects the gap between what governance can do and what the problem requires.

**Key tension:** Mutual restraint produces the best collective outcome, but each actor's dominant strategy is to escalate, and the verification mechanisms that made nuclear arms control possible do not obviously translate to AI.
