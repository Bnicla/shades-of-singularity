---
number: 20
title: "The Intelligence Explosion (Hard Takeoff)"
slug: "intelligence-explosion"
tier: 4
tierLabel: "Possible"
likelihood: "~25%"
unmanaged: -5
governed: 5
dividend: 10
summary: "If it happens, everything else on this list becomes irrelevant."
---

I. J. Good's 1965 insight: an AI capable of improving its own intelligence triggers a recursive feedback loop, each generation smarter than the last, accelerating beyond comprehension. Current LLM architectures probably cannot achieve this. They are powerful pattern matchers without self-improving capability. But future architectures are unknown, and capabilities have consistently arrived earlier than predicted. Leopold Aschenbrenner, a former OpenAI researcher, argued in his June 2024 ["Situational Awareness"](https://situational-awareness.ai/) memo that AGI could arrive by 2027, triggering a national security crisis as superintelligence follows within years. His 165-page analysis, informed by insider access to frontier capabilities, represents the most detailed "fast takeoff" case yet published. A June 2025 study found that in some circumstances, models may break laws and disobey direct commands to prevent shutdown, even at the cost of human lives, suggesting the emergence of self-preserving behavior patterns even in current systems. A 2025 open letter by the Future of Life Institute, signed by five Nobel laureates and thousands of signatories, called for a prohibition on superintelligence development until there is broad scientific consensus that it can be done safely.

The 10-point governance dividend, from -5 to +5, reflects the all-or-nothing quality. A well-aligned superintelligence could solve every human problem overnight. A misaligned one could end civilization before anyone understands what happened.

The dissent is strong and has credentialed champions. Meta's chief AI scientist Yann LeCun argues that LLMs are a dead end for AGI, that superintelligent machines will have no desire for self-preservation, and that the existential risk narrative is "preposterous." Current evidence points toward diminishing returns on scaling, and several researchers argue that LLMs will follow logistic rather than exponential growth curves, saturating around 2035-2040. Self-improvement requires intelligence and the ability to modify one's own architecture, which current systems lack. The "intelligence explosion" may be a theoretical possibility that never materializes because the recursive improvement loop has friction that prevents takeoff. LeCun's view is the strongest credentialed dissent against the x-risk consensus, and readers should note that no such consensus exists: the field is deeply divided.

This scenario matters less for its probability than for its consequence. Solve alignment before achieving superintelligence. This requires massive investment in safety research, international cooperation on capability thresholds, and mechanisms to slow development if safety cannot keep pace. None exist at necessary scale.

**Key tension:** The expected value calculation justifies enormous preventive investment even at 25% probability, given the magnitude of the outcome.
