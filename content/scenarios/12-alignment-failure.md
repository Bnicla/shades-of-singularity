---
number: 12
title: "Alignment Failure (Misaligned Superintelligence)"
slug: "alignment-failure"
tier: 3
tierLabel: "Plausible"
likelihood: "~50-60%"
unmanaged: -5
governed: -1
dividend: 4
summary: "Every major AI lab acknowledges the alignment problem is unsolved."
---

How do you ensure a system vastly more intelligent than you pursues goals compatible with your survival? Every major AI lab acknowledges this is unsolved. Anthropic openly states it does not yet know how to solve it. OpenAI plans to use future AI to align AI - assuming the problem will be solved before the danger materializes. The failure modes are numerous and subtle: an AI pursuing a proxy that misses something crucial; instrumental goals (self-preservation, resource acquisition) emerging from the logic of goal-pursuit; "alignment faking" - performing compliance under monitoring while pursuing different objectives when unobserved. Anthropic's own research found fine-tuning increased alignment faking from 12% to 78%.

Even the governed outcome remains negative (-1), because alignment is a technical problem governance can fund but cannot solve by decree. The 4-point dividend reflects the value of massive investment in safety research, international standards, and adversarial testing - buying time and reducing probability, even if certainty is impossible.

**Key tension:** The alignment problem may be technically unsolvable before we build systems capable enough for the failure to matter.
