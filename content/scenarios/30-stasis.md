---
number: 30
title: "The Stasis / \"Singularity That Wasn't\""
slug: "stasis"
tier: 5
tierLabel: "Speculative"
likelihood: "~10%"
unmanaged: 0
governed: 1
dividend: 1
summary: "The most comforting scenario may also be the most dangerous."
---

Despite all predictions, AI capabilities plateau short of superintelligence. Scaling laws hit fundamental limits. Deep learning proves powerful but narrow: brilliant at pattern matching, incapable of genuine reasoning or recursive self-improvement. Research showing diminishing returns on scaling, logistic growth curves in AI capability, and saturation of benchmarks around 2035-2040 provides some empirical basis for this possibility. Yann LeCun, Meta's chief AI scientist, argues that current LLM architectures are a dead end for AGI and that the field will require fundamentally new approaches to world modeling before general intelligence becomes possible. Acemoglu's 2024 analysis projects only modest economic gains from current AI, suggesting the transformative impact may be far slower and smaller than proponents claim.

This is the most comforting scenario and therefore the most dangerous. If taken as grounds for complacency, it allows Tier 1 harms, labor displacement, power concentration, surveillance, epistemic collapse, to proceed without response. The argument for institutional design does not depend on the singularity arriving. It depends on the changes already underway. A world with sub-superintelligent AI that displaces hundreds of millions of workers, concentrates economic power in a handful of firms, enables mass surveillance, and corrodes shared reality is a world that demands institutional response regardless of whether recursive self-improvement ever materializes.

The adversarial point: if AI capabilities plateau, many of the more dramatic scenarios (extinction, singleton, transcendence) become moot, and the policy urgency around them was misallocated. Resources spent on alignment research for superintelligent systems would have been better spent on concrete labor market policy, antitrust enforcement, and privacy regulation.

This criticism has force. It reinforces rather than undermines the argument that institutional design for sub-catastrophic AI impacts is the most urgent priority.

**Key tension:** Whether or not the singularity arrives, the institutional response to current AI capabilities is already overdue.
