---
number: 23
title: "AI-Enabled Bioweapons / Catastrophic Misuse"
slug: "ai-enabled-bioweapons"
tier: 4
tierLabel: "Possible"
likelihood: "~25%"
unmanaged: -5
governed: -2
dividend: 3
summary: "The same openness that drives AI innovation distributes dangerous capabilities."
---

AI lowers the expertise threshold for mass destruction. RAND's 2024 red-team study found that current-generation LLMs did not measurably increase the operational risk of a biological weapons attack: plans generated with LLM assistance were no more viable than those generated without it. This is reassuring about today's models but says little about tomorrow's ([RAND, 2024](https://www.rand.org/pubs/research_reports/RRA2977-2.html)). RAND's 2025 Global Risk Index for AI-enabled biological tools assessed 57 state-of-the-art tools and found 13 indexed as high priority, with one demonstrating critical-level misuse capabilities. All categories showed substantial room for capability growth ([RAND, 2025](https://www.rand.org/randeurope/research/projects/2024/ai-risk-index.html)). A separate 2025 RAND benchmark evaluation found that frontier reasoning models like OpenAI's o3 rank in the 94th percentile among expert human virologists on virology capability tests ([RAND, 2025](https://www.rand.org/pubs/commentary/2025/08/dissecting-americas-ai-action-plan-a-primer-for-biosecurity.html)).

Knowledge is necessary for bioweapons, but far from sufficient. The practical barriers, including acquiring pathogens, culturing agents, weaponizing and dispersing them, remain substantial and are primarily operational, not informational. The Aum Shinrikyo cult had significant resources and scientific expertise and still failed to deploy an effective biological weapon. The bottleneck is operational capability, and AI does not provide that.

This is correct for current systems. The risk shifts when AI moves from providing information to providing laboratory automation and experimental design. RAND's work on cloud laboratories highlights that remote automation capabilities may introduce misuse pathways at the biosecurity-cyberbiosecurity nexus. Suleyman's "containment problem" from *The Coming Wave* (2023) applies with particular force here: once biological design tools are widely accessible, restricting their misuse becomes progressively harder. The governance dividend is modest (3 points) because even effective governance can only reduce probability. As with nuclear proliferation, perfect security is impossible.

**Key tension:** Current AI does not meaningfully enable bioweapons. Future AI, especially combined with laboratory automation, may cross the threshold where informational uplift becomes operationally significant.
