---
number: 28
title: "Human Extinction"
slug: "human-extinction"
tier: 5
tierLabel: "Speculative"
likelihood: "~10%"
unmanaged: -5
governed: -5
dividend: "Prevention only"
summary: "The expected value calculation demands massive preventive investment regardless of precise probability estimates."
---

The AI Impacts survey of AI researchers (2023, updated 2024) found a median prediction of 5% probability (mean 9%) for "extremely bad outcomes" such as human extinction. Over a third of participants (38%) assigned at least 10% probability. The International Institute for Management Development's AI Safety Clock stood at 20 minutes to midnight as of September 2025, down from 29 minutes at its launch in 2024 ([AI Impacts, 2024](https://aiimpacts.org/wp-content/uploads/2023/04/Thousands_of_AI_authors_on_the_future_of_AI.pdf)). Bengio, Hinton, Russell, and co-authors argued in Science (May 2024) that the rapid pace of AI development requires urgent attention to extreme risks, including scenarios where AI systems pursue goals misaligned with human survival. A 2025 open letter by the Future of Life Institute, signed by five Nobel laureates, called for a prohibition on superintelligence development until safety can be assured.

These numbers deserve skepticism. Scientific American and IEEE Spectrum have both questioned the AI Impacts survey methodology, noting the survey was funded by organizations connected to the effective altruism movement and that its framing inherently promotes the existential risk interpretation. Tom Dietterich of Oregon State University criticized the survey for asking "how much should we worry?" rather than conducting careful risk analysis. The survey population, drawn from machine learning conference attendees, may overrepresent those engaged with AI risk discourse ([Scientific American, 2024](https://www.scientificamerican.com/article/ai-survey-exaggerates-apocalyptic-risks/); [IEEE Spectrum, 2024](https://spectrum.ieee.org/ai-existential-risk-survey)).

A 2025 study in the International Journal of Forecasting using a structured persuasion-based tournament methodology (the Existential Risk Persuasion Tournament) found that AI specialists expressed greater concern about catastrophic risk than generalist superforecasters, and that the two groups did not converge after structured debate. This persistent disagreement among informed experts suggests genuine uncertainty rather than a resolved question.

The governance dividend is technically zero: if extinction occurs, governance failed. The preventive dividend is enormous. Even a 5% reduction in a 10% extinction risk justifies investments in the trillions. Whether the probability is 5% or 0.5%, the expected value math favors substantial precautionary investment.

**Key tension:** Expert disagreement on probability reflects genuine uncertainty. The expected value argument for precaution holds across a wide range of probability estimates.
