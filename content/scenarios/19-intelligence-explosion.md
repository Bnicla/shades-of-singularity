---
number: 19
title: "The Intelligence Explosion (Hard Takeoff)"
slug: "intelligence-explosion"
tier: 4
tierLabel: "Possible"
likelihood: "~25%"
unmanaged: -5
governed: 5
dividend: 10
summary: "If it happens, everything else on this list becomes irrelevant, for good or ill."
---

I. J. Good's 1965 insight: an AI capable of improving its own intelligence triggers a recursive feedback loop, each generation smarter than the last, accelerating beyond comprehension. The timeline could be years, months, or hours. Current LLM architectures probably cannot achieve this, but future architectures are unknown, and capabilities have consistently arrived earlier than predicted. A well-aligned superintelligence could solve every human problem overnight. A misaligned one could end civilization before anyone understands what happened.

The 10-point governance dividend - from -5 to +5 - reflects this all-or-nothing quality. Solve alignment before achieving superintelligence. This requires massive investment in safety research, international cooperation on capability thresholds, and mechanisms to slow development if safety cannot keep pace. None exist at necessary scale.

**Key tension:** If it happens, everything else on this list becomes irrelevant - for good or ill.
