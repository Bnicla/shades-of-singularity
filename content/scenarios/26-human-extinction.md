---
number: 26
title: "Human Extinction"
slug: "human-extinction"
tier: 5
tierLabel: "Speculative"
likelihood: "~5-15%"
unmanaged: -5
governed: -5
dividend: "Prevention only"
summary: "The expected value calculation demands massive preventive investment regardless of precise probability estimates."
---

Toby Ord estimates AI as the single greatest existential risk, exceeding all others combined. The 2022 survey of AI researchers found a majority believing there's at least a 10% chance of existential catastrophe. Mechanisms include misaligned superintelligence; AI-enabled superweapons; ecological collapse from AI-driven resource consumption; or gradual displacement where humans become functionally extinct - alive but irrelevant, like gorillas in a human world.

The governance dividend is technically zero - if extinction occurs, governance failed. The *preventive* dividend is enormous: even a 5% reduction in a 10% extinction risk justifies investments of trillions.

**Key tension:** The expected value calculation demands massive preventive investment regardless of precise probability estimates.
