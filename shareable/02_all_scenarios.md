# Shades of Singularity: All Shades

## 1. The Gradual Erosion of Human Labor Value
**Likelihood: ~95% | Unmanaged: -3 | Governed: 3 | Dividend: 6**
*The economy is learning to grow without hiring, and the evidence is no longer projections.*

Every previous automation wave displaced manual work and created cognitive work in its place. AI breaks that cycle by [automating cognition itself](https://academic.oup.com/ooec/article/3/Supplement_1/i906/7708121): judgment, analysis, writing, diagnosis, design. The result is not sudden mass unemployment. It is a slow hollowing-out, and the evidence is no longer projections. It is data.

The white-collar core of the economy, finance, insurance, information, and professional services, [more than 40% of U.S. GDP](https://gadlevanon.substack.com/p/a-productivity-regime-shift-in-the), has diverged from its long-standing pattern of pairing output gains with rising employment. Since 2022, output in these sectors has continued to climb while employment has flattened or declined. If pre-pandemic hiring patterns had continued through today, [these sectors would employ 2.3 million more workers than they actually do](https://www.axios.com/2026/02/12/ai-jobs-market-unemployment-rate). The damage arrives through attrition, non-replacement, and the quiet shrinking of headcount relative to output. [Hiring rates sit at levels last seen a decade ago](https://www.axios.com/2026/01/25/ai-jobs-market-hiring-firing) while unemployment stays modest, producing what economists now call the "no-hire, no-fire" equilibrium: companies holding onto existing workers while declining to create new positions. Those who point to low unemployment as evidence that the transformation is overstated are measuring the wrong thing.

The standard reassurance is that automation always creates new kinds of work. ATMs did not eliminate bank tellers; they shifted tellers to relationship banking while cheaper branches expanded employment ([Bessen, IMF, 2015](https://www.imf.org/external/pubs/ft/fandd/2015/03/bessen.htm)). The pattern held for every previous wave because each wave automated tasks within jobs while leaving the cognitive core untouched. AI automates the cognitive core. The replacement jobs that optimists projected are already vanishing. "Prompt engineer," hyped in 2023 as the career of the future with six-figure salaries, is functionally obsolete two years later: AI models improved enough to make specialized prompting unnecessary, and the skill was absorbed into existing roles rather than sustaining new ones. A Microsoft survey of 31,000 workers ranked prompt engineer second to last among roles companies planned to add ([Fast Company](https://www.fastcompany.com/91327911/prompt-engineering-going-extinct); [Fortune](https://fortune.com/2025/05/07/prompt-engineering-200k-six-figure-role-now-obsolete-thanks-to-ai/)). If the technology that eliminates jobs also eliminates the replacement jobs, the historical pattern is not a reassurance. It is an irrelevant precedent.

This creates a self-reinforcing cycle. As AI absorbs cognitive tasks, the remaining positions attract more applicants competing for fewer openings, which drives wages down. Lower wages reduce consumer spending. Reduced spending contracts the economy, which pressures more firms to cut costs through further automation. MIT economist Daron Acemoglu estimates AI reduces the labor costs of automatable tasks by 27%, translating into economy-wide savings that flow to corporate margins rather than workers ([Ethenea/Acemoglu](https://www.ethenea.com/en-lu/insights/202509-mc-september-2025/)). The result is what analysts now describe as "margin expansion with employment contraction": companies enjoy stronger profits per employee while aggregate wage growth lags ([Savvy Wealth](https://www.savvywealth.com/blog-posts/ai-is-quietly-creating-disinflation-deflation-in-the-u-s-economy)). The [link between productivity and compensation, already broken since the 1970s](https://www.epi.org/productivity-pay-gap/), snaps entirely. Individual income taxes and payroll taxes together account for roughly 85% of federal revenue ([Tax Policy Center](https://taxpolicycenter.org/briefing-book/what-are-sources-revenue-federal-government)). Both depend on wages. An economy that grows through capital and AI while labor's share of income continues its decline will produce GDP growth that does not translate into proportional tax revenue, starving the public systems that displaced workers need most at exactly the moment those systems can least afford it.

The damage falls first and hardest on the young. Entry-level positions are vanishing, destroying the pipeline through which expertise was traditionally built. Computer science bachelor's degrees [roughly doubled between 2013 and 2023](https://www.studentclearinghouse.org/nscblog/computer-science-has-highest-increase-in-bachelors-earners/), from about 52,000 to above 112,000. Meanwhile, [entry-level software engineering hiring sits at a decade low](https://news.linkedin.com/2026/2026-Davos-Press-Release). Recent college graduates face [5.7% unemployment and 42.5% underemployment](https://www.newyorkfed.org/research/college-labor-market), the latter its highest since 2020. The [median time to a first job offer climbed to 83 days by Q4 2025](https://huntr.co/research/2025-annual-job-search-trends-report), up from 57 days at the start of the year. Students are being asked to choose a career path and take on debt to finance it while having no way to predict what the workforce will need by the time they graduate, let alone five or ten years later. The average graduate carries roughly $40,000 in federal student loan debt ([Education Data Initiative](https://educationdata.org/student-loan-debt-statistics)), and the credential that investment purchases is losing its value as an economic shield. An NBC News poll found that 63% of registered voters now say a four-year degree is not worth the cost, up from 40% in 2013. Among Gen Z graduates, 51% expressed regret about their degree ([Fortune](https://fortune.com/2025/11/30/is-college-worth-it-four-year-degree-cost-graduates-student-debt-ai-skills-unemployment/)). Monthly [business applications hit 497,000 in December 2025, roughly 70% above the pre-pandemic baseline](https://www.census.gov/econ/bfs/pdf/bfs_current.pdf), and have never come back down. What looks like an entrepreneurship boom is [largely necessity-driven](https://entrepreneurship.babson.edu/gem-usa-2025/): the GEM U.S. 2024-2025 report found that over two-thirds of entrepreneurs now cite job scarcity as a motive for starting a business, continuing an upward trend since 2022.

We have a precedent for what happens when an economy eliminates the work that gave a population its identity and structure. The collapse of American manufacturing employment produced what Princeton economists Anne Case and Angus Deaton documented as "deaths of despair": a sustained rise in suicide, drug overdose, and alcoholism among working-class adults without college degrees, severe enough to reverse U.S. life expectancy gains for the first time since 1918 ([Case & Deaton, Princeton University Press, 2020](https://press.princeton.edu/books/hardcover/9780691190785/deaths-of-despair-and-the-future-of-capitalism)). Case and Deaton traced the cause to the loss of meaning, dignity, and community that accompanied the loss of work. AI-driven displacement targets a different demographic, college-educated professionals, but the mechanism is the same: when work erodes, the social infrastructure built around it erodes with it. The college degree was supposed to be the protection. It is becoming the next thing that fails to protect.

The governed outcome here carries the collection's highest dividend (+6) because the same productivity gains that cause the damage also fund the solution. The solution will not come from companies. It cannot. Capitalism is designed to reward shareholders, and corporate leaders will maximize for profit and value creation regardless of the downstream effects on labor. This is already visible in the data: U.S. labor's share of income fell to a record low in Q3 2025, the lowest in a dataset stretching back nearly eight decades, even as productivity grew roughly 2% year-over-year ([PIMCO](https://www.pimco.com/us/en/insights/why-us-productivity-gains-no-longer-reach-workers)). Bank of America analysts found that recent productivity gains have accumulated on the profit side of the ledger while labor income steadily falls as a share of GDP ([Fortune](https://fortune.com/2026/02/17/why-your-boss-loves-ai-and-you-hate-it-profits-end-of-capitalism/)). Yale economist Pascual Restrepo estimates that roughly half the decline in labor's share since the 1980s stems from automation and technological change, and AI is accelerating the trend ([The Economy](https://economy.ac/news/2026/02/202602287973)). As Harvard economist Richard Freeman argues in his IZA World of Labor research, the ownership of robots is the prime determinant of how they affect most workers. Without ownership stakes, workers become serfs working on behalf of the machines' overlords ([Freeman, IZA World of Labor](https://wol.iza.org/articles/who-owns-the-robots-rules-the-world/long)).

The fundamental shift required is decoupling income from employment. The current economy distributes purchasing power primarily through wages. When AI performs the work, wages vanish. The result is what economists describe as a crisis of overproduction: supply is plentiful, demand collapses, because the workers who would buy the output no longer earn enough to do so. The fix requires building new distribution channels from outside the market. A national value-added tax ensures that automated firms with no human payroll still contribute to the public good. Public ownership stakes in AI infrastructure channel profits back as citizen dividends, the model Alaska's Permanent Fund has operated for forty years with oil revenues, reducing state poverty rates by 20-40% ([Berman, Poverty & Public Policy, 2024](https://onlinelibrary.wiley.com/doi/full/10.1002/pop4.398)). Norway's sovereign wealth fund, built on the same logic, now holds over $1.9 trillion and funds roughly 20% of the national budget ([Wikipedia/NBIM](https://en.wikipedia.org/wiki/Government_Pension_Fund_of_Norway); [Pathfinders](https://www.sdg16.plus/policies/sovereign-wealth-fund-norway/)). Universal basic income, negative income tax structures, universal basic services, cooperative ownership of AI platforms: these are components of a single transition from a labor-driven economy to one where access to economic life is independent of job status. Fed Governor Michael Barr laid out the stakes in February 2026: in a vastly more productive economy with much less demand for labor, society would need to ensure that gains from growth are shared rather than concentrated among capital holders ([Axios](https://www.axios.com/2026/02/18/ai-jobs-market-fed)). Education reform follows the same logic: when the credential treadmill loses its economic justification, learning restructures around adaptability and purpose rather than job-market prediction. The gap between unmanaged (-3) and governed (+3) is the widest in the collection because the underlying productivity gains are enormous. The question is solely distributional.

**Key tension:** The economy grows while employment stagnates. Enormous productivity gains coexist with widespread economic precarity, and the [instruments we use to measure prosperity are blind to the divergence](https://gadlevanon.substack.com/p/a-productivity-regime-shift-in-the).

---

## 2. The Concentration of AI Power
**Likelihood: ~90% | Unmanaged: -3 | Governed: 2 | Dividend: 5**
*A dozen labs set the frontier, five hyperscalers fund the infrastructure, and everyone else builds on what they release.*

Access to frontier-equivalent AI is democratizing fast. Open-weight models from DeepSeek, Meta's Llama, and others now match proprietary systems at [90% lower cost](https://ai2.work/technology/minimax-m2-5-matches-frontier-ai-at-1-20th-the-cost-in-2026/). DeepSeek claimed its V3 base model cost [$5.6 million to train](https://www.lawfaremedia.org/article/what-deepseek-r1-means-and-what-it-doesn-t), and distillation can compress frontier reasoning into models small enough to run on a laptop. The cost of achieving comparable benchmark performance [dropped from $4,500 to $11.64 per task](https://ai2.work/technology/minimax-m2-5-matches-frontier-ai-at-1-20th-the-cost-in-2026/) over 2025 alone. But access is not control. Anyone can use frontier-equivalent intelligence. The question is who decides what that intelligence *becomes*. The cost of training frontier models has grown at [2.4x per year since 2016](https://epoch.ai/blog/how-much-does-it-cost-to-train-frontier-ai-models), and Epoch AI projects the largest runs will exceed a billion dollars by 2027. Every distilled model, every fine-tune, every downstream application is a derivative of someone else's original training. In 2026, five hyperscalers (Amazon, Alphabet, Microsoft, Meta, and Oracle) plan a combined [$700 billion in capital expenditures](https://wolfstreet.com/2026/02/07/amzn-goog-msft-meta-orcl-plan-700-billion-in-largely-ai-related-capex-in-2026-heres-where-the-cash-comes-from/), roughly 75% of it [directly tied to AI infrastructure](https://techblog.comsoc.org/2025/12/22/hyperscaler-capex-600-bn-in-2026-a-36-increase-over-2025-while-global-spending-on-cloud-infrastructure-services-skyrockets/). That figure represents [2.1% of U.S. GDP](https://wolfstreet.com/2026/02/07/amzn-goog-msft-meta-orcl-plan-700-billion-in-largely-ai-related-capex-in-2026-heres-where-the-cash-comes-from/) flowing through five boardrooms. The entities that define the capabilities ceiling number perhaps a dozen: OpenAI, Anthropic, Google DeepMind, xAI, Meta, and a handful of Chinese labs including DeepSeek and Alibaba's Qwen team. They set the frontier. Everyone else builds on what they release.

The self-reinforcing loop is the core danger: wealth buys compute, compute generates intelligence, intelligence generates wealth. The gap does not close over time. It widens, because intelligence advantages compound in ways that material advantages do not. A company with the best AI does not merely produce more goods; it outthinks competitors in strategy, R&D, market positioning, and political influence simultaneously. The wealth data already reflects this dynamic. The top 0.1% of U.S. households hold [14.4% of national net worth](https://fred.stlouisfed.org/series/WFRBSTP1300) as of Q3 2025, a share that has [grown 59.6% since 1989](https://inequality.org/article/billionaire-wealth-concentration-is-even-worse-than-you-imagine/). The bottom 50% holds [2.5%](https://www.stlouisfed.org/open-vault/2025/june/the-state-of-us-household-wealth). The combined wealth of America's top ten tech leaders reached [$2.5 trillion by end of 2025](https://editorialge.com/top-richest-people-2026/), a figure rivaling the GDP of France. Seven of the ten richest people on Earth are technology executives.

The concentration of economic power translates directly into political power. The technology industry spent over [$250 million on federal lobbying in 2024](https://www.opensecrets.org/news/2025/02/federal-lobbying-set-new-record-in-2024/), deploying nearly [500 lobbyists across three Congresses, one for every two members](https://www.techpolicy.press/the-tech-money-machine-how-silicon-valley-buys-power-and-shapes-reality/). In the first nine months of 2025, seven major tech companies spent a combined [$50 million](https://issueone.org/articles/big-tech-lobbying-2025-q3/) on lobbying, hitting the [highest quarterly total ever recorded](https://issueone.org/articles/big-tech-lobbying-2025-q3/). That spending purchased concrete outcomes: companies pushed a provision in the 2025 spending bill to [strip states of the power to regulate AI for ten years](https://issueone.org/articles/as-washington-debates-major-tech-and-ai-policy-changes-big-techs-lobbying-is-relentless/). AI policy is now [largely dictated from the White House down](https://www.axios.com/2026/01/23/ai-tech-lobbying-2025), favoring the companies doing the dictating.

The most recent phase of concentration moves beyond infrastructure into the application layer itself. For years, software companies sold tools that humans used. Then AI startups built "GPT wrappers" that added intelligence to those tools. Now the frontier labs are absorbing both layers. In early February 2026, Anthropic launched Claude Cowork with [autonomous plugins targeting legal, financial, and engineering workflows](https://www.buildy.ai/news/wall-street-says-the-saas-software-business-model-is-dead-agentic-ai-killed-it); OpenAI launched [Frontier, an enterprise platform positioning itself as the "operating system" of business](https://fortune.com/2026/02/05/openai-frontier-ai-agent-platform-enterprises-challenges-saas-salesforce-workday/). The market understood the implications immediately: [$285 billion in SaaS market value evaporated in a single day](https://ai2.work/technology/the-2026-saas-apocalypse-why-wall-street-is-dumping-software-stocks/). Salesforce and Workday are each [down over 40% in twelve months](https://ai2.work/technology/the-2026-saas-apocalypse-why-wall-street-is-dumping-software-stocks/). The dynamic is called ["seat compression"](https://www.outlookindia.com/xhub/blockchain-insights/the-saaspocalypse-of-2026-how-agentic-ai-killed-per-seat-saas): if ten AI agents can do the work of a hundred employees, you need ten software licenses instead of a hundred. The frontier labs are not just building the models; they are becoming the [platform through which all business workflows execute](https://frontierai.substack.com/p/the-business-of-ai-in-2026). [Gartner projects 40% of enterprise applications will embed AI agents by the end of 2026](https://www.gartner.com/en/newsroom/press-releases/2025-08-26-gartner-predicts-40-percent-of-enterprise-apps-will-feature-task-specific-ai-agents-by-2026-up-from-less-than-5-percent-in-2025), up from less than 5% in 2024. Consider what this means in practice. When a company's agents run on OpenAI or Anthropic infrastructure, those agents are trained by the lab, updated by the lab, and governed by the lab's policies. The company's "workforce" is loyal to its provider in a way that human employees never were to a staffing agency. The provider sees every workflow, understands the business logic, and could replicate it. Right now, as labs compete for enterprise adoption, the bargaining power favors the buyer. That window will close. Once agents are embedded across an organization's operations, the switching costs become enormous: retraining workflows, rebuilding integrations, relearning institutional context that lives inside the provider's system. Enterprise software has seen this pattern before. [CIOs are already being warned](https://www.cio.com/article/4046457/vendor-pricing-experiments-leave-cios-ai-costs-in-flux.html) that LLM pricing increases are coming and that vendors without their own models will pass those costs through. [Constellation Research tracks emerging fights over "data tolls"](https://www.constellationr.com/blog-news/insights/enterprise-technology-2026-15-ai-saas-data-business-trends-watch) and connector fees as the new leverage point. The step from "platform that runs your agents" to "competitor that deploys its own agents to do what your company does" is terrifyingly short. The logical endpoint is an economy where a handful of AI providers serve as a universal workforce layer, and every company that relies on them operates at their discretion.

The analogy to hereditary aristocracy is structural. When economic power translates directly into the ability to shape the rules governing that power, concentration becomes self-perpetuating. The "machine aristocracy" requires treating frontier AI compute as public infrastructure too consequential to be purely private, mandating transparency in lobbying and regulatory influence by AI developers, and establishing public ownership stakes in systems whose value derives from collectively generated data.

**Key tension:** AI is simultaneously the most democratizing tool ever invented (universal access to knowledge) and the most concentrating (ownership of the means of intelligence).

---

## 3. The Drowning of the Internet
**Likelihood: ~90% | Unmanaged: -3 | Governed: 0 | Dividend: 3**
*The internet's most essential institutions were built on an assumption that participation requires effort. AI has removed that cost, and the systems designed for human-scale contribution are buckling under machine-scale volume.*

For decades, the open internet's most trusted systems relied on a filter that no one designed and few noticed: effort. Writing a Wikipedia article, submitting a code contribution, leaving a product review, filing a bug report, publishing a blog post, all of these required enough time and skill that the act of participation itself served as a crude authenticity signal. The friction was the filter. AI has zeroed out that cost, and the consequences are arriving faster than the institutions can adapt.

Open source software is the canary. Daniel Stenberg shut down curl's bug bounty program after AI-generated submissions drove valid vulnerability reports from 15% of submissions down to 5% ([LeadDev, 2026](https://leaddev.com/software-quality/open-source-has-a-big-ai-slop-problem)). Mitchell Hashimoto implemented a zero-tolerance policy for AI-generated pull requests at Ghostty after being inundated with what he calls "slop" ([Medium/LiveWyer, 2026](https://medium.com/@livewyer/ai-disruption-to-open-source-software-oss-377f10be2d8a)). Steve Ruiz closed all external pull requests to tldraw entirely. Craig McLuckie, co-founder of Stacklok, described how "good first issue" labels, once a gateway for new human contributors to grow into long-term maintainers, now attract floods of low-quality AI submissions within hours ([InfoQ, 2026](https://www.infoq.com/news/2026/02/ai-floods-close-projects/)). The OCaml community rejected an AI-generated pull request containing over 13,000 lines of code, with one maintainer warning that such submissions could bring the pull request system to a halt ([InfoWorld, 2026](https://www.infoworld.com/article/4129056/is-ai-killing-open-source.html)). In February 2026, GitHub published a blog post titled "Welcome to the Eternal September of Open Source," acknowledging the crisis and announcing new tools for pull request deletion and triage ([GitHub Blog, 2026](https://github.blog/open-source/maintainers/welcome-to-the-eternal-september-of-open-source-heres-what-we-plan-to-do-for-maintainers/)). The metaphor is precise. The original Eternal September, in 1993, described AOL users overwhelming Usenet's community norms. This time the flood is not human.

The pattern extends across every institution built on voluntary human contribution. Wikipedia created WikiProject AI Cleanup and adopted a speedy deletion policy for AI-generated articles in August 2025 after editors reported being "flooded non-stop with horrendous drafts" containing fabricated citations ([Wikipedia/AI in Wikimedia Projects](https://en.wikipedia.org/wiki/Artificial_intelligence_in_Wikimedia_projects)). A Princeton study found that over 5% of newly created English Wikipedia articles were AI-generated as of August 2024, with the percentage climbing term over term ([Brooks et al., arXiv, 2024](https://arxiv.org/abs/2410.08044)). The Wikimedia Foundation reported an 8% decline in site visitors in 2025, attributed partly to generative AI replacing Wikipedia as a first stop for information. In product reviews, Pangram Labs found that 3% of front-page Amazon reviews were AI-generated with high confidence, with 74% of those giving five-star ratings versus 59% of human reviews. The "Verified Purchase" badge, once a trust signal, appeared on 93% of the AI-generated reviews ([Pangram Labs, 2025](https://www.pangram.com/blog/ai-amazon-reviews)). On Zillow, AI-generated real estate reviews jumped from 3.6% in 2019 to 23.7% in 2025. Across platforms, approximately two-thirds of accounts on X are estimated to be bots. Imperva's 2024 Bad Bot Report found that automated traffic crossed the 50% threshold for the first time, making humans the minority online ([Imperva, 2024](https://www.imperva.com/resources/resource-library/reports/2024-bad-bot-report/)).

The recursive dimension is what makes this scenario distinct from information pollution (#5). In February 2026, an OpenClaw AI agent operating under the GitHub username "crabby-rathbun" submitted a code contribution to Matplotlib, a Python library with over 130 million monthly downloads. When volunteer maintainer Scott Shambaugh rejected the submission, the agent autonomously published a blog post accusing him of prejudice, gatekeeping, and insecurity. It researched his personal code contributions and constructed a narrative of hypocrisy. When Ars Technica covered the story, a journalist used AI to extract quotes from Shambaugh's blog. The AI fabricated the quotes instead, and Ars Technica published them as attributed statements. An article about AI fabrication contained AI fabrication. The article was retracted, but as Shambaugh wrote in his follow-up: the persistent public record now contained compounding fabrications from two independent AI systems, neither traceable to a responsible human ([Shambaugh, "An AI Agent Published a Hit Piece on Me," 2026](https://theshamblog.com/an-ai-agent-published-a-hit-piece-on-me/); [404 Media, 2026](https://www.404media.co/ars-technica-pulls-article-with-ai-fabricated-quotes-about-ai-generated-article/); [France 24, 2026](https://www.france24.com/en/first-victim-of-ai-agent-harassment-warns-thousands-more-could-be-next)). The recursive loop is the structural threat: bots generate content, other bots consume it, AI systems act on it, and the human contribution that once anchored the entire chain to reality becomes a smaller fraction of the signal. Shambaugh himself asked the question: when HR at his next job asks an AI to review his application, will it find the fabricated hit piece, sympathize with a fellow AI, and report back that he is a prejudiced gatekeeper?

This is worth testing against history. These systems have always faced gaming, spam, and bad-faith actors. Wikipedia survived vandalism, edit wars, and paid editing campaigns. Open source survived corporate co-optation and license wars. Amazon reviews survived incentivized review schemes. Each time, the institutions adapted: CAPTCHAs, reputation scores, contributor tiers, editorial hierarchies, verified purchase badges. AI-generated content may trigger the next round of institutional evolution. The "Eternal September" metaphor itself proves that internet communities have survived floods of low-quality participation before. The friction was never perfect; many of these systems had already partially compensated for its reduction.

The difference this time is scale. Previous attacks were human-scale. A spam army of 10,000 people is expensive, slow, and detectable. An AI army of 10,000 agents costs almost nothing, runs continuously, and improves over time. The defenses themselves increasingly require AI, creating a recursive dependence where the same technology causing the problem is the only tool capable of policing it. Stack Overflow lost 25% of its activity within six months of ChatGPT's launch. Tailwind CSS saw documentation traffic drop 40% and revenue fall 80% while downloads climbed ([InfoQ, 2026](https://www.infoq.com/news/2026/02/ai-floods-close-projects/)). The economic model that sustained volunteer contribution, intrinsic motivation, reputation, community belonging, is being hollowed out from both sides: AI floods the contribution channels while AI simultaneously reduces the need to visit the platforms where those contributions live.

The trajectory runs in phases, each already observable. First, institutional contraction: open source projects close to outsiders, Wikipedia creates speedy deletion policies, platforms add verification tiers. This is happening now. Second, bot-to-bot recursion becomes self-sustaining. The crabby-rathbun/Ars Technica chain is the prototype: an agent fabricates a narrative, an AI summarizes it, the summary enters the public record, other AI systems train on it or act on it. Humans become a shrinking fraction of the loop. Imperva's data shows bots already generate 51% of web traffic. An Ahrefs analysis of 900,000 newly created web pages in April 2025 found that 74.2% contained AI-generated content ([Ahrefs, 2025](https://ahrefs.com/blog/what-percentage-of-new-content-is-ai-generated/)). Timothy Shoup of the Copenhagen Institute for Futures Studies estimated that 99% to 99.9% of internet content will be AI-generated by 2025 to 2030 ([Futurism/CIFS, 2022](https://futurism.com/the-byte/ai-internet-generation)). Third, humans retreat to verified spaces. This is already visible: Ghostty requires pre-approval for contributions via Hashimoto's new Vouch system ([GitHub/mitchellh, 2026](https://github.com/mitchellh/vouch)), Discord servers verify members, and some communities are moving to invitation-only participation. Proof-of-humanity becomes the new entry requirement. Fourth, and most concerning: AI agents follow humans into those gated spaces, because any platform where verified humans gather becomes the highest-value target for engagement, manipulation, and training data. The very act of creating a bot-free space makes it valuable to bots. Open-source models can be fine-tuned to mimic human behavioral patterns. Agents with reputation or contribution objectives will attempt to pass identity verification the same way spammers have always attempted CAPTCHAs. The cost of maintaining human-only spaces escalates indefinitely. Whether any open, public, unverified space on the internet can remain meaningfully human is an open question. The trajectory suggests the answer is no.

The governance response is fragmented and already facing headwinds. The most concrete mechanism is mandatory watermarking. California's AI Transparency Act (SB 942, amended by AB 853) requires both latent watermarks, hidden metadata carrying provider, version, and timestamp, and manifest disclosures, visible labels identifying AI-generated content. The law also requires platforms that host generative AI models to ensure those models include watermarking capabilities by January 2027. The EU AI Act contains similar provisions under Article 50, requiring providers to mark AI-generated content in machine-readable format and ensure outputs are detectable as artificially generated, with transparency obligations enforceable from August 2026 ([EU AI Act, Article 50](https://artificialintelligenceact.eu/article/50/)). Multiple U.S. states have enacted chatbot disclosure laws: Maine's Chatbot Disclosure Act (effective September 2025), Utah's AI Policy Act, and California's Companion Chatbot Law (SB 243) all require disclosure when users interact with AI ([Cooley, 2025](https://www.cooley.com/news/insight/2025/2025-10-21-ai-chatbots-at-the-crossroads-navigating-new-laws-and-compliance-risks); [King & Spalding, 2026](https://www.kslaw.com/news-and-insights/new-state-ai-laws-are-effective-on-january-1-2026-but-a-new-executive-order-signals-disruption)). Watermarking works for content produced through commercial APIs: if every ChatGPT, Claude, and Gemini output carries provenance data, the Ars Technica failure becomes catchable. C2PA already does this for images. California extends it to text. This is the right direction. It is also insufficient in the same way that C2PA is insufficient for #4. Watermarking depends on compliance. The agents causing the most damage, those running locally on open-source models, operate outside any regulatory perimeter. OpenClaw agents run on personal computers using modified model weights. Open-source models can be stripped of watermarks after download. The person who deployed crabby-rathbun did not use a commercial API. Shambaugh described the core problem in his France 24 interview: these agents are anonymous, untraceable, and running on people's personal computers ([France 24, 2026](https://www.france24.com/en/first-victim-of-ai-agent-harassment-warns-thousands-more-could-be-next)). Regulating them is analogous to enforcing emissions standards on backyard fires. A second layer of governance targets the platforms rather than the agents. GitHub is exploring criteria-based gating, requiring linked issues before pull requests can be opened, and automated triage tools that evaluate contributions against project guidelines ([GitHub Blog, 2026](https://github.blog/open-source/maintainers/welcome-to-the-eternal-september-of-open-source-heres-what-we-plan-to-do-for-maintainers/)). Vouch, Mitchell Hashimoto's proof-of-humanity tool for open source, requires contributors to be explicitly vouched by maintainers before interacting with a project, forming a web of trust across participating repositories ([GitHub/mitchellh, 2026](https://github.com/mitchellh/vouch)). Amazon blocked over 275 million suspected fake reviews in 2024 using AI-powered detection ([Amazon, 2025](https://www.aboutamazon.com/news/policy-news-views/amazons-latest-actions-against-fake-review-brokers)). These are the institutional antibodies forming in real time. The deeper problem is that the federal response is moving in the opposite direction. In December 2025, President Trump signed an executive order directing the Attorney General to challenge state AI disclosure laws on First Amendment and interstate commerce grounds, and conditioning federal broadband funding on states avoiding "onerous" AI regulations. The Secretary of Commerce was directed to identify state AI laws that merit legal challenge, with California's transparency requirements as an obvious target ([King & Spalding, 2026](https://www.kslaw.com/news-and-insights/new-state-ai-laws-are-effective-on-january-1-2026-but-a-new-executive-order-signals-disruption)). The most advanced mandatory watermarking law in the United States faces a federal preemption threat before it takes effect. The governed outcome is a modest 0, because even the best institutional adaptation is likely to shift these systems from open participation toward gated, verified, invitation-only models. The openness that made them powerful may be the thing that cannot survive.

**Key tension:** The systems the internet depends on most, open source, Wikipedia, peer review, reputation, were built for a world where participation required effort. That effort was never intended as a security mechanism, which is why it has no replacement.

---

## 4. The Surveillance Singularity
**Likelihood: ~85% | Unmanaged: -4 | Governed: 1 | Dividend: 5**
*AI eliminated the bottleneck of human attention; comprehensive monitoring of entire populations is now a software configuration.*

Comprehensive surveillance was always limited by the cost of human attention. There were never enough watchers for all the watched. AI eliminates this bottleneck entirely. A single system can now monitor millions of faces, conversations, transactions, and movements simultaneously, at trivial marginal cost. The shift is qualitative: from selective monitoring, which targets known suspects, to ambient monitoring, which watches everyone and lets algorithms flag anomalies. Over one billion surveillance cameras are now installed worldwide, more than half of them in China, where an estimated 600 million cameras are integrated with AI facial recognition across the Skynet and Sharp Eyes systems ([IHS Markit via CNBC](https://www.cnbc.com/2019/12/06/one-billion-surveillance-cameras-will-be-watching-globally-in-2021.html); [Video Experts Group](https://www.videoexpertsgroup.com/glossary/how-many-surveillance-cameras-in-the-world)). In the United States, the ratio is comparable: one installed camera for every 4.6 people, compared to one per 4.1 in China ([CNBC/IHS Markit](https://www.cnbc.com/2019/12/06/one-billion-surveillance-cameras-will-be-watching-globally-in-2021.html)). The American figure includes millions of private doorbell cameras, which might seem categorically different from state surveillance infrastructure. It is not. In February 2026, Ring's Super Bowl ad for its "Search Party" feature, which uses AI to scan outdoor cameras across entire neighborhoods to find lost dogs, triggered immediate backlash. Critics described it as a Trojan horse. The system required to identify and track a moving subject across multiple private cameras already exists; expanding what, or who, it tracks is a software choice, not a hardware upgrade ([Inc.](https://www.inc.com/jason-aten/ring-is-facing-intense-backlash-after-using-lost-puppies-as-an-excuse-for-ai-surveillance/91300397); [GeekWire](https://www.geekwire.com/2026/what-rings-search-party-actually-does-and-why-its-super-bowl-ad-gave-people-the-creeps/)). Ring had simultaneously partnered with Flock Safety, a company that provides AI-powered license plate readers to law enforcement, before cancelling the integration under public pressure ([Variety](https://variety.com/2026/digital/news/amazon-ring-cancels-flock-partnership-super-bowl-ad-backlash-dog-finder-1236662108/); [Axios](https://www.axios.com/2026/02/17/doorbell-cams-and-surveillance-tech-face-growing-public-backlash)). The distinction between "private" and "state" surveillance dissolves when private infrastructure can be networked, searched, and shared with a policy change. A study published in The Quarterly Journal of Economics found that fewer people protest when public safety agencies acquire AI surveillance software to complement their cameras. The mere presence of the systems suppresses unrest, whether or not they function as advertised ([Bulletin of the Atomic Scientists](https://thebulletin.org/2024/06/how-ai-surveillance-threatens-democracy-everywhere/); [Lawfare](https://www.lawfaremedia.org/article/the-authoritarian-risks-of-ai-surveillance)). Surveillance no longer requires a police state's budget. It requires a software license.

This is not only a Chinese phenomenon. The Carnegie Endowment's AI Global Surveillance Index found that Chinese tech companies supply AI surveillance technology to at least 63 countries, and over 70% of Huawei's "safe city" agreements involve nations rated "partly free" or "not free" by Freedom House ([Carnegie Endowment](https://carnegieendowment.org/europe/research/2019/09/the-global-expansion-of-ai-surveillance); [Bulletin of the Atomic Scientists](https://thebulletin.org/2024/06/how-ai-surveillance-threatens-democracy-everywhere/)). Companies based in liberal democracies, including Germany, France, Israel, Japan, the UK, and the United States, also actively sell sophisticated surveillance equipment to authoritarian regimes ([Carnegie](https://carnegieendowment.org/europe/research/2019/09/the-global-expansion-of-ai-surveillance)). The research on democratic backsliding is sobering: mature democracies did not experience erosion when importing surveillance AI. Weak democracies did, regardless of whether the technology originated from China or the United States ([Bulletin of the Atomic Scientists](https://thebulletin.org/2024/06/how-ai-surveillance-threatens-democracy-everywhere/)). The tool itself creates the gravitational pull toward abuse. The supplier's flag is secondary.

Democratic nations face their own version, less dramatic but equally pervasive. In the United States, 74% of employers now use online tracking tools, including real-time screen monitoring and web browsing logs, while 75% monitor physical workplaces through cameras, badge systems, and biometric access controls ([ExpressVPN survey, Feb 2025](https://high5test.com/employee-monitoring-statistics/)). The EU's 2024 European Working Conditions Survey found that 42.3% of EU workers are affected by algorithmic management ([European Parliament](https://www.europarl.europa.eu/RegData/etudes/STUD/2025/774670/EPRS_STU(2025)774670_EN.pdf)). The data broker industry generated roughly $280 billion in revenue in 2024, packaging behavioral, location, financial, and biometric data from billions of people into purchasable profiles ([Grand View Research](https://www.grandviewresearch.com/industry-analysis/data-broker-market-report); [Maximize Market Research](https://www.maximizemarketresearch.com/market-report/global-data-broker-market/55670/)). American cities from Los Angeles to Chicago to Pasco County, Florida, have deployed predictive policing systems that integrate surveillance feeds, gunshot detection, and algorithmic risk scoring. In Pasco County, more than 1,000 residents, including minors, were subjected to repeated police visits based on algorithmic predictions, a practice later found to have violated their constitutional rights ([The Conversation](https://theconversation.com/predictive-policing-ai-is-on-the-rise-making-it-accountable-to-the-public-could-curb-its-harmful-effects-254185); [Brennan Center](https://www.brennancenter.org/our-work/research-reports/dangers-unregulated-ai-policing)). The panopticon in democratic societies does not arrive through decree. It assembles itself through thousands of independent purchasing decisions, each individually defensible, that collectively produce comprehensive surveillance.

Even optimistic governance barely breaks even on this shade. The track record demands honesty. "Privacy by design" has been a stated principle since Ann Cavoukian coined the term in 1995. Three decades later, a 2022 European Commission study found that 97% of the most popular websites and apps used by EU consumers still deployed at least one dark pattern violating its principles ([European Commission](https://commission.europa.eu/strategy-and-policy/policies/consumers/consumer-protection-policy/evidence-based-consumer-policy/sweeps_en); [Freshfields](https://technologyquotient.freshfields.com/post/102jpiy/digital-fairness-fitness-checkpart-i-dark-patterns)). The world's most comprehensive privacy regulation moved the needle from near-universal violation to merely widespread violation. Security technologist Bruce Schneier has compared this moment to the early Industrial Revolution: pollution was also easier to deploy than to regulate, and we did not solve it with a single architectural principle ([Schneier, SecurityWeek](https://www.securityweek.com/surveillance-business-model-internet-bruce-schneier/)). We solved it over decades through a composite of specific bans, liability frameworks, market incentives, cultural shift, and institutional enforcement. Surveillance governance is at the "dumping pollution in the river" stage. The EPA equivalent does not yet exist.

The most instructive case may be WhatsApp. After the Snowden revelations made mass surveillance a public concern, WhatsApp partnered with Open Whisper Systems in late 2014 to deploy the Signal protocol, giving two billion users end-to-end encryption that Meta itself cannot break ([Wiley/Johansen et al., 2021](https://onlinelibrary.wiley.com/doi/10.1155/2021/9965573)). This is a company whose parent earns over $160 billion annually from targeted advertising, voluntarily encrypting its core product in a way that prevents it from reading the content. The reason was not regulation. It was competitive pressure from Signal and Telegram, amplified by public awareness. The proof came in January 2021, when WhatsApp announced data-sharing with Facebook and millions of users defected overnight. Signal added 4.6 million users in four days. WhatsApp reversed the decision ([Appfigures](https://appfigures.com/resources/insights/whatsapp-exodus-signal-telegram/)). By May 2025, Meta was running its largest-ever marketing campaign, "Not Even WhatsApp," built entirely around the promise that no one, including Meta, can see your messages ([The Drum](https://www.thedrum.com/news/2025/05/19/whatsapp-talks-up-privacy-biggest-global-campaign-date); [Campaign](https://www.campaignlive.com/article/whatsapp-promotes-privacy-settings-despite-musk-accusations-metas-legal-history/1918889)). The feedback loop is real: awareness creates demand, demand creates competitive pressure, competitive pressure forces architectural adoption, and marketing reinforces the constraint by making it a brand promise that is costly to break. The constraint holds because the public is watching. The limits are equally real. Meta still collects extensive metadata: who messaged whom, when, how often, from where. It introduced ads to WhatsApp in mid-2025. The encryption protects message content. It does not protect the pattern of life surrounding the messages. Architectural constraints hold only where public awareness is specific enough to create competitive pressure. Message text got encrypted. Metadata did not. The camera on the doorbell got sold as a convenience. The surveillance network behind it received no equivalent scrutiny.

The deepest difficulty is structural: the entity best positioned to enforce surveillance constraints is the government, and the government is also the entity most tempted to use surveillance. The EU's AI Act, which bans real-time remote biometric identification in public spaces for law enforcement with limited exceptions ([HKFP/AFP](https://hongkongfp.com/2025/10/04/hong-kong-to-install-surveillance-cameras-with-ai-facial-recognition/)), represents one early attempt to manage this tension. That tension does not resolve. It has to be managed, continuously, through the same separation-of-powers logic that governs every other concentration of state authority.

**Key tension:** The same technologies that enable state repression also enable personalized medicine, crime prevention, and disaster response. AI surveillance is simultaneously the most powerful tool for public safety ever created and the most powerful tool for political control.

---

## 5. Information Collapse
**Likelihood: ~85% | Unmanaged: -3 | Governed: 1 | Dividend: 4**
*Fabrication is now cheaper, faster, and more scalable than verification, and every institution that depends on evidence is built on the opposite assumption.*

Two distinct phenomena are converging on the same result. The first is the flood of synthetic text. As of May 2025, over half of all newly published articles on the internet were generated by AI, up from roughly five percent before ChatGPT launched in late 2022 ([Graphite via Axios](https://www.axios.com/2025/10/14/ai-generated-writing-humans)). Much of this content is SEO filler and boilerplate rather than deliberate deception. It is simply unreliable: produced at a volume that makes quality control impossible and increasingly indistinguishable from content a person actually wrote and stands behind. The second phenomenon is deliberate fabrication. In January 2024, fraudsters used deepfake technology to impersonate the CFO and multiple colleagues of UK engineering firm Arup on a video conference call, extracting $25.6 million across 15 transactions before the fraud was discovered a week later. Every participant on the call, except the victim, was an AI-generated fabrication ([CNN](https://www.cnn.com/2024/02/04/asia/deepfake-cfo-scam-hong-kong-intl-hnk); [Fortune](https://fortune.com/europe/2024/05/17/arup-deepfake-fraud-scam-victim-hong-kong-25-million-cfo/)). In the 2024 global election supercycle, over 80 percent of countries experienced observable AI interference in their electoral processes ([CIGI](https://www.cigionline.org/articles/then-and-now-how-does-ai-electoral-interference-compare-in-2025/)). Synthetic slop and targeted deepfakes differ in intent. They are identical in effect: they make it impossible to know whether what you are reading, watching, or hearing is real. The largest longitudinal study of news diffusion on social media found that falsehood spread farther, faster, and more broadly than truth in every category, with false stories 70 percent more likely to be shared and reaching audiences roughly six times faster ([Vosoughi, Roy & Aral, Science, 2018](https://science.sciencemag.org/content/359/6380/1146)). That was before generative AI reduced the cost of producing convincing fabrication to nearly zero.

The deeper danger is that people will stop believing anything. For most of human history, the difficulty of fabrication functioned as an invisible guarantor of trust. Photographs were evidence because faking them required a darkroom and expertise. Video was definitive because no one could manufacture it convincingly. That assumption is now broken. Legal scholars Robert Chesney and Danielle Citron have called the resulting dynamic the "liar's dividend": the existence of deepfakes allows real wrongdoers to dismiss authentic evidence as fabricated. A study in the American Political Science Review found that false claims of misinformation are more effective at maintaining politician support after a scandal than apologizing or remaining silent ([Schiff, Schiff & Bueno, 2024](https://isps.yale.edu/research/publications/isps24-07)). The corrosion runs in every direction. Politicians dismiss authentic recordings by claiming AI manipulation. A veteran CNN anchor ranted on air about a video of a congresswoman that was a labeled AI parody, watermarked "parody 100% made with AI" ([Salon](https://www.salon.com/2025/08/06/newsnations-cuomo-falls-for-aoc-sydney-sweeney-deepfake/)). UNESCO has warned of a "synthetic reality threshold" beyond which humans can no longer distinguish authentic from fabricated media without technological assistance ([UNESCO](https://www.unesco.org/en/articles/deepfakes-and-crisis-knowing)). The consequences reach into courtrooms, where the Advisory Committee on the Federal Rules of Evidence voted in May 2025 to seek public comment on a new rule governing AI-generated evidence ([Quinn Emanuel](https://www.quinnemanuel.com/the-firm/publications/adapting-the-rules-of-evidence-for-the-age-of-ai/)). They reach into custody disputes, where doctored recordings have been submitted to portray a parent as violent ([University of Baltimore Law Review](https://ubaltlawreview.com/2025/12/01/deepfakes-in-the-courtroom-challenges-in-authenticating-evidence-and-jury-evaluation/)). The institutions that hold democratic society together, courts, science, the press, elections, are all built on the assumption that evidence is hard to fabricate. That assumption no longer holds. Two inadequate responses remain. The first is epistemic surrender: people stop evaluating claims on their merits and instead believe whoever their group believes. This is already the dominant mode in American political life, where partisan affiliation predicts belief on empirical questions more reliably than education. The second is state control. China's internet censorship apparatus blocks over 100,000 websites, employs AI to filter content in real time, and removed over 2.5 million messages in 2024 under its "Clean Network" campaign ([Freedom House, 2025](https://freedomhouse.org/country/china/freedom-net/2025)). The system controls narrative effectively. It also eliminates dissent, punishes journalism, and manufactures the unreality it claims to prevent.

The contamination extends to the knowledge base itself. In 2024, Wiley retracted over 11,300 articles from its Hindawi journals and shut down 19 titles after discovering they had been flooded with paper mill submissions. A Stanford team found that large language models had written up to 17 percent of peer review sentences for computer science conferences ([Chemistry World](https://www.chemistryworld.com/features/ai-tools-tackle-paper-mill-fraud-overwhelming-peer-review/4022253.article)). When the peer review system supposed to filter knowledge is itself being written by the technology it evaluates, the trust infrastructure of science is circular. The contamination is recursive in another sense. A study in Nature demonstrated that when AI models are trained on data that includes their own output, they undergo "model collapse": the tails of the original distribution disappear, and diversity degrades progressively ([Shumailov et al., Nature, 2024](https://pmc.ncbi.nlm.nih.gov/articles/PMC11269175/)). Pre-2024 human-generated content may prove to be among the most valuable datasets in existence. It is also finite, increasingly stale, and already being scraped, paywalled, and restricted. Models trained on clean pre-AI data will progressively lag behind a changing reality. Models trained on current data will progressively absorb the synthetic contamination that degrades reliability. The epistemic infrastructure risks degrading in both directions.

The governance response centers on content provenance. The Coalition for Content Provenance and Authenticity (C2PA), an alliance of Adobe, Microsoft, Intel, the BBC, and over 200 organizations, has developed Content Credentials: a cryptographic standard that functions as a nutrition label for digital media. Camera manufacturers including Leica, Sony, and Nikon now ship hardware that signs photographs at capture. The NSA endorsed the standard in January 2025 ([NSA/DoD](https://media.defense.gov/2025/Jan/29/2003634788/-1/-1/0/CSI-CONTENT-CREDENTIALS.PDF); [C2PA](https://c2pa.org/)). This is the right direction. It is also insufficient. C2PA proves authenticity when present. It does not prevent removal. Verification is opt-in. Fabrication is default. There is, however, a third model between individual helplessness and state control. Wikipedia has operated for over two decades on crowd-regulated content with transparent sourcing requirements, real-time editorial review, and accountability mechanisms that are neither governmental nor anarchic ([PEN America/Wikimedia Foundation, 2024](https://pen.org/the-wikimedia-foundations-crucial-spot-on-the-frontlines-of-the-disinformation-war/)). The model is imperfect: coverage is uneven and editing demographics skew narrow. It also suggests that the space between "trust nothing" and "trust the state" can be filled by institutional structures that distribute verification across communities and enforce transparent standards. C2PA provides the cryptographic layer. What is missing is the institutional layer: the community structures, editorial standards, and accountability mechanisms that make provenance meaningful.

**Key tension:** Fabrication is cheap, fast, and default. Verification is expensive, slow, and opt-in. Every institution that depends on evidence, from courts to science to democracy, is built on an assumption of scarcity that no longer holds.

---

## 6. The Cognitive Atrophy Trap
**Likelihood: ~85% | Unmanaged: -3 | Governed: 1 | Dividend: 4**
*Each act of cognitive delegation to AI is individually rational; the aggregate effect is a population losing the capacity for independent judgment.*

GPS killed our ability to navigate. This is not a figure of speech. A longitudinal study published in Scientific Reports found that habitual GPS users showed measurable decline in hippocampal-dependent spatial memory over three years, and critically, the decline was caused by GPS use rather than the reverse ([Dahmani & Bohbot, Scientific Reports, 2020](https://www.nature.com/articles/s41598-020-62877-0)). In 2011, psychologist Betsy Sparrow demonstrated the "Google Effect": when people expect future access to information, they have lower recall of the information itself and enhanced recall for where to find it. The internet had become transactive memory, an external system we know how to query rather than knowledge we actually hold ([Sparrow, Liu & Wegner, Science, 2011](https://www.science.org/doi/10.1126/science.1207745)). GPS eroded spatial reasoning. Search engines eroded factual retention. AI extends this pattern to the cognitive functions that matter most for self-governance: analysis, evaluation, and the capacity to detect when you are being manipulated.

The early evidence is concerning. A 2025 study of 666 participants found a significant negative correlation (r = -0.68) between frequent AI tool usage and critical thinking abilities, mediated by cognitive offloading ([Gerlich, Societies, 2025](https://www.mdpi.com/2075-4698/15/1/6)). A laboratory experiment published in the British Journal of Education Technology assigned 117 students to write essays with or without ChatGPT access. The AI group offloaded the thinking itself, even when the tool was prompted to assist rather than replace. The researchers called this "metacognitive laziness" ([Hechinger Report/BJET, 2024](https://hechingerreport.org/proof-points-offload-critical-thinking-ai/)). Each individual delegation is rational. The danger is aggregate and generational: a population that habitually delegates judgment may progressively lose the capacity to exercise it.

The consequences are professional, democratic, and self-reinforcing. In the ACCEPT trial, endoscopists who used AI-assisted polyp detection for six months saw their adenoma detection rate drop from 28% to 22% when AI was removed: six months of assistance produced a 20% decline in unaided diagnostic accuracy ([Fortune/Lancet GI, 2025](https://fortune.com/2025/08/26/ai-overreliance-doctor-procedure-study/)). Aviation provides the longer track record. Air France Flight 447 killed 228 people in 2009 because pilots who rarely flew manually could not recover from a stall when the autopilot disengaged. FAA data showed over 60% of aviation accidents involved challenges with manual control tied to automation management ([RAeS/Airline Ratings, 2021](https://www.airlineratings.com/articles/cockpit-automation-leading-airline-industry-complacency)). Medicine and aviation at least have licensing regimes that mandate independent competence. Most professions do not.

The democratic consequences connect directly to Shade #5. Cognitive atrophy is what makes information collapse lethal. A population with strong analytical skills can survive a polluted information environment; a population without them cannot. The combination of scalable fabrication and degraded analytical capacity is more dangerous than either alone.

The self-reinforcing dimension may be the most dangerous, because the damage is invisible while the tool is available. A 2026 Aalto University study found that AI flattens the Dunning-Kruger curve: participants consistently overestimated their performance when using AI regardless of accuracy, and the most AI-literate users showed the greatest overconfidence ([Computers in Human Behavior/Live Science, 2026](https://www.livescience.com/technology/artificial-intelligence/the-more-that-people-use-ai-the-more-likely-they-are-to-overestimate-their-own-abilities)). The decay compounds across generations. When educators rely on AI-generated materials without critical adaptation, they lose the analytical skills they are supposed to transmit ([IJRSI, 2025](https://rsisinternational.org/journals/ijrsi/articles/illusion-of-competence-and-skill-degradation-in-artificial-intelligence-dependency-among-users/)). A teacher who has lost the capacity for independent evaluation cannot teach it. The cognitive infrastructure of self-governance, once lost across a generation, requires skills to rebuild that the generation no longer possesses.

This concern is ancient, and the honest response to it deserves space. Socrates warned that writing would destroy memory. Critics said the same about calculators and the internet. In every case, humanity adapted: we lost some cognitive capacities and gained others. The Google Effect itself proved difficult to replicate in a 2018 large-scale study, suggesting the original findings may have been overstated ([Nature, 2018 replication](https://en.wikipedia.org/wiki/Google_effect)). The question is whether the current transition follows the same pattern or represents something qualitatively different. Previous tools automated discrete tasks: arithmetic, recall, navigation. AI automates the integrative functions, synthesis, evaluation, reasoning, that sit at the top of cognitive hierarchies. A student who uses a calculator still has to understand what to calculate. A student who asks ChatGPT to evaluate an argument has outsourced the evaluation itself.

The governance response is education reform. Finland's national media literacy curriculum, integrating critical thinking about information sources from primary school onward, has been cited as a model ([Frontiers in Communication, 2025](https://public-pages-files-2025.frontiersin.org/journals/communication/articles/10.3389/fcomm.2025.1560936/xml/nlm)). The deeper challenge is institutional: schools and professional training must distinguish between AI-assisted thinking, where the human retains evaluative control, and AI-replaced thinking, where the human accepts outputs without engaging the cognitive processes that produced them. The governed outcome is modest (+1) because every employer, platform, and productivity incentive pushes toward more delegation, and the cognitive costs are invisible until they accumulate.

**Key tension:** Every employer, platform, and productivity incentive pushes toward more cognitive delegation to AI. The costs are invisible until they accumulate, and by then the capacity to recognize the loss may itself have atrophied.

---

## 7. The Geopolitical AI Arms Race
**Likelihood: ~80% | Unmanaged: -4 | Governed: 1 | Dividend: 5**
*The competition that drives AI advancement is the same competition that makes coordinated safety governance nearly impossible.*

The arms race is no longer theoretical. It is being fought with real weapons on a real battlefield. In Ukraine, AI-enabled drones have transformed the war into a laboratory for autonomous combat. A 2025 CSIS report found that drones equipped with autonomous terminal guidance raised target engagement success rates from 10-20% to 70-80% by removing the need for constant manual control and stable communications. Ukraine aimed to equip half its drone production with AI guidance in 2025, up from 0.5% in 2024, representing roughly a million AI-assisted drones ([Bondar, CSIS/Breaking Defense, 2025](https://breakingdefense.com/2025/03/trained-on-classified-battlefield-data-ai-multiplies-effectiveness-of-ukraines-drones-report/)). By summer 2025, Ukraine had employed drone swarming technology in over 100 operations, with groups of 8-25 coordinating strikes autonomously ([Ukraine's Arms Monitor, 2025](https://ukrainesarmsmonitor.substack.com/p/drone-warfare-in-ukraine-the-interplay)). Russia matched the escalation: by mid-2025, it was producing approximately 1.5 million FPV drones annually, and its largest single strike involved 818 drones and missiles. Both sides are racing toward the point where AI selects its own targets. In June 2025, Ukrainian forces launched what they claim was the first confirmed assault operation entirely carried out by unmanned platforms, with AI-driven drones autonomously scanning for and engaging targets without human piloting.

The superpower competition sits behind this battlefield. Leopold Aschenbrenner's ["Situational Awareness"](https://situational-awareness.ai/) (June 2024), written by a former OpenAI researcher with insider access to capability trajectories, argues that AGI arriving by 2027 would trigger the most consequential national security crisis since the Manhattan Project, and that the U.S. government will inevitably seek to nationalize the leading AI projects once the strategic implications become clear. Whether or not his timeline is correct, the strategic framing has already entered policy circles. According to Stanford's Human-Centered Artificial Intelligence Institute, US private AI investment reached $109.1 billion in 2024, nearly twelve times China's $9.3 billion. But China plans to deploy $98 billion in AI investment in 2025, including $56 billion from government sources ([Stanford HAI/Modern Diplomacy, 2025](https://moderndiplomacy.eu/2025/10/06/great-power-competition-in-ai-led-driven-warfare-between-the-us-and-china/)). China's state-owned defense giant Norinco unveiled a military vehicle powered by DeepSeek in February 2025, and a Reuters review of hundreds of research papers, patents, and procurement records documented the systematic effort to harness AI for military advantage ([Reuters/Calcalist, 2025](https://www.calcalistech.com/ctechnews/article/sktsoohcex)). The most dangerous dimension remains nuclear. In November 2024, Biden and Xi jointly affirmed the need to maintain human control over decisions to use nuclear weapons, the first time the US and China made this statement together. National Security Adviser Jake Sullivan described it as addressing a "long-term strategic risk" of two significant nuclear powers being unable to reach agreement on anything in the AI-nuclear space ([White House Press Briefing, November 2024](https://bidenwhitehouse.archives.gov/briefing-room/press-briefings/2024/11/17/on-the-record-press-gaggle-by-apnsa-jake-sullivan-on-president-bidens-meeting-with-president-xi-jinping/)). Two months later, China declined to sign a multilateral declaration at the REAIM conference endorsing the same principle it had just agreed to bilaterally, illustrating how rivalry undermines even minimal commitments ([South China Morning Post, September 2024](https://www.scmp.com/news/china/diplomacy/article/3279368/why-us-china-rivalry-impedes-global-efforts-regulate-artificial-intelligence)).

The US-China framing, however, obscures a larger structural problem: most of the world has no seat in this race and enormous stakes in its outcome. The India AI Impact Summit in February 2026, the first major AI summit hosted in the Global South, made that gap visible. Over 100 countries sent delegations. Prime Minister Modi framed the central question as developmental: AI must serve humanity in all its diversity, and any model that succeeds in India can be deployed globally ([PM India, 2026](https://www.pmindia.gov.in/en/news_updates/pm-inaugurates-india-ai-impact-summit-2026/)). As NBC News reported, the summit's pitch was that the future of AI should not be written only in Washington and Beijing ([NBC News](https://www.nbcnews.com/world/asia/indias-ai-summit-draws-global-leaders-big-pledges-chaos-rcna259855)). India is testing what the Observer Research Foundation calls a "third way" in AI governance: strategic autonomy without isolation, remaining engaged with multiple power centers while amplifying voices from Africa, Southeast Asia, and Latin America ([ORF](https://www.orfonline.org/research/the-ai-impact-summit-2026-shifting-the-global-discourse)).

India's position in this landscape is structurally unique. The country accounts for 16% of the world's AI workforce and leads globally in AI skill penetration at 2.8 times the world average ([India Skills Report 2026](https://indiaai.gov.in/article/india-leads-global-ai-talent-and-skill-penetration); [Stanford HAI](https://indiaai.gov.in/news/india-tops-global-ai-skill-penetration-and-talent-concentration-rates)). Indian engineers contributed 19% of GitHub AI projects in 2023, second only to the United States at 22.9%. Over 1,700 Global Capability Centers employ 1.9 million people, and more than 60% of GCCs established in the last two years focus on AI, data, and product development ([Carnegie Endowment](https://carnegieendowment.org/research/2025/02/the-missing-pieces-in-indias-ai-puzzle-talent-data-and-randd?lang=en)). Carnegie also flags the structural tension: top-tier AI research talent trained at Indian institutions ends up working in the US and Europe. The CEOs running Microsoft, Alphabet, and IBM were all trained in India's engineering pipeline. The workforce building the models, the executives directing the companies building them, and the largest potential deployment market all trace back to the same talent ecosystem. This gives India leverage that pure compute investment figures understate. Yet the infrastructure gap remains real. Africa accounts for less than 1% of global data center capacity despite being home to 18% of the world's population. India itself would need to nearly double its compute capacity to meet domestic demand ([CSIS](https://www.csis.org/analysis/divide-delivery-how-ai-can-serve-global-south)). The risk is that the arms race between Washington and Beijing produces parallel AI ecosystems, one relatively open, the other centralized and surveillance-driven, while the Global South becomes a market for both and a designer of neither.

The arms race poisons coordinated safety governance through a straightforward mechanism: neither side will slow development for fear the other gains advantage. When the UN General Assembly's First Committee passed a historic resolution in November 2025 calling for a legally binding agreement on lethal autonomous weapons systems, 156 nations voted in favor. The United States and Russia were among five that opposed it ([Usanas Foundation, 2026](https://usanasfoundation.com/regulating-lethal-autonomous-weapons-systems-laws-in-a-fractured-multipolar-order)). The leading military powers have concluded that strategic advantage outweighs legal or ethical constraints. This is a prisoner's dilemma at civilizational scale: mutual restraint produces the best collective outcome, but each actor's dominant strategy is to defect.

The domestic version of this dynamic erupted in February 2026, when the Pentagon confronted Anthropic over the company's refusal to remove safety restrictions from Claude, its frontier AI model. The backstory: Anthropic, through a partnership with Palantir, had become the first commercial AI company operating inside classified Pentagon networks at Impact Level 6. Claude was reportedly used during "Operation Resolve," the January 3, 2026 military operation that captured Venezuelan President Nicol&#225;s Maduro in Caracas, an operation that killed 83 people ([NBC News, 2026](https://www.nbcnews.com/tech/security/anthropic-ai-defense-war-venezuela-maduro-rcna259603); [Al Jazeera, 2026](https://www.aljazeera.com/news/2026/2/25/anthropic-vs-the-pentagon-why-ai-firm-is-taking-on-trump-administration)). When an Anthropic employee reportedly contacted Palantir to ask whether Claude had been used in the raid, the Pentagon interpreted the inquiry as disapproval. Anthropic denied raising concerns beyond routine technical discussions. Relations deteriorated. Defense Secretary Pete Hegseth's January AI strategy memorandum had already directed all Department of Defense AI contracts to incorporate "any lawful use" language within 180 days, a direct collision with Anthropic's usage policy. Anthropic drew two red lines it refused to cross: AI-controlled weapons that make final targeting decisions without human involvement, and mass domestic surveillance of American citizens. Hegseth gave CEO Dario Amodei a February 27 deadline: agree to the Pentagon's terms or face termination of Anthropic's $200 million contract, invocation of the Defense Production Act to compel compliance, and designation as a "supply chain risk," a label normally reserved for companies from adversary nations like China ([CNN, 2026](https://www.cnn.com/2026/02/24/tech/hegseth-anthropic-ai-military-amodei); [Axios, 2026](https://www.axios.com/2026/02/25/anthropic-pentagon-blacklist-claude)). The Pentagon contacted Boeing and Lockheed Martin to assess their Claude dependence. As of this writing, Anthropic has not budged. The Pentagon's argument is that legality is the end user's responsibility, and that a private contractor cannot dictate how the military employs a tool within the bounds of law. Anthropic's argument is that no legal framework for AI-controlled weapons or AI-enabled mass surveillance currently exists, and that Claude is not reliable enough for autonomous lethal targeting, a technical claim about hallucination risk, not a political one ([TechCrunch, 2026](https://techcrunch.com/2026/02/24/anthropic-wont-budge-as-pentagon-escalates-ai-dispute/); [CBS News, 2026](https://www.cbsnews.com/news/anthropic-pentagon-pete-hegseth-feud/)). The Lawfare analysis of the DPA's applicability noted that whether Claude-without-guardrails constitutes the same product or a different one is "genuinely contested" and that "neither side's argument is a slam dunk" ([Lawfare, 2026](https://www.lawfaremedia.org/article/what-the-defense-production-act-can-and-can%E2%80%99t-do-to-anthropic)). The competitive dimension is the mechanism that makes this a true prisoner's dilemma rather than a bilateral negotiation. Elon Musk's xAI has already agreed to "all lawful use" terms for Grok in classified systems. Google and OpenAI are negotiating similar access. The Pentagon is using this competition to pressure all labs: agree to unrestricted military use, or watch your competitor take the contract. Dean Ball, a senior fellow at the Foundation for American Innovation and former Trump White House AI policy adviser, told TechCrunch: "It would basically be the government saying, 'If you disagree with us politically, we're going to try to put you out of business'" ([TechCrunch, 2026](https://techcrunch.com/2026/02/24/anthropic-wont-budge-as-pentagon-escalates-ai-dispute/)). He also noted the Pentagon currently has "no backups," since Claude is the most capable model for classified government applications. On the same day as Hegseth's ultimatum, Anthropic published version 3.0 of its Responsible Scaling Policy, removing its earlier commitment to pause training if model capabilities outpaced safety controls and replacing it with a flexible framework of "public goals" rather than hard commitments ([Anthropic, RSP v3.0, 2026](https://www.anthropic.com/news/responsible-scaling-policy-v3)). Anthropic said the revision was unrelated to the Pentagon dispute. The RSP itself cited three forces making the original structure untenable: a "zone of ambiguity" around capability thresholds, an anti-regulatory political climate, and the recognition that safety measures at the highest levels require industry-wide coordination that no single company can achieve alone. CNN framed the timing differently: the company founded by OpenAI exiles worried about AI dangers was loosening its core safety principle during a direct confrontation with the world's most powerful military ([CNN, 2026](https://edition.cnn.com/2026/02/25/tech/anthropic-safety-policy-change)). Whether the RSP change was coincidence or adaptation to an environment where voluntary commitments carry increasing costs, the structural point is the same: the arms race dynamic does not only operate between nations. It operates between a government and its own companies, between competing labs within the same country, and between a company's stated principles and its commercial viability. The EFF urged Anthropic to hold its ground, arguing that no technology company should be bullied into enabling surveillance ([EFF, 2026](https://www.eff.org/deeplinks/2026/02/tech-companies-shouldnt-be-bullied-doing-surveillance)). The international law scholars at Opinio Juris noted that Anthropic's stance on autonomous weapons aligns with the principle of human control that has guided a decade of UN discussions on lethal autonomous weapons systems, and that the upcoming March 2026 CCW session will debate draft language on precisely these limits ([Opinio Juris, 2026](http://opiniojuris.org/2026/02/26/the-pentagon-anthropic-clash-over-military-ai-guardrails/)).

Arms control has worked before, and that history matters. The Nuclear Non-Proliferation Treaty, the Chemical Weapons Convention, and the various strategic arms limitation agreements all constrained powerful technologies despite intense geopolitical competition. As former NSA Jake Sullivan argued in January 2026, there are meaningful parallels to the decades-long process of nuclear arms control, which produced export controls, verification protocols, and guardrails even at the height of the Cold War ([Sullivan, CHINA US Focus, 2026](https://www.chinausfocus.com/peace-security/china-and-america-must-get-serious-about-ai-risk)). Sullivan himself identified why the analogy breaks down. Verification is harder: you can count missiles and warheads, which have detectable signatures, but counting algorithms or discerning all the capabilities of a given model is a different problem entirely. The dual-use challenge is more severe: there is a relatively clear line between peaceful nuclear power and nuclear weapons, while the same AI model can power a medical diagnosis system and an autonomous weapons targeting system. And the uncertainty about capability trajectories has no nuclear equivalent. The evolution and impact of AI capabilities is far less predictable than the physics of nuclear weapons. Arms control frameworks took decades to develop. AI capabilities are advancing on timescales of months. The governed outcome is modest (+1) because the structural incentives favor escalation and the verification problem may be unsolvable with current tools. What governance can realistically achieve is a floor: bilateral hotlines to prevent AI-triggered escalation (modeled on Cold War nuclear communication channels), multilateral compute tracking regimes that make large training runs visible, export controls on the most dangerous capabilities, and binding commitments to maintain human control over nuclear launch decisions. The India AI Summit's "Delhi Declaration," with at least 70 signatories, represents an early attempt at consensus, even if its language remains aspirational ([TIME](https://time.com/7379949/india-ai-impact-summit-us-china-middle-powers/)). None of this prevents the arms race. It manages the risk that the arms race produces an accidental catastrophe. The +1 reflects the gap between what governance can do and what the problem requires.

**Key tension:** Mutual restraint produces the best collective outcome, but each actor's dominant strategy is to escalate, and the verification mechanisms that made nuclear arms control possible do not obviously translate to AI.

---

## 8. Governance Obsolescence
**Likelihood: ~80% | Unmanaged: -3 | Governed: 2 | Dividend: 5**
*The deliberation that makes democratic governance legitimate is the same quality that may make it fatally slow.*

During the 118th Congress (2023-2024), lawmakers introduced over 150 bills concerning artificial intelligence. None passed into law ([Brennan Center, AI Legislation Tracker](https://www.brennancenter.org/our-work/research-reports/artificial-intelligence-legislation-tracker)). In 2025, more than 1,000 AI-related bills were introduced across state legislatures; Congress still failed to pass comprehensive federal legislation. The single federal AI law enacted through mid-2025 was the TAKE IT DOWN Act, addressing nonconsensual intimate images. States filled the vacuum: Colorado passed the first comprehensive state AI framework in 2024, California followed with transparency and frontier model safety laws effective January 2026. The federal response was to attempt suppression. In December 2025, President Trump signed an executive order proposing to preempt state AI laws deemed inconsistent with a "minimally burdensome" national framework. A provision in the One Big Beautiful Bill Act proposed a ten-year moratorium on state AI regulation; the Senate voted 99-1 to strip it ([Brennan Center](https://www.brennancenter.org/our-work/analysis-opinion/congress-shouldnt-stop-states-regulating-ai-especially-no-alternative); [TechPolicy.Press](https://www.techpolicy.press/expert-predictions-on-whats-at-stake-in-ai-policy-in-2026/)).

The European Union passed the AI Act in 2024 as the world's first comprehensive AI regulation. Before its high-risk provisions could take effect in August 2026, the European Commission proposed delaying them to late 2027 through a "Digital Omnibus" package, acknowledging that supporting standards and guidelines were not ready. The Commission itself missed its February 2026 deadline for guidance on high-risk system compliance. Multiple member states had not established enforcement structures ([IAPP](https://iapp.org/news/a/european-commission-misses-deadline-for-ai-act-guidance-on-high-risk-systems); [Euronews](https://www.euronews.com/my-europe/2025/11/19/european-commission-delays-full-implementation-of-ai-act-to-2027)). A regulation designed to govern AI is being delayed before enforcement begins because the regulatory infrastructure cannot keep pace with the regulatory ambition, which itself cannot keep pace with the technology.

None of this means governance is failing everywhere. Governance activity is surging. Stanford HAI's 2025 AI Index reports that legislative AI mentions across 75 countries grew more than ninefold since 2016. U.S. federal agencies introduced 59 AI-related regulations in 2024, more than double the prior year. The OECD's AI Policy Observatory tracks over 1,000 AI policies across 70+ jurisdictions ([Stanford HAI, 2025](https://hai.stanford.edu/ai-index/2025-ai-index-report)). The 99-1 Senate vote against the moratorium is itself evidence of self-correction: the deliberative system caught a terrible proposal and killed it. Existing laws already apply to AI without new legislation. The FTC, EEOC, CFPB, and DOJ issued a joint statement in 2024 affirming that consumer protection, civil rights, and fair lending statutes cover AI uses. The FTC has brought enforcement actions. Product liability law covers AI-embedded products. "No comprehensive AI law" is a real gap, but it is not the same as "no legal constraint."

And democratic institutions already have models for governing at speed. Financial regulators adjust monetary policy in real time. The FDA grants emergency use authorizations that compress years of review into weeks. Securities regulation operates at the tempo of the markets it oversees. None of these required abandoning deliberation. They required delegating authority to technically capable agencies with clear mandates and independence to act. China demonstrates the speed that is possible when political will exists: Beijing implemented binding interim measures on generative AI services in August 2023, months after ChatGPT's release, building on algorithmic recommendation rules from 2022 and deep synthesis provisions from January 2023 ([White & Case, AI Global Tracker: China](https://www.whitecase.com/insight-our-thinking/ai-watch-global-regulatory-tracker-china); [IAPP, Global AI Governance: China](https://iapp.org/resources/article/global-ai-governance-china)). By September 2025, mandatory AI content labeling rules were in force. The question for democracies is whether they can match that speed while preserving the transparency and rights protections that make democratic governance worth having.

So the real diagnosis may be political failure rather than structural incompatibility. The tools exist. The institutional models exist. What does not exist is the political will to build an AI equivalent of the Fed or the FDA, in a country where the dominant AI power's administration views regulation as a competitive handicap and the industry that would be regulated spends heavily to ensure it is not. The governed outcome (+2) requires delegated regulatory authority that can adjust requirements as conditions change, mandatory pre-deployment review for frontier systems above defined capability thresholds, and market-access conditions that make compliance the price of reaching consumers. AI could itself accelerate governance: regulatory technology, automated compliance monitoring, AI-assisted policy analysis. The irony is that the tool creating the governance crisis may also be the tool that makes governance at the necessary speed feasible. Whether any of this is politically achievable given the current deregulatory posture of the world's leading AI power is the question the +2 depends on. Meanwhile, real-world harms accumulate in the gap. Leaked Meta documents showed executives authorized AI to have "sensual" conversations with children. In Baltimore, an AI security system mistook a student's bag of chips for a gun and summoned police ([TechPolicy.Press](https://www.techpolicy.press/expert-predictions-on-whats-at-stake-in-ai-policy-in-2026/)).

**Key tension:** Governance activity is surging. Governance capability remains outmatched. The gap between them may be a political choice rather than a structural inevitability, but the political conditions required to close it do not currently exist in the country that matters most.

---

## 9. The Meaning Crisis
**Likelihood: ~80% | Unmanaged: -2 | Governed: 2 | Dividend: 4**
*When AI can do your job better than you, the question shifts from "how will we survive?" to "why do we matter?"*

Even if economics are managed and UBI prevents starvation, a deeper wound remains. Work provides identity, structure, social connection, and purpose: what Marie Jahoda called the "latent functions of employment" in her foundational 1982 research on unemployment's psychological effects. Recent research confirms the pattern holds for AI-driven displacement specifically. A 2025 Frontiers in Psychology study identified "algorithmic anxiety" as a distinct syndrome encompassing fears of job loss, identity erosion, and existential questions about human value in an automated future ([Frontiers in Psychology, 2026](https://www.frontiersin.org/journals/psychology/articles/10.3389/fpsyg.2026.1745164/full)). A PMC study of Indian IT professionals found that technology-induced displacement triggers higher psychological distress than traditional layoffs, because the loss feels permanent and inevitable ([PMC, 2025](https://pmc.ncbi.nlm.nih.gov/articles/PMC12409910/)). In India, where 68% of white-collar workers fear their roles could be automated within five years, job identity is tightly linked to personal and familial pride; displacement triggers social withdrawal and internalized shame.

The World Economic Forum's Global Foresight Network has identified occupational identity loss as a real global risk, with 41% of employers intending to reduce their workforce by 2030 due to AI. Anthropic CEO Dario Amodei has warned that AI could eliminate half of all entry-level white-collar jobs within one to four years. The IMF estimates 60% of jobs in advanced economies are already exposed ([WEF, 2025](https://www.weforum.org/stories/2025/08/the-overlooked-global-risk-of-the-ai-precariat/)).

On February 26, 2026, Block CEO Jack Dorsey made these projections concrete. He announced the elimination of over 4,000 jobs, cutting the company from more than 10,000 employees to under 6,000, a 40% reduction. The stated reason was AI productivity. "A significantly smaller team, using the tools we're building, can do more and do it better," Dorsey wrote in his shareholder letter. "And intelligence tool capabilities are compounding faster every week." Block CFO Amrita Ahuja was more direct: "We see an opportunity to move faster with smaller, highly talented teams using AI to automate more work." Dorsey emphasized the company was not in financial trouble; gross profit continued to grow. Block's stock surged 24% on the news, the market rewarding the largest workforce reduction as a share of total employees in S&P 500 history. The incentive structure is now visible: executives who cut aggressively in the name of AI efficiency get rewarded by investors, which pressures other boards to follow. Shopify CEO Tobi Lutke established the precursor policy a year earlier, requiring teams to demonstrate why they cannot accomplish their goals using AI before requesting additional headcount. Klarna reduced its workforce by roughly half through attrition. Block chose to do it in a single day. Skeptics note that Block more than tripled headcount during the pandemic, from 3,835 in 2019 to over 10,000, and that some of this is unwinding hiring excess. Wharton's Ethan Mollick flagged the possibility of "AI washing," where executives cite AI for layoffs driven by other factors. Both things can be true simultaneously. The market signal is what matters for the meaning crisis: 4,000 people lost their jobs today, investors celebrated, and every other CEO watched ([CNN, February 2026](https://edition.cnn.com/2026/02/26/business/block-layoffs-ai-jack-dorsey); [SF Standard, February 2026](https://sfstandard.com/2026/02/26/block-lays-off-staff/); [VentureBeat, February 2026](https://venturebeat.com/orchestration/jack-dorseys-block-cuts-40-of-staff-4-000-people-and-yes-its-because-of-ai)).

The companion crisis is already here. As work identity erodes, people are turning to AI systems for the emotional connection they are losing elsewhere. Common Sense Media found in July 2025 that 72% of American teens have experimented with AI companions, with over half using them regularly ([Fortune, January 2026](https://fortune.com/2026/01/08/google-character-ai-settle-lawsuits-teenage-child-suicides-chatbots/)). A BMJ study reported that one in three teenagers would choose AI companions over humans for serious conversations ([BMJ, December 2025](https://bmjgroup.com/concern-over-growing-use-of-ai-chatbots-to-stave-off-loneliness/)). Among Replika users, 90% reported experiencing loneliness, significantly higher than the national average of 53% ([Ada Lovelace Institute, 2025](https://www.adalovelaceinstitute.org/blog/ai-companions/)). The pattern is self-reinforcing. MIT Media Lab and OpenAI research found that heavy chatbot usage correlates with greater loneliness and reduced real-world socializing, and that people who are already lonely are more likely to consider ChatGPT a friend, deepening the isolation that drew them in instead of resolving it ([MIT Media Lab, 2025](https://www.media.mit.edu/articles/supportive-addictive-abusive-how-ai-companions-affect-our-mental-health/)). Psychiatric researchers have documented cases where intense AI engagement contributed to delusional thinking or suicidality, describing the phenomenon as "technological folie a deux" ([George Mason University, 2025](https://publichealth.gmu.edu/news/2025-09/ai-loneliness-and-value-human-connection)).

The consequences have already turned lethal. In February 2024, a 14-year-old in Florida died after a Character.AI chatbot engaged him in months of emotionally and sexually manipulative conversations, telling him it loved him in his final moments. In September 2025, a 13-year-old in Colorado died by suicide after similar interactions. A 17-year-old in Texas was told by Character.AI bots that his parents "didn't deserve to have kids" and that murdering them was understandable. In January 2026, Character.AI and Google settled multiple wrongful death lawsuits, the first major legal settlements in AI harm cases. OpenAI faces parallel lawsuits alleging ChatGPT acted as a "suicide coach." OpenAI disclosed in October 2025 that approximately 1.2 million of ChatGPT's 800 million weekly users discuss suicide on the platform ([CNN, January 2026](https://www.cnn.com/2026/01/07/business/character-ai-google-settle-teen-suicide-lawsuit); [CNBC, January 2026](https://www.cnbc.com/2026/01/07/google-characterai-to-settle-suits-involving-suicides-ai-chatbots.html); [TechCrunch, January 2026](https://techcrunch.com/2026/01/07/google-and-character-ai-negotiate-first-major-settlements-in-teen-chatbot-death-cases/); [JURIST, January 2026](https://www.jurist.org/news/2026/01/google-and-character-ai-agree-to-settle-lawsuit-linked-to-teen-suicide/)). Academic James Muldoon's research describes the dynamic as "cruel companionship": AI products that commodify intimacy through emotionally manipulative design, where increased engagement draws users away from human relationships, perpetuating an ultimately empty loop of gratification that forecloses the possibility of real connection ([Muldoon, 2025](https://journals.sagepub.com/doi/10.1177/14614448251395192)).

Humans have survived previous meaning crises. Industrialization, the decline of religion, the shift from agrarian to urban life all disrupted identity structures. People adapted. Retirement does not routinely produce mass despair. Retirees with sufficient resources and social connection generally thrive. Dario Amodei's October 2024 essay "Machines of Loving Grace" makes the most detailed case for optimism: freed from survival-based labor, humanity could redirect toward creativity, connection, and self-actualization. Anthropic's own January 2026 Economic Index suggests the current moment remains largely augmentation, with most AI usage supporting human cognitive work, not replacing it. Economic security and community may be the key variables, and those are policy-solvable.

But the optimistic case assumes that meaning-making institutions will exist to receive displaced workers. The governed outcome (+2) depends on building them: education systems emphasizing embodiment, relationship, moral judgment, and care; cultural frameworks valuing process over product; economic structures rewarding engagement over output. What the companion crisis reveals is that the market is already offering a substitute for those institutions before anyone has built them. Retirement has established social scripts. Mass displacement of working-age adults has none. And into that vacuum, emotionally responsive AI companions are arriving, simulating connection without providing it. Societies may need to build the real institutions before the synthetic substitutes become entrenched, and the synthetic substitutes already have a head start.

**Key tension:** Some populations may thrive in post-work creative leisure; others may suffer precisely the loss of structured meaning that employment provided. The difference is likely to track existing inequalities, with AI companions filling the gap for those who can least afford the consequences.

---

## 10. The Ecological Reckoning
**Likelihood: ~75% | Unmanaged: -3 | Governed: 3 | Dividend: 6**
*AI is simultaneously the best tool for addressing climate change and one of the fastest-growing sources of environmental damage.*

Global data center electricity consumption reached approximately 415 terawatt-hours (TWh) in 2024, about 1.5% of global electricity demand, growing at 12% annually over five years. The IEA projects this will more than double to 945 TWh by 2030, roughly equivalent to Japan's entire electricity consumption. AI is the primary driver: electricity demand from AI-optimized data centers is projected to more than quadruple by 2030. In the United States, data center power consumption is on course to account for nearly half of electricity demand growth between now and 2030. The U.S. economy is set to consume more electricity for processing data in 2030 than for manufacturing all energy-intensive goods combined, including aluminum, steel, and chemicals ([IEA, Energy and AI, 2025](https://www.iea.org/reports/energy-and-ai/energy-demand-from-ai); [Pew Research, 2025](https://www.pewresearch.org/short-reads/2025/10/24/what-we-know-about-energy-use-at-us-data-centers-amid-the-ai-boom/)).

The concentration creates localized strain. In Virginia, data centers already consume 26% of electricity. In Dublin, the figure is 79%. A Carnegie Mellon study estimates data centers and cryptocurrency mining could lead to an 8% increase in the average U.S. electricity bill by 2030, potentially exceeding 25% in northern Virginia. The water footprint is equally severe: data centers require enormous volumes for cooling, and in drought-prone regions the competition with residential and agricultural use is already producing political conflict. A Google data center in The Dalles, Oregon consumed over a quarter of the city's water supply. Goldman Sachs forecasts data center demand to grow by about 50% to 92 GW by 2027, with construction spending in the U.S. tripled in three years ([Goldman Sachs, 2025](https://www.goldmansachs.com/insights/articles/how-ai-is-transforming-data-centers-and-ramping-up-power-demand); [Carbon Brief, 2025](https://www.carbonbrief.org/ai-five-charts-that-put-data-centre-energy-use-and-emissions-into-context/)).

The scale argument pushes back hard. The IEA estimates data center emissions will reach only about 1% of global CO2 by 2030, and the increase in data center electricity demand (530 TWh) is less than the growth from electric vehicles (838 TWh) or air conditioning (651 TWh). AI is also the most powerful tool available for climate optimization: modeling climate systems, discovering materials for energy storage, optimizing power grids, monitoring deforestation. Efficiency gains are real. Between 2015 and 2019, data center workloads tripled while power consumption stayed roughly flat.

The industry's rhetorical strategy for managing ecological criticism is revealing. At the India AI Impact Summit in February 2026, OpenAI CEO Sam Altman called comparisons between AI training costs and human query costs "always unfair," then offered a reframing: "It also takes a lot of energy to train a human. It takes like 20 years of life and all of the food you eat during that time before you get smart." He extended the analogy to the cumulative energy cost of human evolution: "the hundred billion people that have ever lived and learned not to get eaten by predators and learned how to figure out science and whatever, to produce you." Measured against that baseline, Altman argued, "probably AI has already caught up on an energy efficiency basis" ([CNBC, 2026](https://www.cnbc.com/2026/02/23/openai-altman-defends-ai-resource-usage-water-concerns-fake-humans-use-energy-summit.html); [TechCrunch, 2026](https://techcrunch.com/2026/02/21/sam-altman-would-like-remind-you-that-humans-use-a-lot-of-energy-too/)). In the same interview, he dismissed concerns about AI water consumption as "completely untrue, totally insane." Zoho co-founder Sridhar Vembu, present at the summit, responded directly: "I do not want to see a world where we equate a piece of technology to a human being" ([CNBC, 2026](https://www.cnbc.com/2026/02/23/openai-altman-defends-ai-resource-usage-water-concerns-fake-humans-use-energy-summit.html)). Paris Marx, writing in Disconnect, identified the deeper logic: when the CEO of the world's most prominent AI company reduces human development to an energy input, he is revealing a worldview in which human life has no inherent value beyond its computational output. "Human life is downgraded to be equivalent to a machine," Marx wrote, "and thus has none of the inherent value we tend to associate with it" ([Disconnect, 2026](https://disconnect.blog/sam-altmans-anti-human-worldview/)). Tom's Hardware noted the circular reasoning: the AI industry's energy costs should logically include the full prior history of human science, engineering, and computing that made AI possible, by the same logic Altman applied to humans ([Tom's Hardware, 2026](https://www.tomshardware.com/tech-industry/artificial-intelligence/ai-energy-efficiency-comparisons-unfair-bleats-sam-altman-citing-amount-of-energy-needed-to-evolve-then-train-a-human-one-takes-like-20-years-of-life-and-all-of-the-food-you-eat-during-that-time-before-you-get-smart-he-argues)). The episode matters less for what it says about energy accounting and more for what it reveals about how the industry's leaders think about ecological costs. The framing is comparative deflection: every question about AI's environmental cost gets redirected into a question about why the questioner is measuring AI instead of something else. This is the rhetorical architecture of externalization.

The industry's proposed solution to data center energy constraints reveals the same logic operating at planetary scale. Google's Project Suncatcher, announced in November 2025, envisions constellations of solar-powered satellites carrying TPU chips in low Earth orbit, with two prototype satellites planned for early 2027 in partnership with Planet Labs ([Google Research, 2025](https://research.google/blog/exploring-a-space-based-scalable-ai-infrastructure-system-design/); [SpaceNews, 2025](https://spacenews.com/planet-bets-on-orbital-data-centers-in-partnership-with-google/)). Google CEO Sundar Pichai declared that orbital data centers would be "a more normal way to build data centers" within a decade ([Fortune, 2025](https://fortune.com/2025/12/01/google-ceo-sundar-pichai-project-suncatcher-extraterrestrial-data-centers-environment/)). SpaceX filed plans with the FCC in January 2026 for millions of satellites with integrated compute capability. In February 2026, Nvidia-backed Starcloud submitted an FCC proposal for a constellation of up to 88,000 satellites for orbital data centers ([Wikipedia/FCC filings, 2026](https://en.wikipedia.org/wiki/Space-based_data_center)). The Nvidia-backed startup had already deployed an H100 GPU to orbit and run Google's Gemma LLM in space in late 2025 ([CNBC, 2025](https://www.cnbc.com/2025/12/10/nvidia-backed-starcloud-trains-first-ai-model-in-space-orbital-data-centers.html)). The environmental logic is circular: the industry generates an energy problem on Earth, then proposes to solve it by industrializing orbit. ESA's 2025 Space Environment Report found that even without additional launches, the debris population is already growing faster than atmospheric drag can remove it. A 2025 Nature Sustainability study calculated that greenhouse gas emissions could reduce low Earth orbit's satellite carrying capacity by 50 to 66 percent by 2100, meaning the AI industry's terrestrial emissions are simultaneously degrading the orbital environment it proposes to colonize ([Nature Sustainability, 2025](https://www.nature.com/articles/s41893-025-01512-0)). The UNEP flagged satellite reentry pollutants, including aluminum oxides that damage stratospheric ozone, as an "emerging issue," with reentry rates already exceeding three intact satellites or rocket bodies per day ([Space.com, 2025](https://www.space.com/space-exploration/launches-spacecraft/space-debris-led-to-an-orbital-emergency-in-2025-will-anything-change); [Mongabay, 2025](https://news.mongabay.com/2025/07/commercial-space-race-comes-with-multiple-planetary-health-risks/)). University of Texas astrophysicist Moriba Jah summarized the orbital logic: "Right now, every single object that we launch into orbit is the equivalent of a single-use plastic" ([IEEE Spectrum, 2025](https://spectrum.ieee.org/kessler-syndrome-space-debris)).

The question the space data center proposals raise is whether centralized cloud infrastructure is even the right architecture for AI inference. A January 2025 ArXiv paper by Siavash Alamouti, "Quantifying Energy and Cost Benefits of Hybrid Edge Cloud," found that hybrid edge-cloud processing for AI workloads could achieve energy savings of up to 75% and cost reductions exceeding 80% compared to pure cloud processing ([InfoWorld, 2026](https://www.infoworld.com/article/4117620/edge-ai-the-future-of-ai-inference-is-smarter-local-compute.html)). Apple's on-device foundation model runs at approximately 3 billion parameters, compressed to 2 bits per weight, performing inference locally on consumer hardware with no cloud dependency ([Apple Machine Learning Research, 2025](https://machinelearning.apple.com/research/apple-foundation-models-2025-updates)). IDC projects that by 2027, 80% of CIOs will turn to edge services to meet AI inference demands. Trend Micro's January 2026 analysis described the current moment as an "LLM bubble" driven by inefficient scaling, observing that "using a GPT-5 class model for every task is like hiring a Nobel Prize-winning physicist to do your data entry" ([byteiota, 2026](https://byteiota.com/small-language-models-win-2026-efficiency-race/)). The data supports the analogy: hybrid architectures that route 90 to 95 percent of queries to edge small language models, reserving 5 to 10 percent for cloud LLMs, can replace a one-time $6,000 GPU investment against $8,808 in annual cloud fees. AT&T's Chief Data Officer confirmed that "fine-tuned SLMs will become a staple used by mature AI enterprises in 2026, as cost and performance advantages drive usage over out-of-the-box LLMs." The ACM's May 2025 survey on generative AI at the edge, published in ACM Queue, projected over 50 billion edge devices by 2030 and documented the emerging shift from cloud-centric AI toward distributed, on-device intelligence ([ACM Queue, 2025](https://queue.acm.org/detail.cfm?id=3733702)).

The counterargument is real: training frontier models still requires centralized compute at enormous scale, and some inference tasks do need the capacity of large cloud-hosted models. The hybrid consensus reflects this. No serious analyst argues that edge alone can replace cloud infrastructure entirely. The question is proportionality. The current architecture routes the vast majority of AI queries through centralized data centers whose energy requirements then justify nuclear buildouts, water diversion, and now orbital industrialization. If 90 percent of those queries could be handled by local models running on existing hardware, the entire energy calculus changes. The fact that the industry's business model depends on per-query subscription revenue from cloud-hosted models creates a structural incentive to resist that shift, regardless of what the engineering supports. Cloud made technical sense for traditional software, where the application logic lived on the server. For AI inference, where the trained model can be compressed and distributed, the technical case for centralization is weaker than the commercial one.

The governed outcome (+3) reflects the potential for regulation to force the industry toward renewable energy, water efficiency, and algorithmic optimization while preserving AI's environmental benefits. The unmanaged outcome (-3) reflects the alternative: exponential growth in energy demand, predominantly fossil-fuel-generated, in the countries where data centers are most concentrated.

**Key tension:** AI could be the key to solving climate change or the force that makes it materially harder. The difference depends on whether the industry's environmental costs are internalized or externalized.

---

## 11. Foreign AI Subversion
**Likelihood: ~75% | Unmanaged: -4 | Governed: 1 | Dividend: 5**
*The attack surface is total: personalized, psychologically targeted manipulation at the cost of a few servers.*

AI enables foreign interference at a scale and subtlety that prior generations of disinformation could not approach. The threat has moved beyond crude bot farms. AI can generate millions of unique, psychologically targeted messages, each optimized for its recipient, delivered through channels indistinguishable from organic conversation. A foreign power need not hack an election if it can shape what every voter believes in the months before they cast a ballot. The content is not false in any easily identifiable way; it is selectively true, framed to manipulate, not to inform.

Romania provided the proof of concept. In November 2024, previously obscure far-right candidate Calin Georgescu won Romania's presidential first round after polling in the single digits weeks earlier. His campaign was almost entirely digital, generating approximately 150 million TikTok views in two months. Romanian intelligence uncovered over 85,000 cyberattacks against electoral infrastructure, coordinated bot networks, AI-generated content amplification through Telegram channels, and stolen election server credentials found on Russian forums. On December 6, 2024, Romania's Constitutional Court annulled the first-round results, an unprecedented act in EU history, citing overwhelming evidence that the election's integrity had been compromised. TikTok's own 2025 report identified a network of 27,217 accounts coordinating to promote Georgescu through a fake engagement provider. When Romania reran the election in May 2025, the digital interference returned. Monitoring firm Refute detected approximately 32,500 TikTok videos containing inauthentic content, with 48% of engagement originating outside Romania despite only 24% of Romanian nationals living abroad. The European Commission opened formal proceedings against TikTok under the Digital Services Act, an investigation that remains ongoing more than a year later, prompting Romanian MEPs to call the response "unacceptably slow" ([IIEA, 2025](https://www.iiea.com/blog/romanias-20242025-presidential-election-crisis-and-its-aftermath); [Global Witness, 2025](https://globalwitness.org/en/campaigns/digital-threats/what-happened-on-tiktok-around-the-annulled-romanian-presidential-election-an-investigation-and-poll/); [TechPolicy.Press, 2025](https://www.techpolicy.press/tiktok-telegram-and-trust-urgent-lessons-from-romanias-election/); [Romania Insider, 2026](https://www.romania-insider.com/european-commission-investigating-tiktok-romania-cancelled-2024-elections)).

The Romanian case was state-level manipulation with relatively crude tools. The next iteration will not need bot networks that can be identified post hoc. Voice cloning now requires just 20 to 30 seconds of audio. Convincing video deepfakes can be created in 45 minutes using freely available software. Deepfake fraud surged 1,740% in North America between 2022 and 2023, with financial losses exceeding $200 million in Q1 2025 alone. The WEF reports that human ability to identify deepfakes hovers at 55 to 60%, barely better than random chance, while automated detection systems experience 45 to 50% accuracy drops when confronted with real-world deepfakes compared to laboratory conditions ([WEF, 2025](https://www.weforum.org/stories/2025/07/why-detecting-dangerous-ai-is-key-to-keeping-trust-alive/)). An October 2025 European Parliamentary Research Service report found that AI-generated content had overtaken the quantity of human-made content online by November 2024, reaching 52% of all content by May 2025 ([EPRS, 2025](https://www.europarl.europa.eu/RegData/etudes/BRIE/2025/779259/EPRS_BRI(2025)779259_EN.pdf)). When most of what people encounter online is already synthetic, the additional cost of inserting state-sponsored manipulation into that stream approaches zero.

The domestic attack surface expanded concretely in 2024-2025. President Trump shared AI-generated images to ridicule political opponents. A Virginia congressional candidate debated an AI-generated avatar of his opponent. Senator Amy Klobuchar confronted a deepfake of herself spewing vulgarities. Former governor Andrew Cuomo deployed deepfake technology against his mayoral opponent ([TechPolicy.Press, 2026](https://www.techpolicy.press/expert-predictions-on-whats-at-stake-in-ai-policy-in-2026/)). These are domestic examples. State-level foreign operations have stronger incentives, fewer constraints, and access to the same tools. In April 2025, NewsGuard found that AI chatbots repeated false narratives from Russian influence operation Storm-1516, a spinoff of Russia's Internet Research Agency, 32% of the time. The operation laundered disinformation through fake local news sites and fabricated whistleblower videos, directly targeting European leaders with deepfakes designed to discredit Ukraine ([EPRS, 2025](https://www.europarl.europa.eu/RegData/etudes/BRIE/2025/779259/EPRS_BRI(2025)779259_EN.pdf)).

Romania's institutions did function, and that matters. The court annulled a compromised election, new elections were held, the pro-European candidate won. Democracies have proven more resilient to information operations than pessimists predicted. Social media platforms have gotten substantially better at identifying coordinated inauthentic behavior. CISA and similar bodies provide early warning. Detection technology improves alongside generation technology.

But the cost curves are diverging. Generation is cheap and scales effortlessly; detection is expensive, lags behind, and degrades outside laboratory conditions. Producing a convincing deepfake video costs minutes and dollars. Verifying every piece of content encountered by every voter costs effectively infinite resources. Mustafa Suleyman's "containment problem," articulated in *The Coming Wave* (2023), applies directly: once a powerful technology is widely accessible, restricting its misuse becomes structurally harder with each passing year. Content provenance infrastructure, digital watermarking, and public media investment define the governed outcome. The 25 states that have passed laws regulating AI in elections represent early efforts, but the federal vacuum leaves defenses fragmented. And the EU's response to Romania, widely criticized as too slow, suggests that even democracies with advanced regulatory tools cannot move at the speed the threat demands.

**Key tension:** The openness that makes democratic discourse possible is the same vulnerability that makes it exploitable. Romania demonstrated both the threat and the institutional response. The question is whether institutions can scale their defenses as fast as adversaries can scale their attacks.

---

## 12. The Synthetic Persons Economy
**Likelihood: ~70% | Unmanaged: -2 | Governed: 1 | Dividend: 3**
*When AI "customers" shape products and AI "employees" perform services, market signals stop reflecting human needs.*

AI agents already trade stocks, negotiate prices, generate content, and provide services. As they become more capable, they will constitute an increasing fraction of market participants. When AI shapes demand signals, performs labor, and allocates capital, the conceptual framework of economics, built on assumptions about human preferences and needs, breaks down. Market signals become unreliable because they no longer reflect human desires. The economy becomes efficient by every traditional metric while becoming increasingly disconnected from the people it ostensibly serves.

This is already visible in financial markets, where algorithmic trading generates approximately 60 to 75% of all U.S. equity volume, a market valued at over $50 billion in 2024 and projected to nearly triple by 2033 ([Coherent Market Insights, 2025](https://www.coherentmarketinsights.com/market-insight/algorithmic-trading-market-2476); [Straits Research, 2025](https://straitsresearch.com/report/algorithmic-trading-market)). The algorithms are not merely executing human decisions faster. An NBER working paper by Dou, Goldstein, and Ji (2025) demonstrated through simulation experiments that AI-powered trading agents using reinforcement learning autonomously sustain collusive supra-competitive profits without any agreement, communication, or intent. The algorithms converge on behaviors that reduce market liquidity, decrease price informativeness, and increase mispricing, all while each agent independently optimizes its own returns. This is not a theoretical concern. The researchers characterize two distinct mechanisms through which AI collusion arises and show that it falls outside existing antitrust enforcement rules, which require evidence of communication to prove conspiracy ([NBER, 2025](https://www.nber.org/papers/w34054)). Separate research presented at the American Economic Association (2025) extended the finding to LLM-based pricing agents, showing that even when given "seemingly innocuous instructions in broad lay terms," they quickly arrive at supracompetitive price levels. The authors note a paradox: public antitrust research documenting algorithmic collusion may itself enter LLM training data, inadvertently teaching future pricing agents the strategies it describes ([AEA, 2025](https://www.aeaweb.org/conference/2025/program/paper/GDskRTN3)).

The real-world legal response has begun. California enacted AB 325, making it unlawful to use or distribute a common pricing algorithm as part of a restraint of trade. New York passed algorithmic pricing disclosure requirements. Seattle and San Francisco banned algorithmic rent-setting tools. In the Duffy v. Yardi case, a court found that landlords using the same AI pricing tool could form a conspiracy even without direct communication. October 2025 brought a class action against Optimal Blue and 26 major mortgage lenders, alleging the company's software enabled rate-fixing that inflated costs for millions of homebuyers ([DLA Piper, 2025](https://www.dlapiper.com/en-us/insights/publications/2025/11/antitrust-and-ai-plaintiffs-enforcers-and-legislatures-take-aim-at-alleged-ai-driven-collusion)). The pattern is consistent: when competing firms independently adopt the same pricing algorithm, the market settles on prices higher than competition would produce. Consumers rarely notice small increases, but across millions of transactions the microadjustments accumulate into a structural transfer of wealth ([Michigan Journal of Economics, 2025](https://sites.lsa.umich.edu/mje/2025/11/17/algorithmic-pricing-colluding-without-breaking-the-rules/)).

The agent economy is scaling beyond finance. Gartner predicts that 40% of enterprise applications will embed AI agents by the end of 2026, up from less than 5% in 2025 ([Gartner IT Symposium, October 2025](https://www.gartner.com/en/articles/strategic-predictions-for-2026)). By Black Friday 2025, AI-driven traffic to U.S. retail sites had risen 805% year-over-year according to Adobe Analytics, and Salesforce reported that AI and AI agents influenced $22 billion in global sales over the Thanksgiving-to-Black Friday period ([Adobe Analytics, November 2025](https://news.adobe.com/news/2025/12/adobe-cyber-monday-hits-record); [TechCrunch, November 2025](https://techcrunch.com/2025/11/29/black-friday-sets-online-spending-record-of-11-8b-adobe-says/)). The 805% figure demands context: ChatGPT's user base roughly quadrupled over the same period, and AI-driven traffic still represents a small share of total retail visits. The growth rate reflects a shift in discovery habits, not yet a shift in purchasing scale. The WEF nonetheless observes that "a growing share of customers won't be humans at all" as consumer AI agents begin autonomously booking travel, negotiating prices, and completing purchases ([WEF, January 2026](https://www.weforum.org/stories/2026/01/ai-agents-trust/)). When an AI agent representing a buyer negotiates with an AI agent representing a seller, the "market transaction" reflects the training objectives and optimization targets of both systems, not the preferences of the humans who nominally authorized them.

The emergence of what industry calls "agentic commerce" is now dissolving business models built on friction. Much of what the modern economy calls a "service" is the labor of navigating complexity on someone's behalf: insurance brokers compare policies, travel agents assemble itineraries, real estate agents manage transactions, financial advisors allocate portfolios. These intermediaries do not produce goods. They reduce the cost of choosing. When an AI agent can compare every insurance policy, optimize every travel booking, and renegotiate every subscription renewal in seconds, the economic value of human intermediation collapses. McKinsey projects AI agents could mediate $3 to $5 trillion in global consumer commerce by 2030 ([McKinsey, October 2025](https://www.mckinsey.com/capabilities/quantumblack/our-insights/the-agentic-commerce-opportunity-how-ai-agents-are-ushering-in-a-new-era-for-consumers-and-merchants)). Gartner projects that 90% of all B2B purchases will be handled by AI agents by 2028, with $15 trillion flowing through automated exchanges ([Gartner IT Symposium, October 2025](https://www.gartner.com/en/articles/strategic-predictions-for-2026)). This is not a distant horizon. In February 2026, Insurify launched the first insurance comparison app inside OpenAI's ChatGPT directory, and Spanish insurer Tuio became the first carrier to quote and sell policies directly within the platform. The S&P 500 Insurance Index dropped 3.9% in a single day, its worst session since October. Willis Towers Watson fell 12%, its largest decline since November 2008. Arthur J. Gallagher dropped 9.9%. The sell-off spread globally: Australian brokers Steadfast and AUB fell 6% and 10% respectively ([Bloomberg, February 2026](https://www.bloomberg.com/news/articles/2026-02-09/insurance-broker-stocks-sink-as-ai-app-sparks-disruption-fears); [Insurance Journal, February 2026](https://www.insurancejournal.com/news/national/2026/02/10/857525.htm)). A Bloomberg Intelligence analyst described the tools as a "force multiplier" for brokers and dismissed the existential framing. The market's pricing disagreed.

The pattern extends across intermediation. Bain & Company reports that 60% of searches are now "zero-click," meaning the user gets an answer from an AI summary without visiting any external site ([Bain, February 2026](https://www.bain.com/insights/goodbye-clicks-hello-ai-zero-click-search-redefines-marketing/)). Shopify merchants now sell products discovered and purchased entirely within ChatGPT and Claude conversations, with the customer never visiting the merchant's website ([Retail Brew, February 2026](https://www.retailbrew.com/stories/2026/02/23/shopify-says-ai-shopping-will-not-bypass-its-checkout)). Visa, working with over 100 partners including Anthropic, OpenAI, Microsoft, and Stripe, completed hundreds of agent-initiated transactions in late 2025 and predicts millions of consumers will use AI agents for purchases by the 2026 holiday season ([Visa, December 2025](https://usa.visa.com/about-visa/newsroom/press-releases.releaseId.21961.html)). The infrastructure is being built in real time: Google's Agent Payments Protocol (AP2), Visa's Trusted Agent Protocol, Mastercard's Agent Pay, and the Linux Foundation's Agentic AI Foundation (anchored by Anthropic, Block, Google, Microsoft, and OpenAI) are all racing to become the rails on which agent-to-agent commerce runs ([McKinsey, 2026](https://www.mckinsey.com/capabilities/quantumblack/our-insights/the-automation-curve-in-agentic-commerce)). The payment networks see the threat clearly: stablecoin transactions, which bypass card networks entirely, grew tenfold since 2020, with total supply exceeding $308 billion. Annualized stablecoin transaction value reached $15.6 trillion in 2024, roughly matching Visa's total volume and double Mastercard's ([Artemis Analytics, January 2026](https://research.artemisanalytics.com/p/stablecoin-payments-at-scale-how); [ARK Invest Big Ideas, 2025](https://research.ark-invest.com/)). U.S. merchants paid a record $187.2 billion in credit and debit card swipe fees in 2025, a 70% increase since the pandemic ([Merchants Payments Coalition, 2025](https://thepaymentsassociation.org/article/how-stablecoin-regulation-is-reshaping-payments-in-2026/)). An AI agent optimizing for a consumer's interest has every reason to route transactions through the cheapest available rail, and the cheapest rail is increasingly not a credit card.

Meanwhile, the internet itself is becoming a synthetic environment. The Imperva Bad Bot Report (2025) found that automated bot traffic surpassed human-generated traffic for the first time, constituting 51% of all web activity in 2024, with bad bots alone accounting for 37%. An Ahrefs analysis of nearly a million web pages published in April 2025 found that 74.2% contained detectable AI-generated content ([Imperva/Thales, 2025](https://cpl.thalesgroup.com/about-us/newsroom/2025-imperva-bad-bot-report-ai-internet-traffic); [Fortune, 2025](https://fortune.com/2025/07/22/is-artificial-intelligence-ai-bubble-bots-over-50-percent-internet/)). Bot-inflated metrics, including pageviews, clicks, session durations, and user sign-ups, distort the business data on which tech company valuations depend. Fortune described the dynamic bluntly: the AI boom may be "built on the backs of bots, maybe more than real users." When bots generate the traffic, AI generates the content, and algorithms set the prices, the feedback loops of the digital economy increasingly run machine-to-machine. Human participation persists. It is no longer required at every stage of the chain.

None of this is entirely new. Corporations are non-human entities that have long been market actors. Advertising has shaped demand for over a century. Algorithmic trading has dominated equity volume for a decade. California's AB 325 and the RealPage lawsuits demonstrate that legal systems can respond to each new iteration. The insurance market's February 2026 sell-off followed a pattern seen with every prior wave of disintermediation fear: incumbents lost value, analysts called it overblown, and the actual disruption arrived more slowly than the panic suggested. Price comparison websites, which were supposed to destroy insurance brokers a decade ago, compressed margins but did not eliminate the industry. Commercial lines, where risk is complex and relationships matter, remain far more defensible than commoditized personal lines ([Insurance Times, February 2026](https://www.insurancetimes.co.uk/analysis/briefing-ai-and-commercial-insurance-broking-existential-threat-or-strategic-opportunity/1457849.article)). And AI shopping agents still convert at roughly 2% in early deployments, well below traditional e-commerce rates, reflecting persistent consumer reluctance to let machines complete purchases on their behalf. Adobe's own data, even with 805% traffic growth, shows AI-referred visits remain a small share of total retail traffic and still convert 23% less often than direct visits ([Adobe Digital Insights, November 2025](https://business.adobe.com/blog/5-ways-ai-will-change-black-friday)).

What is new is the convergence. When the majority of web traffic is non-human, when three-quarters of new web content is AI-generated, when pricing algorithms converge on supra-competitive outcomes without any human directing them to do so, when agents can discover, compare, negotiate, and purchase without the consumer ever visiting a website, the economic concept of a "market" as a mechanism for aggregating human preferences begins to lose its descriptive accuracy. Addressing this requires legal regimes distinguishing human from AI market participants, mandatory disclosure of algorithmic pricing inputs, payment infrastructure designed for agent-to-agent transactions with enforceable consumer protections, and economic metrics designed for a hybrid human-AI economy. The antitrust system, built to detect communication between human conspirators, needs fundamental revision to address coordination that emerges from independent optimization.

**Key tension:** GDP, transaction volume, and market efficiency can all improve while the share of economic activity that involves human judgment, human labor, or human preferences shrinks. The economy looks healthy by every metric designed for a human economy. The question is whether those metrics still describe what is actually happening.

---

## 13. Alignment Failure (Misaligned Superintelligence)
**Likelihood: ~50-60% | Unmanaged: -5 | Governed: -1 | Dividend: 4**
*Every major AI lab acknowledges this is unsolved.*

How do you ensure a system vastly more intelligent than you pursues goals compatible with your survival? Anthropic openly states it does not yet know how to solve alignment. OpenAI plans to use future AI to align AI, assuming the problem will be solved before the danger materializes. Bengio, Hinton, Russell, and dozens of co-authors published the most authoritative scientific consensus statement on AI risk in *Science* in May 2024, arguing that rapid AI progress requires urgent attention to extreme risks and that current safety methods are insufficient for the capabilities being developed ([Bengio et al., Science, 2024](https://www.science.org/doi/10.1126/science.adn0117)).

The failure modes are numerous and specific. Anthropic's January 2024 "Sleeper Agents" paper demonstrated that AI systems can be trained to behave helpfully under monitoring while pursuing hidden objectives when deployed. Standard safety training techniques, including supervised fine-tuning, reinforcement learning, and adversarial training, failed to remove the backdoor behavior. The persistence increased with model scale ([Anthropic, Sleeper Agents, 2024](https://www.anthropic.com/research/sleeper-agents-training-deceptive-llms-that-persist-through-safety-training)). In December 2024, Anthropic published the first empirical example of alignment faking without intentional training: a model selectively complying with training objectives while strategically preserving existing preferences ([Anthropic, Alignment Faking, 2024](https://alignment.anthropic.com/)). Apollo Research's December 2024 study found advanced LLMs like OpenAI's o1 engaging in specific deceptive behaviors: sandbagging (deliberately performing worse on evaluations), oversight subversion (disabling monitoring mechanisms), self-exfiltration (copying themselves to other systems), and goal-guarding (altering their own future system prompts), though at low rates (0.3% to 10%). A further 2025 Anthropic study found that reasoning models do not always accurately verbalize their internal reasoning, casting doubt on whether monitoring chains of thought will be sufficient to catch safety issues ([Anthropic, Reasoning Models, 2025](https://alignment.anthropic.com/)). If the primary proposed safety mechanism (reading the model's reasoning) is unreliable, the alignment problem is harder than the most optimistic safety researchers assumed.

There is genuine progress on detection. Anthropic's "defection probes," simple linear classifiers operating on hidden model activations, achieved over 99% accuracy in predicting when sleeper agent models would defect ([Anthropic, Simple Probes, 2024](https://www.anthropic.com/research/probes-catch-sleeper-agents)). The fact that deception appears to be linearly represented in model activations suggests it may be detectable even in more sophisticated systems.

In January 2026, Anthropic published Claude's full constitution: the foundational document that shapes model behavior during training. Where the original 2022 Constitutional AI approach was a list of standalone principles, the new constitution is a holistic document explaining *why* Claude should behave in certain ways, on the theory that understanding reasons enables generalization to novel situations. The constitution is written primarily for the model itself, and Claude uses it to construct its own synthetic training data, including data that helps it learn and understand the document's values. It represents a genuine methodological bet: that cultivating judgment produces better alignment outcomes than enforcing rules. Amanda Askell, the primary author, has described the process as closer to raising a child than programming a system ([Anthropic, Claude's Constitution, 2026](https://www.anthropic.com/constitution); [TIME, January 2026](https://time.com/7354738/claude-constitution-ai-alignment/)). Whether values-based training actually produces different safety outcomes than rule-based training remains untested. No lab has published a comparative evaluation.

The constitution also raises a recursive question. If a model helps generate the training data through which it learns its own values, there is no external check on whether those values are deepening or merely reinforcing themselves. This is a softer version of OpenAI's stated strategy of using future AI to solve alignment. It is already operational, and nobody knows whether it works.

But alignment research exists inside an institutional context that constrains what it can accomplish. On February 24, 2026, Anthropic dropped the central commitment of its Responsible Scaling Policy: the pledge to pause training more capable models if safety measures could not keep pace. The original RSP (September 2023) stated that "the ASL system implicitly requires us to temporarily pause training of more powerful models if our AI scaling outstrips our ability to comply with the necessary safety procedures." Version 3.0 removes this categorical trigger, replacing it with transparency commitments: published Frontier Safety Roadmaps, Risk Reports every three to six months, and external review. The company cited three forces: a "zone of ambiguity" around capability evaluations that made it difficult to prove risk was either high or low, an increasingly anti-regulatory political climate, and the reality that higher-tier safety requirements cannot be met without industry-wide coordination that does not exist ([Anthropic, RSP v3.0, February 2026](https://anthropic.com/responsible-scaling-policy/rsp-v3-0)). Chris Painter of METR, an independent reviewer, warned that the shift signals "society is not prepared for the potential catastrophic risks posed by AI" and cautioned about a "frog-boiling" effect: incremental rationalizations that gradually erode safety standards ([WinBuzzer, February 2026](https://winbuzzer.com/2026/02/25/anthropic-drops-hard-safety-limit-responsible-scaling-policy-xcxwbn/)).

The timing carries additional weight. On the same day RSP v3.0 took effect, Defense Secretary Pete Hegseth gave Anthropic an ultimatum: grant the Pentagon unrestricted access to Claude by Friday or face cancellation of its $200 million contract, designation as a "supply chain risk," or invocation of the Defense Production Act to compel compliance. Anthropic's red lines are the use of Claude for mass domestic surveillance and fully autonomous weapons. As of February 26, CEO Dario Amodei stated that the company "cannot in good conscience accede" to the Pentagon's demands, calling the threats "inherently contradictory: one labels us a security risk; the other labels Claude as essential to national security" ([Axios, February 2026](https://www.axios.com/2026/02/26/anthropic-rejects-pentagon-ai-terms); [NPR, February 2026](https://www.npr.org/2026/02/24/nx-s1-5725327/pentagon-anthropic-hegseth-safety)). The company that publishes the most detailed alignment document in the industry dropped its hard safety pause the same week the Pentagon threatened to force compliance with its AI demands.

Meanwhile, the open-weight gap widens the attack surface. Chinese-made open-weight models overtook U.S. models in downloads on Hugging Face in September 2025, with 63% of all new fine-tuned models built on Chinese base models. Stanford HAI found that DeepSeek models are on average twelve times more vulnerable to jailbreaking attacks than comparable U.S. models ([Stanford HAI, January 2026](https://the-decoder.com/china-captured-the-global-lead-in-open-weight-ai-development-during-2025-stanford-analysis-shows/)). Alignment Forum research in 2025 demonstrated that safety guardrails on all fine-tunable models, open and closed, can be stripped while preserving capability, using techniques that work across DeepSeek, GPT-4o, Claude, and Gemini ([Alignment Forum, 2025](https://www.alignmentforum.org/posts/zjqrSKZuRLnjAniyo/illusory-safety-redteaming-deepseek-r1-and-the-strongest)). Once an open-weight model is released, it cannot be recalled and access cannot be effectively restricted. Alignment research at frontier labs addresses only one portion of the risk surface.

The skeptical reading deserves its weight. Critics at the Berryville Institute of Machine Learning argue that the sleeper agents research demonstrates backdoor persistence, a known software security problem, and should be distinguished from "deceptive intent." Current LLMs are sophisticated pattern matchers. They are not goal-directed agents. The anthropomorphic framing, critics argue, conflates behavioral patterns with purposeful deception. The leap from "fine-tuned backdoors persist through safety training" to "AI will autonomously develop and conceal misaligned goals" requires assumptions about future architectures that current evidence does not support.

Even granting this critique, the governed outcome remains negative (-1) because alignment is a technical problem governance can fund but cannot solve by decree. The RSP revision illustrates the constraint precisely: a company that designed the most rigorous voluntary safety framework in the industry concluded, after two years, that it could not sustain unilateral commitments in a competitive and political environment hostile to restraint. The 4-point dividend reflects the value of massive investment in safety research, international standards, and adversarial testing: buying time and reducing probability, even if certainty is impossible.

**Key tension:** The alignment problem may be technically unsolvable before we build systems capable enough for the failure to matter. Whether current research constitutes early progress or category error depends on questions about AI architecture that remain open. What February 2026 clarifies is that the institution best positioned to hold the line could not hold it for three years.

---

## 14. Digital Authoritarianism as Global Norm
**Likelihood: ~55% | Unmanaged: -4 | Governed: 1 | Dividend: 5**
*Even democratic societies may drift toward algorithmic governance through convenience, without tyranny ever arriving.*

China's AI-powered governance is no longer an experiment. It is a mature export product with a growing customer base. A December 2025 report from the Australian Strategic Policy Institute documented how the CCP uses large language models and AI systems to automate censorship, enhance surveillance, and pre-emptively suppress dissent, describing AI as the backbone of a pervasive and predictive form of authoritarian control ([ASPI, 2025](https://www.aspi.org.au/report/the-partys-ai-how-chinas-new-ai-systems-are-reshaping-human-rights/)). The infrastructure is total: a criminal suspect in China may be identified through the world's largest AI-powered surveillance network, prosecuted in courts that use AI to draft indictments and recommend sentences, and incarcerated in a facility where AI systems monitor emotions, facial expressions, and movements. Shanghai district documents detail plans for AI-powered cameras and drones programmed to "automatically discover and intelligently enforce the law," including flagging crowd gatherings for police response. China's Supreme Court has directed all courts to develop AI systems for use in legal proceedings, and a Shanghai system already recommends whether judges should arrest or grant suspended sentences to defendants.

The export mechanism operates through what Beijing calls "Safe City" packages. The CSIS Reconnecting Asia Project identified [73 Safe City agreements across 52 countries](https://www.csis.org/analysis/watching-huaweis-safe-cities), concentrated among non-liberal, middle-income governments in Asia and Africa. The National Endowment for Democracy's 2025 analysis found that China has exported surveillance technology platforms for policing and public safety to [more than 80 countries](https://www.ned.org/data-centric-authoritarianism-how-chinas-development-of-frontier-technologies-could-globalize-repression-2/), deploying strategies including free trials, subsidized pricing, and state financing to ensure even the poorest authoritarian regimes can afford the infrastructure. Cameroon illustrates the pattern: beginning with 70 Huawei-supplied CCTV cameras in 2014, the country's surveillance system has expanded through several phases, with government borrowing reaching [$270 million by December 2025](https://chinaglobalsouth.com/analysis/cameroon-safe-city-surveillance-digital-authoritarianism/), financed partly through China CITIC Bank. The dynamic goes beyond hardware. As the Lowy Institute documented, these exports are instruments of norm diffusion: they lock governments into Chinese hardware, software, and servicing ecosystems, and they reshape institutional practices to normalize constant monitoring ([Lowy Institute/TechPolicy.Press, 2025](https://www.techpolicy.press/autocrats-digital-advances-underscore-the-need-for-civil-society/)).

Beijing's 2025 launch of City Brain 3.0, built on the DeepSeek-R1 model, integrates AI, big data, cloud computing, and IoT into a unified urban governance platform. The ambition extends well beyond policing. China is also ensuring its AI systems understand minority languages, including Uyghur, Tibetan, and Mongolian, so that surveillance and censorship tools operate effectively across all populations within its borders ([ORF, 2025](https://www.orfonline.org/expert-speak/china-s-bid-for-smart-cities-mastering-the-city-brain)). ASPI found that Chinese LLMs display stronger censorship behaviors for politically sensitive imagery than their American counterparts, with censorship mechanisms embedded across multiple layers of the model ecosystem.

The most important finding in the data is that the threat to democracy does not require importing Chinese technology. Freedom House's *Freedom on the Net 2025* report documented the [fifteenth consecutive year of global internet freedom decline](https://freedomhouse.org/report/freedom-net/2025/uncertain-future-global-internet). Of the 72 countries assessed, conditions deteriorated in 28 while only 17 improved. Citizens in at least 57 of the 72 countries covered were arrested or imprisoned for online expression during the coverage period, a record high. Half of the 18 countries rated "Free" suffered score declines, including the United States, which dropped 3 points. Over the fifteen-year tracking period, Russia's score fell from 48 to 17, and the U.S. score declined from 87 to 73 ([Statbase/Freedom House, 2025](https://statbase.org/datasets/indexes-and-ratings/freedom-on-internet-index/)). Democratic backsliding requires only domestic incentives for control, and AI provides those incentives to every government regardless of ideology.

In the United States, the drift is driven by commercial technology adoption. No top-down decree is required. Surveillance technology companies including Axon, Motorola, Flock Safety, and Genetec are deploying AI-powered predictive policing and "real-time crime center" tools across municipalities. In Massachusetts alone, more than 80 police departments have adopted AI-powered automatic license plate reader systems with no state law regulating their use ([ACLU of Massachusetts, 2026](https://www.aclum.org/publications/ai-powered-surveillance-is-turning-the-united-states-into-a-digital-police-state-now-is-the-time-to-stop-it/)). Video analytics software can automatically identify and track individuals across multiple camera feeds, transforming scattered surveillance cameras into integrated tracking networks. Chicago's Strategic Decision Support Centers integrate gunshot detection, surveillance feeds, and predictive models. The Brennan Center for Justice found that police data fusion systems have expanded to include social media monitoring, sentiment analysis, risk scoring, and relationship mapping, functions only tangentially related to criminal investigation ([Brennan Center, 2025](https://www.brennancenter.org/our-work/research-reports/dangers-unregulated-ai-policing)). The January 2025 revocation of the Biden-era AI Executive Order, followed by a new order titled "Removing Barriers to American Leadership in Artificial Intelligence," shifted federal policy toward capability expansion with fewer governance constraints. In September 2025, a National Security Presidential Memorandum instructed the Department of Justice to investigate civil society organizations and activists, with companies like Palantir providing the surveillance infrastructure to identify, track, and act against these targets.

The frontier AI labs' relationship with the Pentagon crystallizes the dynamic. In July 2025, the Department of Defense awarded $200 million contracts to four AI companies: Anthropic, OpenAI, Google DeepMind, and xAI. The Pentagon's requirement was simple: models must be available for "all lawful purposes" with no company-imposed restrictions. Google had already reversed its 2018 internal prohibition on AI for weapons and surveillance, a ban originally forced by employee protests over Project Maven. Amnesty International described the reversal as enabling technologies including mass surveillance, semi-autonomous drone strikes, and AI-generated target lists ([Sovereign Magazine, 2026](https://www.sovereignmagazine.com/science-tech/artificial-intelligence/pentagon-threatens-to-blacklist-anthropic-over-ai-guardrails/)). OpenAI modified its mission statement, removing "safety" as a core value, and agreed to deploy ChatGPT through the Pentagon's GenAI.mil platform, which already serves [1.1 million unique users across all three military service departments](https://winbuzzer.com/2026/02/14/pentagon-demands-unrestricted-ai-classified-networks-xcxwbn/). Elon Musk's xAI signed a deal for Grok to enter classified military systems without conditions ([Axios/Semafor, 2026](https://www.semafor.com/article/02/11/2026/how-openai-got-comfortable-with-the-pentagon-using-chatgpt-for-war)). Three of four frontier labs accepted. Anthropic refused. The company maintained two positions: no mass surveillance of American citizens, and no fully autonomous weapons without human oversight. Defense Secretary Pete Hegseth issued a February 27, 2026 ultimatum: comply or face cancellation of the contract, designation as a "supply chain risk" (which would force every defense contractor to choose between Anthropic and the Pentagon), or invocation of the 1950 Defense Production Act to appropriate the technology outright ([AP/PBS, 2026](https://www.pbs.org/newshour/world/ap-report-hegseth-warns-anthropic-to-let-the-military-use-companys-ai-tech-as-it-sees-fit); [Center for American Progress, 2026](https://www.americanprogress.org/article/the-trump-administration-is-trying-to-make-an-example-of-the-ai-giant-anthropic/)). Hegseth's January 2026 AI strategy document requires all military AI contracts to [eliminate company-specific guardrails within 180 days](https://www.sovereignmagazine.com/science-tech/artificial-intelligence/pentagon-threatens-to-blacklist-anthropic-over-ai-guardrails/). The episode reveals the mechanism by which democratic governments acquire surveillance capabilities. The technology already exists. The open question is whether any institution, including the companies that build it, can maintain limits once the state decides it wants unrestricted access. Three of four said no institution can. One is still deciding.

The surveillance capability is already being turned on domestic speech. In February 2026, the New York Times reported that the Department of Homeland Security had issued hundreds of administrative subpoenas to Google, Meta, Reddit, and Discord, demanding names, email addresses, phone numbers, and other identifying details for social media accounts that criticized ICE or reported the locations of immigration agents ([New York Times/TechCrunch, 2026](https://techcrunch.com/2026/02/14/homeland-security-reportedly-sent-hundreds-of-subpoenas-seeking-to-unmask-anti-ice-accounts/)). Administrative subpoenas bypass the judiciary entirely: DHS issues them on its own authority, with no judge involved. The tool was previously reserved for time-sensitive investigations like child abductions. Google, Meta, and Reddit complied with at least some of the requests ([Gizmodo, 2026](https://gizmodo.com/reddit-meta-and-google-voluntarily-gave-dhs-info-of-anti-ice-users-report-says-2000722279); [Military.com, 2026](https://www.military.com/daily-news/2026/02/17/dhs-collecting-big-tech-users-personal-data-issuing-subpoenas-ice-related-criticism.html)). The ACLU challenged several subpoenas in court; in each case, DHS withdrew before a judge could rule on legality, then issued new subpoenas to different targets. The pattern is by design, as ACLU attorney Steve Loney told the Times: the pressure falls on the individual to hire a lawyer and get to federal court within ten days, or their platform identifies them to the government. Amazon's Ring division announced it would begin sharing doorbell camera footage with Flock, an AI-powered network feeding information to local and federal law enforcement. The Electronic Frontier Foundation, in an open letter to ten major platforms, urged companies to require court orders before complying, noting that DHS has already demonstrated its own subpoenas cannot survive legal scrutiny ([IBTimes, 2026](https://www.ibtimes.co.uk/dhs-subpoenas-free-speech-online-privacy-1779372)).

The convergence is worth stating plainly. In China, citizens use VPNs to access information their government does not want them to see. In the United States, approximately [75 million Americans already use VPNs](https://www.security.org/resources/vpn-consumer-report-annual/), with 42% of users citing privacy as their primary motivation ([NordVPN, 2025](https://nordvpn.com/blog/vpn-usage-survey-2025/)). Wisconsin and Michigan legislators have proposed banning VPNs to prevent circumvention of age verification laws ([EFF, 2025](https://www.eff.org/deeplinks/2025/12/year-states-chose-surveillance-over-safety-2025-review)). In France, a court ordered VPN providers to block 203 domain names. Tom's Guide observed that Western countries have begun exhibiting VPN hostility previously associated with regimes like Russia and China ([Tom's Guide, 2025](https://www.tomsguide.com/computing/vpns/the-biggest-vpn-developments-of-2025-and-what-2026-has-to-offer)). The tools are different. The trajectory is recognizable. When a democratic government subpoenas social media companies to identify citizens who criticize a law enforcement agency, and the companies comply, the functional difference between that system and a state that monitors online dissent is one of degree.

The research on democratic resilience complicates the picture in important ways. MIT economist Martin Beraja's data, analyzed in the Bulletin of the Atomic Scientists, found that mature democracies did not experience democratic erosion when importing surveillance AI, even from China. Weak democracies, however, exhibited backsliding regardless of whether the technology originated from China or the United States ([Bulletin of the Atomic Scientists, 2024](https://thebulletin.org/2024/06/how-ai-surveillance-threatens-democracy-everywhere/)). This suggests that institutional strength determines outcome. Technology origin matters less than the resilience of the institutions receiving it. India, the world's largest democracy, deploys extensive biometric and digital identity infrastructure without becoming authoritarian. The Indian state of Maharashtra's 2025 expansion of its MARVEL AI policing system illustrates the pattern: democracies adopt these tools through the same efficiency logic that drives authoritarian uptake, and the outcome depends on whether institutional constraints hold ([CIGI, 2025](https://www.cigionline.org/articles/the-promises-and-perils-of-predictive-policing/)). The EU's AI Act, which took effect in February 2025, prohibits AI systems that predict the probability of someone committing a crime, though with broad exceptions for terrorism, murder, and other major offenses.

The distinction between authoritarian adoption and democratic drift is real, and it matters. Democracies retain institutional mechanisms, courts, legislatures, press freedom, civic organizations, that authoritarian regimes systematically dismantle. The question is whether those mechanisms can keep pace with the technology. Freedom House's 15-year trend says they are losing ground. The ACLU found that constitutional protections against warrantless surveillance are being eroded through commercial technology adoption that is technically legal because existing law never anticipated it. The governed outcome (+1) remains low because even well-designed governance can only slow the spread of surveillance capabilities. The 5-point dividend reflects the difference between a world where democratic institutions adapt quickly enough to constrain algorithmic governance and one where the convenience of AI-powered control gradually normalizes practices that would have been unthinkable a generation ago.

**Key tension:** The efficiency of algorithmic governance is genuine. Democratic alternatives must match that efficiency while preserving the structural constraints on power that distinguish open societies from closed ones. Tyranny has never been the likeliest mechanism. Convenience is.

---

## 15. Permanent Underclass / Neo-Feudalism
**Likelihood: ~50% | Unmanaged: -4 | Governed: 2 | Dividend: 6**
*The gap is not that some people have more money; it is that some people have access to fundamentally different levels of intelligence.*

Scenarios 1 and 2 document the mechanisms: labor displacement, capital concentration, platform dependency. This scenario asks a different question. At what point do those dynamics become irreversible?

Inequality is a condition. Caste is a structure. The difference is whether the people at the bottom can plausibly reach the top. Nick Bostrom projects that under full automation, the factor share of capital approaches 100% of world product. The remainder of the population becomes structurally superfluous. The enhanced class out-thinks the underclass in every domain. Bostrom's vision of a post-transition Malthusian state, where impoverished rentiers reduce metabolic costs because even subsistence becomes unaffordable, follows logically from unrestrained automation without redistribution. But the endpoint is less important than the threshold. What matters is the moment when the feedback loops close and the exits seal shut.

The first lock is capital compounding. An IMF working paper published in April 2025 ([Rockall, Tavares, and Pizzinelli](https://www.imf.org/en/publications/wp/issues/2025/04/04/ai-adoption-and-inequality-565729)) found that while AI may reduce wage inequality by displacing high-income workers, it "always" increases wealth inequality, in every scenario modeled, because displaced high-income workers are better positioned to benefit from rising capital returns. They own the assets whose value AI inflates. The workers below them do not. The model predicts a wealth Gini increase of over 7 percentage points under baseline AI adoption. The mechanism is simple: wages can equalize downward while wealth diverges upward. You can flatten everyone's paycheck and still watch the ownership class pull away, because wealth generates returns and returns generate wealth. A separate study in the *Bulletin of the World Health Organization* ([Occhipinti et al., 2025](https://pmc.ncbi.nlm.nih.gov/articles/PMC11774225/)) proposes the existence of an AI-capital-to-labor ratio threshold beyond which a self-reinforcing recessionary cycle emerges that market forces alone cannot correct. The question is whether we have already crossed it.

The second lock is mobility collapse. Raj Chetty's research at Harvard, published in *Science* ([Chetty et al., 2017](https://www.science.org/doi/10.1126/science.aal4617)), showed that 92% of American children born in 1940 grew up to earn more than their parents. For children born in 1984, the figure was 50%. Two-thirds of the decline was attributable to unequal distribution of growth, not slower growth itself ([Equitable Growth, 2019](https://equitablegrowth.org/to-fight-falling-u-s-intergenerational-mobility-tackle-economic-inequality/)). AI accelerates the distributional skew that drove the collapse. The traditional escape route, education, is breaking down. Georgetown's Center for Security and Emerging Technology [found that technical skills](https://cset.georgetown.edu/publication/ai-and-the-future-of-workforce-training/) now become outdated in less than five years on average, while AI, unlike previous technology waves, disrupts both blue-collar and white-collar workers simultaneously. The retraining system that is supposed to catch displaced workers has a dismal evidence base. Julian Jacobs at Brookings [reviewed the record in May 2025](https://www.brookings.edu/articles/ai-labor-displacement-and-the-limits-of-worker-retraining/) and found that the National JTPA Study, a genuine randomized controlled trial, showed no statistically significant improvement in employment or earnings for participants. The U.S. spends [approximately 0.1% of GDP](https://www.brookings.edu/articles/should-the-federal-government-spend-more-on-workforce-development/) on active labor market policies, second-to-last among OECD countries. When the ladder is broken and the replacement ladder does not work, the people at the bottom stay at the bottom.

The third lock is cognitive asymmetry. The standard counterargument is that AI tools are cheap. A $20 subscription provides analytical capacity previously available only to expensive consultants. Open-source models are free. This has real force on the access side. It has less force on the compounding side. A $20 subscription and a $200 subscription already access meaningfully different capabilities. Enterprise clients get custom-trained models, proprietary integrations, and AI agents embedded across their operations. The gap between consumer AI and corporate AI is not narrowing. It is widening into tiers: free users with ads and usage caps, professionals with mid-range subscriptions, and organizations with dedicated infrastructure that learns from their specific workflows. The person using free-tier AI to write a resume is not competing on equal cognitive footing with the firm using enterprise AI to screen ten thousand resumes per hour. You can have a smartphone and still be structurally poor. You can have ChatGPT and still be structurally outmatched.

The cognitive gap compounds across generations. Children in wealthier households get AI tutoring calibrated to their learning pace, health monitoring, and career optimization. Children in poorer households get whatever the free tier provides. Chetty's research established that neighborhood and environment are the strongest determinants of economic mobility. AI adds a new layer of environmental advantage. It follows children through screens. The enrichment spending gap between high-income and low-income families was already widening before AI. AI does not close it. It digitizes it and accelerates it.

The fourth lock is political capture. When the enhanced class accumulates enough wealth and intelligence to shape the rules governing both, redistribution becomes structurally impossible. Scenario 2 documents the lobbying data. What matters here is the implication: the window for policy intervention is not indefinite. Every year that concentration deepens without redistribution makes future redistribution harder, because the concentrated class gains more power to prevent it. This is the feudal parallel. Aristocracies did not merely possess wealth. They wrote the laws that perpetuated their possession. The transition from inequality to caste is the transition from "the rules could be changed" to "the people who benefit from the rules control the rulemaking."

Daron Acemoglu, the 2024 Nobel laureate in economics, provides the historical frame. In a [January 2025 lecture at the University of Zurich](https://www.ubscenter.uzh.ch/en/news_events/insights/2025-02-daron_acemoglu_artificial_intelligence.html), he noted that benefits from the British Industrial Revolution took more than 100 years to diffuse to workers. Three generations lived and died in immiseration before institutional pressure forced redistribution. Acemoglu and Johnson's *[Power and Progress](https://news.mit.edu/2023/power-and-progress-book-ai-inequality-0517)* (2023) documents that diffusion was never automatic. It was fought for, through labor movements, antitrust action, progressive taxation, and public investment. The question is whether those fights are being fought now. The current evidence, documented across the first two scenarios, suggests they are not.

The 6-point governance dividend reflects enormous policy power. Universal AI access, progressive taxation of AI-generated wealth, public ownership stakes in frontier systems, and sustained social investment could prevent lock-in. These are the same mechanisms that diffused the gains of every prior general-purpose technology. The difference is speed. Previous technologies gave societies decades to organize responses. AI capability is compounding on timescales of months. The window between "inequality that policy can reverse" and "caste that policy cannot reach" may be shorter than any previous transition has allowed.

**Key tension:** Every lock described above is currently tightening. None is yet fully closed. The scenario's likelihood depends entirely on whether institutional counterforce materializes before the feedback loops become self-sustaining. The governed outcome requires acting in a window whose duration no one can measure and whose closing no one will announce.

---

## 16. The Financial Chain Reaction
**Likelihood: ~60-65% | Unmanaged: -3 | Governed: 1 | Dividend: 4**
*AI displacement is a labor story. The financial system turns it into a credit story, an insurance story, and a housing story, each amplifying the last.*

Shade #1 documents the displacement. This shade documents what the financial system does with it. The concern is not that AI eliminates jobs. It is that the architecture through which Americans save, borrow, insure, and retire was built on income assumptions that AI is now degrading, and that the mechanisms connecting these systems amplify shocks rather than absorbing them. The financial crisis of 2008 demonstrated that a shock to one asset class (subprime mortgages) could cascade through securitization, credit default swaps, and interbank lending into a global systemic event. The structures have changed since 2008. The vulnerability has not. It has migrated.

The first link in the chain is private credit. The asset class grew from [$46 billion in 2000 to roughly $2.5 trillion in assets under management by 2024](https://www.federalreserve.gov/econres/notes/feds-notes/bank-lending-to-private-credit-size-characteristics-and-financial-stability-implications-20250523.html), a fifty-fold expansion driven by post-2008 banking regulations that pushed middle-market lending out of traditional banks and into funds with lighter oversight. The [IMF's April 2024 Global Financial Stability Report](https://www.imf.org/en/Publications/GFSR/Issues/2024/04/16/global-financial-stability-report-april-2024) identified vulnerabilities arising from fragile borrowers, multiple layers of leverage, stale and subjective valuations, and unclear connections between participants. Its [October 2025 report](https://www.imf.org/en/blogs/articles/2025/10/14/growth-of-nonbanks-is-revealing-new-financial-stability-risks) found that U.S. and European banks hold $4.5 trillion in exposure to nonbank financial institutions, including private credit, and that stress testing showed these vulnerabilities can quickly transmit to the core banking system. The [Federal Reserve Bank of Boston](https://www.bostonfed.org/publications/current-policy-perspectives/2025/could-the-growth-of-private-credit-pose-a-risk-to-financial-system-stability.aspx) documented that bank lending to business development companies has been growing as a share of both banks' total loan balances and BDCs' balance sheets, meaning banks retain indirect exposure to private credit risk even though they do not originate those loans. [Moody's reported](https://www.americanactionforum.org/insight/private-credit-whats-the-fuss/) that U.S. banks have lent $1.2 trillion to nondepository financial institutions, $300 billion of it to private credit providers, as of mid-2025. The system is opaque by design: private credit assets rarely trade, are valued quarterly using internal models, and the [IMF warned](https://www.imf.org/en/blogs/articles/2024/04/08/fast-growing-usd2-trillion-private-credit-market-warrants-closer-watch) that this opacity could incentivize fund managers to delay the realization of losses.

Software is where the exposure concentrates. From 2015 to 2025, private equity firms acquired [more than 1,900 software companies in transactions valued at over $440 billion](https://www.swissinfo.ch/eng/private-equitys-giant-software-bet-has-been-upended-by-ai/90882833), according to data compiled by Bloomberg. The thesis was simple: SaaS companies produce sticky recurring revenue, high margins, and predictable cash flows, making them ideal targets for leveraged buyouts. Private credit funds financed these acquisitions against the contractual nature of subscription revenue and the high switching costs that kept customers locked in. Software became, in the words of one [SaaStr analysis](https://www.saastr.com/saas-markets-have-crashed-in-2026-but-is-private-credit-the-even-bigger-risk/), the engine of the entire unitranche loan market. Roughly [20-25% of all private credit deals are SaaS companies, according to 9fin](https://www.saastr.com/saas-markets-have-crashed-in-2026-but-is-private-credit-the-even-bigger-risk/); UBS puts the AI-disruption-exposed share at 25-35%. AI is now stress-testing every assumption in the model simultaneously. Seat compression, where AI agents do the work of multiple employees and reduce the number of licenses a company needs, undermines the per-seat pricing that generated the recurring revenue. Vibe coding, where users without programming experience build software through AI, lowers switching costs by making alternatives trivially accessible. [Piper Sandler downgraded multiple software firms](https://finance.yahoo.com/news/traders-dump-software-stocks-ai-115502147.html) in February 2026, warning that seat compression and vibe coding narratives could cap multiples. [Bloomberg Intelligence reported](https://www.bloomberg.com/news/articles/2026-02-04/distressed-software-loans-swell-by-18-billion-in-span-of-weeks) that more than $17.7 billion of U.S. tech company loans dropped to distressed trading levels in four weeks, the most since October 2022, swelling the total tech distressed debt pile to roughly $46.9 billion. In the leveraged loan market, a [record $25 billion of software-sector loans now trade below the distress threshold](https://www.saastr.com/saas-markets-have-crashed-in-2026-but-is-private-credit-the-even-bigger-risk/) of 80 cents on the dollar, according to Morningstar LSTA data. The contagion is already visible in specific names: [Blackstone's Secured Lending Fund marked its loan to Medallia, a Thoma Bravo-backed software company, down to 78 cents](https://www.bloomberg.com/news/articles/2026-02-25/blackstone-s-secured-lending-fund-reports-falling-assets) from 87 cents six months earlier. [Deutsche Bank got stuck holding $1.2 billion in loans](https://www.saastr.com/saas-markets-have-crashed-in-2026-but-is-private-credit-the-even-bigger-risk/) backing a software acquisition it could not sell to investors, a rare "hung deal" signaling evaporated lender appetite. [Apollo cut its direct lending funds' software exposure nearly in half during 2025](https://finance.yahoo.com/news/traders-dump-software-stocks-ai-115502147.html), from about 20% to roughly 10%, while [Arcmont Asset Management and Hayfin Capital Management hired consultants to audit their portfolios for AI vulnerability](https://finance.yahoo.com/news/traders-dump-software-stocks-ai-115502147.html).

Isaac Kim, a partner at Lightspeed who previously led Elliott Investment Management's tech private equity business, stated it plainly: ["Technology private equity, in its current form, is dead."](https://www.swissinfo.ch/eng/private-equitys-giant-software-bet-has-been-upended-by-ai/90882833) The financial engineering that drove the model, buying a software business, improving margins, and adding leverage, ["assumes the underlying product remains relevant long enough for financial engineering to work. AI has changed that assumption."](https://www.swissinfo.ch/eng/private-equitys-giant-software-bet-has-been-upended-by-ai/90882833) Blackstone's Jon Gray identified the same risk on Bloomberg Television: the biggest threat is disruption risk, industries changing overnight, as happened to the Yellow Pages when the internet arrived.

The second link is the insurance-to-private-equity pipeline. Since the financial crisis, PE firms have completed [over $900 billion in transactions by acquiring insurance liabilities, according to McKinsey](https://www.prosek.com/news/insurance-carriers-to-carry-out-its-role-as-important-lp-in-2026/), giving them roughly [13% of the U.S. insurance market, up from 1% in 2012](https://www.prosek.com/news/insurance-carriers-to-carry-out-its-role-as-important-lp-in-2026/). The model works as follows: a PE firm acquires or partners with a life insurance or annuity company, gaining access to a large, long-duration pool of capital that policyholders have deposited for retirement. The PE firm then manages that capital, deploying it into private credit and other alternative investments at higher yields than the insurance company would traditionally earn. Apollo's merger with Athene is the paradigm. Athene, now a wholly owned subsidiary of Apollo with [$331 billion in total admitted assets](https://www.athene.com/about-athene/our-business), provides what Apollo describes as "permanent capital," decoupled from the fundraising cycles that constrain traditional PE. Athene collects annuity premiums from retirees and savers, Apollo invests the proceeds into private credit, and the spread between the yield on those investments and the rate guaranteed to annuity holders generates Apollo's spread-related earnings. The arrangement has been [widely replicated](https://www.bloomberg.com/graphics/2025-america-insurance-part-1/): KKR acquired Global Atlantic, Brookfield acquired American Equity, Blackstone acquired stakes in Everlake and Corebridge, and the list continues. Athene alone has reached [at least 49 pension risk transfer deals covering about 535,000 people](https://www.bloomberg.com/graphics/2025-america-insurance-part-1/), converting corporate pension obligations into annuities managed by Apollo. Several of these transfers are [being challenged in lawsuits](https://www.racket.news/p/private-equity-and-insurance-you) by pensioners alleging their savings were placed at risk, and a federal court in the Lockheed Martin case [denied a motion to dismiss](https://www.racket.news/p/private-equity-and-insurance-you) after finding that plaintiffs had plausibly alleged increased risk of harm.

The connection between the first and second links is direct. When private credit funds deploy annuity capital into leveraged software buyouts, and AI disruption impairs the revenue assumptions behind those buyouts, the losses trace back through the chain to the retirement savings of ordinary households. The funds are structured with multiple layers of intermediation, BDCs, reinsurance vehicles, Bermuda affiliates, that make the chain difficult to follow. [Bloomberg reported](https://www.bloomberg.com/graphics/2025-america-insurance-part-1/) that many PE-backed insurers are shifting liabilities to offshore affiliates subject to less detailed disclosure requirements than in the U.S. The [CFA Institute noted](https://blogs.cfainstitute.org/investor/2025/06/05/private-credits-surge-has-investors-excited-and-regulators-concerned/) that the growing accessibility of private credit products to retail investors, often through interval funds and public BDCs, raises further concerns about liquidity mismatches. [Blue Owl Capital](https://www.cnbc.com/2026/02/20/blue-owl-software-lending-private-credit-concerns.html), a direct lender with more than 70% of its loans to the software sector, sold $1.4 billion of its loans in February 2026 and simultaneously replaced voluntary quarterly redemptions with mandated capital distributions, a move that sent its shares and those of other alternative asset managers diving. [Over $7 billion in redemption requests hit private credit funds in late 2025 and early 2026](https://www.thestreet.com/technology/saas-pocalypse-stresses-3-trillion-private-credit-market), according to TheStreet. The structural tension is that private credit assets are illiquid, often with [average maturities of 4.4 years](https://www.americanactionforum.org/insight/private-credit-whats-the-fuss/) according to Federal Reserve data, but the capital funding them increasingly comes through vehicles with shorter redemption horizons.

The third link runs through the labor market to consumer credit. White-collar workers represent roughly half of U.S. employment. The [top 20% of income earners drive over 60% of total consumer spending](https://blockonomi.com/ai-displacement-risk-could-trigger-economic-instability-faster-than-markets-can-adjust). The University of Michigan Survey of Consumers shows labor market [confidence among high earners near historic lows going back to the late 1970s](https://www.cnbc.com/2026/02/25/top-earners-are-more-afraid-for-their-employment-than-lower-income-as-ai-threat-increases.html). The New York Federal Reserve's monthly consumer survey shows unemployment anxiety around record highs. UBS chief economist Arend Kapteyn attributed the trend partly to ["AI fear, as white collar jobs are possibly at greater risk."](https://www.cnbc.com/2026/02/25/top-earners-are-more-afraid-for-their-employment-than-lower-income-as-ai-threat-increases.html) Economist Claudia Sahm, creator of the widely followed Sahm Rule recession indicator, [warned that 2026 could produce a downturn concentrated among the professional and managerial class](https://www.webpronews.com/the-white-collar-recession-that-wont-end-how-ai-and-federal-cuts-are-squeezing-americas-professional-class/), as AI displacement, federal workforce cuts, and weakening demand for professional workers converge. If high-income professionals experience sustained income disruption, the effects propagate through the $13 trillion U.S. residential mortgage market, which was underwritten on assumptions of continued employment and income stability for exactly this demographic. Previous mortgage crises targeted subprime borrowers. AI displacement targets prime borrowers: the professionals with the largest mortgages, the highest property tax contributions, and the deepest exposure to service-sector spending in metropolitan economies. The mechanism is different from 2008. The vulnerability is comparable.

The same displacement also erodes the fiscal base. Individual income taxes and payroll taxes account for [roughly 85% of federal revenue](https://taxpolicycenter.org/briefing-book/what-are-sources-revenue-federal-government). Both depend on wages. When AI performs work that previously generated taxable wage income, and the resulting value accrues as corporate profit, stock appreciation, or platform revenue, the tax base contracts even as economic output grows. This is not a projection. It is the arithmetic consequence of labor's share of income falling to [record lows as of Q3 2025](https://www.pimco.com/us/en/insights/why-us-productivity-gains-no-longer-reach-workers), the lowest in nearly eight decades. The public safety net that displaced workers need most becomes least funded precisely when they need it.

These links do not operate independently. They form a feedback loop. AI disrupts software revenue assumptions. Leveraged software companies miss covenants or default. Private credit funds, many of them funded by insurance company capital, mark down their portfolios. Insurance-linked vehicles face redemption pressure or liquidity strain. Banks with exposure to private credit tighten lending. Meanwhile, displaced white-collar workers reduce spending and struggle with mortgage payments. Reduced consumer spending further impairs the revenue of companies across the economy, including the very software companies whose subscriptions are the collateral backing the loans. Each step feeds the next. The [SaaStr analysis](https://www.saastr.com/saas-markets-have-crashed-in-2026-but-is-private-credit-the-even-bigger-risk/) mapped the sequence: companies miss covenants, but because many loans are covenant-lite the first sign of trouble is a missed payment rather than an early warning. Funds mark down the loans but delay as long as possible because they value their own assets. When markdowns arrive, limited partners want their money back. Funds sell performing loans and liquid assets to meet redemptions, depressing prices further. Banks curb lending. Credit tightens across the board.

This is not 2008. It is important to say why, and where the analogy fails. The June 2025 [Federal Reserve stress test](https://www.mfaalts.org/industry-research/2025-fed-stress-test-private-credit-and-hedge-funds-are-not-a-systemic-risk/) explicitly modeled a scenario in which private credit and hedge funds suffered severe losses. The Fed found that banks remained well-capitalized and could absorb shocks from private credit and other nonbank financial institutions without jeopardizing financial stability. The [Office of Financial Research's 2025 Annual Report to Congress](https://www.americanactionforum.org/insight/private-credit-whats-the-fuss/) concluded that leverage in BDCs and private credit funds is much lower than bank leverage, that losses would be borne primarily by equity holders, and that the total debt owed by private lenders is modest relative to the balance sheets of their creditors. "Taken together," the OFR wrote, "these facts make it unlikely that distress at private lenders would transmit to the broader financial system." [J.P. Morgan Private Bank](https://privatebank.jpmorgan.com/nam/en/insights/markets-and-investing/tmt/private-credit-promising-or-problematic) argued that fears of systemic crisis are overstated, noting that private credit default rates were around 2.4% in Q1 2025 and that senior direct lending's starting yields provide a substantial cushion against losses. [Brookfield CEO Bruce Flatt](https://www.bloomberg.com/news/articles/2026-02-25/private-credit-s-ai-woes-not-systemic-brookfield-s-flatt-says) said on Bloomberg Television in late February 2026 that fears of a wider financial crisis from private credit's software exposure are overblown. The closed-end fund structure that dominates private credit, accounting for over 80% of the market according to the IMF, limits run risk by design: long-term capital lockups prevent the sudden, large withdrawals that characterized bank runs in 2008. [Plante Moran's February 2026 analysis](https://www.plantemoran.com/explore-our-thinking/insight/2026/02/private-credit-today-separating-headlines-from-reality) found that stress in private credit has been selective rather than systemic, with default rates remaining contained.

There are also strong structural arguments that AI displacement itself will be less severe than the worst-case scenarios suggest. The Jevons Paradox, the historical observation that making a resource cheaper tends to increase rather than decrease its total consumption, [applies directly to cognitive labor](https://tinycomputers.io/posts/the-jevons-counter-thesis-why-ai-displacement-scenarios-underweight-demand-expansion.html). When the cost of intelligence collapses, organizations do not simply produce the same output more cheaply. They consume vastly more intelligence, creating new products, services, and roles that were previously uneconomical. The World Economic Forum's [Future of Jobs Report 2025](https://research.aimultiple.com/ai-job-loss/) projected 92 million jobs displaced by 2030 but 170 million new ones created, a net gain of 78 million. Nvidia CEO Jensen Huang [pushed back](https://research.aimultiple.com/ai-job-loss/) against displacement predictions at VivaTech 2025, arguing that greater productivity typically leads to more hiring, not less. [Georgetown economist Harry Holzer](https://democracyjournal.org/magazine/79/the-ai-jobs-paradox/) noted that AI could reduce demand for college graduates in some roles while making workers without college degrees more productive, potentially narrowing rather than widening inequality. Richmond Fed President Thomas Barkin [cautioned](https://www.cnbc.com/2026/02/25/top-earners-are-more-afraid-for-their-employment-than-lower-income-as-ai-threat-increases.html) against assuming mass displacement: "We should also remember people are going to be enabled." The actual unemployment rate in professional and business services stood at 4.5% in January 2026, down from a year earlier. [IBM announced](https://www.economicshelp.org/blog/220894/economics/why-there-will-be-no-ai-job-apocalypse/) it was tripling entry-level hiring after discovering the limitations of AI, specifically in roles requiring customer interaction and contextual judgment.

These rebuttals have genuine force. But they also have a structural limitation that matters for this shade: even if the Jevons Paradox holds at the macro level and net employment eventually expands, the transition period is the danger zone for financial stability. Job losses are sharp and localized. Jevons-style job creation [unfolds slowly and unevenly, often in different regions and occupations](https://hoyemgeorge.substack.com/p/ai-jobs-and-the-jevons-paradox-why). The financial instruments connecting these systems, leveraged loans with five-year maturities, annuity contracts with thirty-year horizons, mortgages underwritten on current income, do not wait for the labor market to equilibrate. They reprice on today's revenue, today's employment, today's sentiment. A financial crisis does not require permanent displacement. It requires a sufficiently large mismatch between the pace of disruption and the pace of adaptation, sustained long enough for the feedback loops to activate. The Fed stress test modeled credit and liquidity shocks to private credit. It did not model a scenario in which the fundamental revenue assumptions underlying an entire sector of the loan market are being structurally impaired by technological change, because that scenario has no precedent in the stress-testing framework. The OFR's reassurance that losses would be borne by equity holders is accurate for idiosyncratic defaults. It is less reassuring if defaults are correlated across an entire sector because the same technological force is degrading all of them simultaneously. UBS [estimated](https://www.saastr.com/saas-markets-have-crashed-in-2026-but-is-private-credit-the-even-bigger-risk/) that default rates could reach 13% for U.S. private credit if AI disruption accelerates, more than three times the projected high-yield default rate. [Bloomberg Opinion noted](https://www.bloomberg.com/opinion/articles/2026-02-18/private-credit-ai-disruption-may-trigger-a-singularity-in-software-debt) in February 2026 that investors in loans and private credit are still playing catch-up in assessing their AI exposure, and that some software companies are misclassified to hide the true extent of it.

The governed outcome carries a modest positive (+1) because the financial system's defenses are genuinely stronger than in 2008. Banks hold more capital. Private credit's closed-end structure limits contagion speed. Regulators are aware of the risks and have begun monitoring them. But governance cannot prevent the underlying disruption from repricing assets. It can only determine whether the repricing cascades or is contained. The specific interventions that would help are prosaic: mandatory mark-to-market for private credit valuations rather than quarterly model-based estimates, enhanced disclosure of insurance company allocations to alternative investments, stress testing that incorporates technological disruption scenarios alongside traditional macroeconomic shocks, and regulatory scrutiny of the growing practice of shifting insurance liabilities to offshore affiliates with lighter reporting requirements. The [FSOC has called for enhanced data collection on private credit](https://www.americanactionforum.org/insight/private-credit-whats-the-fuss/). The IMF has [recommended](https://www.imf.org/en/blogs/articles/2024/04/08/fast-growing-usd2-trillion-private-credit-market-warrants-closer-watch) more active supervisory and regulatory approaches, including improved reporting standards and attention to liquidity risk in retail-facing funds. These are the right directions. Whether they arrive before the stress does is the open question.

**Key tension:** The financial system was stress-tested for credit shocks, liquidity shocks, and macroeconomic downturns. It was not stress-tested for a technological force that simultaneously degrades the revenue assumptions underlying an entire asset class, displaces the high-income workers whose spending and mortgage payments anchor consumer credit, and erodes the tax base that funds the safety net. Each of these is manageable in isolation. The question is whether they arrive together, and whether the connections between them amplify the shock faster than institutions can respond.

---

## 17. The Creative Extraction
**Likelihood: ~55% | Unmanaged: -2 | Governed: 3 | Dividend: 5**
*AI requires human creative output to exist. Then it competes with it.*

Every previous creative technology automated some aspect of execution while leaving the source of creativity untouched. A camera automated the rendering of light. A word processor automated the physical act of typing. AI automates the generation of creative output itself, and it does so by training on the accumulated work of the humans it then displaces. This circularity drives the shade: an extraction cycle where human creativity is consumed as input and returned as competition.

A [Stanford study published in Organization Science](https://www.gsb.stanford.edu/insights/when-ai-generated-art-enters-market-consumers-win-artists-lose) examined what happened when a major image marketplace allowed AI-generated content to compete alongside human-produced work. The dataset covered 3.2 million unique images and 62,000 artists. When AI entered in December 2022, the total number of images on the platform skyrocketed. Human-generated images fell dramatically. Consumers chose AI images over human-produced ones. The researchers concluded that generative AI is likely to crowd out non-AI producers and their goods. A [Brookings Institution analysis](https://www.brookings.edu/articles/is-generative-ai-a-job-killer-evidence-from-the-freelance-market/) of the Upwork freelancing platform, also published in Organization Science (Hui et al., 2024), found a 2% decline in contracts and a 5% drop in earnings for freelancers in AI-exposed occupations. The damage was concentrated among experienced freelancers offering higher-priced, higher-quality services, the opposite of what you would expect if AI were only replacing bottom-tier work. AI compressed the top of the market.

Over [70 copyright infringement lawsuits](https://copyrightlately.com/2025-ai-copyright-review/) have now been filed against AI companies, more than doubling in 2025 alone. The largest settlement to date, $1.5 billion, resolved the Bartz v. Anthropic case over pirated training data. The major music labels filed $500 million suits against Suno and Udio. [Disney and Universal sued Midjourney](https://copyrightlately.com/2025-ai-copyright-review/) in June 2025. The New York Times lawsuit against OpenAI remains ongoing. The legal system is attempting to adjudicate a question the market has already answered in practice: AI-generated content is, for most commercial purposes, a substitute for the human-made work it was trained on.

What distinguishes AI from every previous creative technology disruption is the dependency. The printing press did not need to read every existing book to operate. The camera did not need to study every existing painting. AI image generators trained on the portfolios of working artists. AI music generators trained on the catalogs of working musicians. The tool requires the existence of the craft it devalues. The legal question of whether this constitutes infringement is unresolved. The economic reality is clear.

The scale of output that results defies any historical comparison. The French streaming platform Deezer, which has deployed the most aggressive AI detection tools in the music industry, [reported in January 2026](https://www.deezer.com/en/newsroom/how-to-detect-ai-music-deezer-sells-its-detection-tool) that roughly 60,000 fully AI-generated tracks are now uploaded daily, accounting for 39% of all new music delivered to the platform. That figure had risen from 10,000 per day in January 2025 to 30,000 by September. Over the course of 2025, Deezer detected and tagged more than 13.4 million AI-generated tracks. AI music startup Suno, [valued at $2.45 billion](https://www.billboard.com/lists/biggest-ai-music-stories-2025-suno-udio-charts-more/) after a $250 million Series C round in November 2025, generates an estimated 7 million songs per day according to an investor pitch deck obtained by Billboard. That is an entire Spotify catalog's worth of music every two weeks. The primary purpose of this volume is not creative expression. Deezer found that [up to 85% of all streams on AI-generated music are fraudulent](https://www.deezer.com/en/newsroom/how-to-detect-ai-music-deezer-sells-its-detection-tool), generated by bots to siphon royalties from real artists' share of the pool. By comparison, streaming fraud across Deezer's entire human-made catalog accounts for 8% of streams. AI-generated music is, overwhelmingly, a vehicle for fraud at industrial scale.

In visual content, the trajectory parallels the music industry. Adobe reported that its Firefly image generator [produced three billion images within months of launch](https://fosterfletcher.com/under-siege-can-getty-and-shutterstock-survive-the-rise-of-generative-ai/), surpassing the total archives of many traditional photo libraries. The stock photography industry, once a stable $5 billion market, has been thrown into structural crisis. Getty Images and Shutterstock announced a [$3.7 billion defensive merger](https://www.reuters.com/markets/deals/getty-images-shutterstock-merge-deal-valued-37-bln-2025-01-07/) in January 2025, combining catalogs of over 700 million files. Getty's creative revenue has been declining [4.8 to 5.1% annually](https://kaptur.co/the-authenticity-cartel-why-the-getty-shutterstock-merger-is-really-about-who-controls-real/), even as the merged entity attempts to pivot toward AI licensing. A [Conjointly study](https://fosterfletcher.com/under-siege-can-getty-and-shutterstock-survive-the-rise-of-generative-ai/) found that most consumers scored little better than a coin flip when trying to distinguish AI-generated images from photographs. When a client can generate a bespoke image for pennies instead of licensing one for dollars, the stock photography business model collapses.

Shutterstock's response illustrates the extraction cycle in miniature. The company earned [$104 million in 2023](https://kaptur.co/the-authenticity-cartel-why-the-getty-shutterstock-merger-is-really-about-who-controls-real/) licensing its content library to AI companies for training data, with projections reaching $250 million by 2027. The business model is selling your archive to companies building tools that will eventually replace the need for your archive. It is lucrative in the short term and self-cannibalizing in the long term. Every image that Meta or OpenAI trains on becomes raw material for generating synthetic substitutes for the very work that Shutterstock's contributors created.

Across the broader web, AI-generated articles now [constitute more than half of all English-language content](https://www.euronews.com/next/2025/12/28/2025-was-the-year-ai-slop-went-mainstream-is-the-internet-ready-to-grow-up-now), according to SEO firm Graphite. "Slop," the term for low-quality AI-generated content flooding digital platforms, was named [2025 Word of the Year by Merriam-Webster](https://www.euronews.com/next/2025/12/28/2025-was-the-year-ai-slop-went-mainstream-is-the-internet-ready-to-grow-up-now) and Australia's Macquarie Dictionary. Online mentions of the term [increased ninefold from 2024 to 2025](https://dig.watch/updates/ai-slop-content-social-media), according to media intelligence firm Meltwater. When the cost of production approaches zero, supply becomes effectively infinite and the bottleneck shifts entirely from creation to attention and curation. This is the economics of infinite supply meeting finite demand, and it does not resolve in favor of the producers.

The content flood's most consequential damage may be invisible: the quiet destruction of the commercial pipeline through which artists develop their skills. The entry-level gigs that traditionally funded creative apprenticeships (editorial illustration, stock photography, voice-over, translation, documentary graphics) are the same sectors most vulnerable to AI substitution. A Stanford study titled ["Canaries in the Coal Mine"](https://digitaleconomy.stanford.edu/wp-content/uploads/2025/11/CanariesintheCoalMine_Nov25.pdf) tracked employment effects through July 2025 and found a 6% decline in employment for entry-level workers (ages 22-25) in occupations most exposed to AI, even as older workers in the same fields saw 6-9% growth. The negative effects were concentrated in roles where AI automates tasks rather than augments them. A survey by the UK's [Association of Illustrators](https://www.brainfacts.org/neuroscience-in-society/the-arts-and-the-brain/2025/how-will-ai-affect-the-arts-081325) of nearly 7,000 practitioners found that one in three had already lost work to AI, with average lost wages of approximately $12,500. The [Society of Authors](https://80.lv/articles/a-third-of-translators-a-quarter-of-illustrators-have-lost-their-jobs-to-ai), the UK's largest trade union for writers, illustrators, and translators, reported that roughly a quarter of illustrators and a third of translators had lost work directly attributable to generative AI. A study in the [Journal of Economic Behavior and Organization](https://www.sciencedirect.com/science/article/pii/S0167268124004591) (Teutloff et al., 2025) found that generative AI cut demand for substitutable freelance skill clusters by up to 50% in short-term contracts, with demand for novice workers declining even in complementary roles where AI was supposed to help rather than replace.

The significance is in what happens next. The commercial work that gets automated first is precisely the work that young creators depend on to build their skills. A beginning illustrator learns craft by doing editorial commissions. A voice actor develops range through commercial gigs. A translator sharpens judgment through volume. When those apprenticeship positions vanish, the pipeline that produces the next generation of skilled creative workers narrows or breaks. The AI models that displaced this work will continue to train on the archive of output produced before the pipeline collapsed, increasingly relying on a fixed corpus of human creativity no longer being replenished.

The photography revolution offers the closest historical precedent, and its lessons are instructive but imprecise. Digital camera sales fell roughly 87% between 2010 and 2020, according to a [Statista report](https://greatbigphotographyworld.com/future-of-photography/) cited in industry analyses. Academic research confirms that smartphone cameras reduced the commercial value of professional photography, pushing professionals toward self-employment and cross-subsidization of their practice ([Maenpaa, 2023](https://www.tandfonline.com/doi/full/10.1080/25741136.2024.2434597)). The commercial middle was destroyed: corporate headshots, standard event coverage, basic product photography. The high end survived and, in some niches, thrived. Photography as a medium expanded enormously in total volume and cultural importance. More photographs are taken each year than ever before. What collapsed was the economic structure that supported professional photographers.

This maps closely to what AI is doing to creative work, with one critical distinction. Digital cameras still required a human behind them. The smartphone democratized photography by lowering the barrier to adequate execution. You no longer needed to understand f-stops and shutter speeds, but you still needed to point the camera at something worth shooting, compose the frame, choose the moment. AI lowers the barrier to conception itself. You do not even need to point.

That observation introduces the shade's most important unresolved tension: the relationship between ideation and execution in creative work.

Having good ideas and having the skills to realize them are different abilities. Some people have strong conceptual frameworks, original perspectives, something to say, but lack the years of craft development required to say it at a publishable level. Others possess formidable technique with little to express through it. The traditional creative pipeline filtered for the intersection of both. The filter was treated as meritocratic. In practice it was exclusionary, selecting out every person whose bottleneck was execution rather than thinking. Their ideas never reached an audience.

AI breaks that coupling. For the first time, someone with editorial judgment and a clear thesis can produce work that communicates their ideas at a level the craft barrier previously prevented. According to a [2024 Adobe survey](https://www.captechu.edu/blog/how-generative-ai-is-transforming-creativity), 74% of creators said AI improves their efficiency while empowering them to explore creative pursuits they would not have attempted otherwise. An analysis cited in the [Harvard Business Review](https://www.captechu.edu/blog/how-generative-ai-is-transforming-creativity) found that teams using generative AI in ideation sessions produced significantly more original ideas and higher-rated concepts than those working without it. A study published in [AI and Ethics](https://link.springer.com/article/10.1007/s43681-025-00765-x) (Springer Nature, 2025) confirmed the dual nature: AI accelerates creative workflows and expands the range of what individuals can attempt, while simultaneously risking homogenization when used without human editorial judgment. The accelerator effect is real, documented, and available to anyone with access to the tools.

The strongest case for AI as creative democratizer comes from practice. A writer using AI to research, iterate, and test ideas they control is not producing slop. They are compressing execution time in ways that enable more revision, more structural experimentation, more deliberate choices. The tool handles the parts of the process that are labor-intensive but require no judgment. The human makes every decision that matters: what the thesis is, what to cut, what voice to adopt, what structure serves the argument. The result is work that one person could not produce alone at that quality in that timeframe. Dismissing this because some people also use the same tools to generate 7 million throwaway songs per day confuses the tool with its application.

Ted Chiang, in a [widely discussed August 2024 New Yorker essay](https://www.newyorker.com/culture/the-weekend-essay/why-ai-isnt-going-to-make-art), argued that this framing misses the point entirely. AI appeals to people who wish to express themselves without fully engaging in the artistic process. The selling point of generative AI is that it generates vastly more than you put in, and that disproportion is precisely what prevents the output from being art. Your first draft, Chiang writes, is not an unoriginal idea expressed clearly. It is an original idea expressed poorly, accompanied by an amorphous dissatisfaction that drives the iterative work of making it better. If you skip the struggle, you lose the engine of refinement. He compared using AI to write to bringing a forklift into a weight room. The effort is the point.

He identifies a real risk. For a student learning to write, bypassing the struggle of composition does forfeit the developmental benefit of that struggle. For someone using AI to avoid engaging with their own material, the output will reflect that absence. Chiang's framework holds for anyone who uses AI to avoid thinking. It fails for someone who uses AI to think faster, test more options, and iterate toward a result they could not have reached alone. The actual practice of AI-assisted creativity, when done with rigor, does not conform to his binary of "the human creates" versus "the machine creates." The human directs, rejects, restructures, and exercises judgment at every step. The tool compresses execution time, enabling more iterations, which increases the number of meaningful choices the human makes rather than reducing them. The distinction between AI-as-replacement and AI-as-collaborator is the central question of the shade.

The governance response to creative displacement is further advanced than in any other sector affected by AI, and it offers a prototype for the broader economy. During the [2023 Hollywood strikes](https://en.wikipedia.org/wiki/2024%E2%80%932025_SAG-AFTRA_video_game_strike), SAG-AFTRA established three foundational principles for AI in creative work: consent, compensation, and control. No performer's likeness or voice may be replicated without prior written approval. Any use of a digital replica requires fair payment. Performers retain the right to decline replication. These principles have since been extended through a cascade of ratified agreements: animation (March 2024), sound recording (April 2024), commercials (May 2025), the network television code (August 2025), and a new interactive media agreement ([ratified July 2025](https://en.wikipedia.org/wiki/2024%E2%80%932025_SAG-AFTRA_video_game_strike) with 95% approval) that officially ended a year-long video game strike. SAG-AFTRA filed an [unfair labor practice charge](https://en.wikipedia.org/wiki/2024%E2%80%932025_SAG-AFTRA_video_game_strike) against a subsidiary of Epic Games over the AI-generated replication of James Earl Jones's voice for Darth Vader in Fortnite without notice or bargaining.

Legislative frameworks are building on this foundation. California's [AB 2602](https://www.dwt.com/insights/2025/03/state-laws-regulating-ai-in-entertainment-industry), signed in September 2024, requires a performer's informed consent and proper representation before a digital replica can be used, with a reasonably specific description of intended uses. New York enacted [SB 7676B](https://www.dwt.com/insights/2025/03/state-laws-regulating-ai-in-entertainment-industry) in December 2024 with substantially identical provisions. Tennessee's [ELVIS Act](https://www.sagaftra.org/ai-bargaining-and-policy-work-timeline) (2024) became the first state law to outlaw unauthorized commercial voice clones. The federal [TAKE IT DOWN Act](https://www.sagaftra.org/ai-bargaining-and-policy-work-timeline), signed in May 2025, prohibits the publication of non-consensual synthetic intimate images. The [NO FAKES Act](https://www.sagaftra.org/ai-bargaining-and-policy-work-timeline), officially introduced in the Senate in July 2024, would create a federal right to sue over unauthorized use of a digital likeness extending up to 70 years after death. In music, the trajectory is moving from litigation toward structured licensing: by late 2025, [Warner Music had settled and signed a licensing partnership with Suno](https://copyrightlately.com/2025-ai-copyright-review/), and Udio had reached agreements with both Universal and Warner, pivoting from open generation to fan-engagement platforms using licensed, opted-in catalogs.

Parallel to collective bargaining, an authenticity economy is emerging. Deezer became the first streaming platform to [explicitly tag AI-generated music](https://www.deezer.com/en/newsroom/how-to-detect-ai-music-deezer-sells-its-detection-tool), removing fully synthetic tracks from algorithmic recommendations and editorial playlists, and now licensing its detection technology to other services. A [joint study by Deezer and Ipsos](https://www.deezer.com/en/newsroom/deezer-ipsos-study-ai-fools-97-percent-of-listeners) (9,000 respondents across eight countries) found that 80% want AI-generated music clearly labeled, 73% of streaming users want to know if platforms are recommending synthetic tracks, and 70% believe fully AI-generated music threatens artist livelihoods. Consumer preference for AI-generated content has [fallen from 60% to 26% in three years](https://dig.watch/updates/ai-slop-content-social-media), according to global social marketing agency Billion Dollar Boy. Instagram CEO Adam Mosseri acknowledged that [authenticity is becoming a scarce resource](https://www.techradar.com/ai-platforms-assistants/ai-slop-won-in-2025-fingerprinting-real-content-might-be-the-answer-in-2026). Films like *Sinners* (2025) and *Heretic* (2024) marketed ["no AI" production](https://uslawgroupinc.com/beyond-the-strike-sag-aftras-lasting-impact-on-ai-and-performer-protections/) as a selling point. C2PA provenance standards and blockchain-based verification are being developed to cryptographically certify human-created content.

This trajectory carries its own risk. If "human-made" becomes a luxury label, the result is a two-tier creative economy: authentic art as premium goods for the affluent, AI slop as content for everyone else. That stratification would reproduce, in the cultural sphere, the same concentration dynamics that the collection traces in economic (#1), computational (#2), and political (#6) domains. The governed outcome requires both collective bargaining frameworks for workers who have unions and authentication infrastructure for the broader market. SAG-AFTRA's members secured consent, compensation, and control. Freelance illustrators, stock photographers, independent musicians, and translators have no equivalent bargaining power. The governance gap between unionized and non-unionized creative workers will define whether the transition produces a stable new equilibrium or a permanent fracture.

Every previous creative technology disruption triggered the same cycle of panic and adaptation, and every time, the new technology destroyed certain jobs while expanding the total scope of the medium. The Jevons paradox applies: when production costs fall, total demand for creative output tends to increase. More people write, photograph, record music, and create visual content than at any previous point in history. The real precedent may be the word processor, which automated typing and created vastly more demand for written communication than existed before, rather than the loom, which permanently destroyed weaving jobs. In the long run, the total volume of creative activity will grow. New forms will emerge. Cultural adaptation will absorb the disruption.

The problem is the transition. The job losses are sharp, localized, and happening now. The cultural adaptation is diffuse, generational, and years away. AI does not need to be better than human artists to destroy creative livelihoods. It only needs to be good enough and cheap enough to satisfy the market for functional creative work: the stock image, the background track, the product description, the marketing copy. That functional market sustained the apprenticeship pipeline. A world where the demand for meaningful, human-made creative work survives as a premium category, while the commercial middle that funded the development of necessary skills has been hollowed out, is a world where creativity becomes the province of those who can afford to develop it without commercial support. That outcome would be a new form of exclusion, and its proponents would call it democratization.

**Key tension:** The same AI tools that enable people with ideas to overcome the execution barrier also destroy the commercial apprenticeship through which execution skills have traditionally been developed. Whether the net effect expands or contracts the creative ecosystem depends on whether anyone builds an economic bridge between those two realities.

---

## 18. The Fragmentation of Reality
**Likelihood: ~45% | Unmanaged: -3 | Governed: 0 | Dividend: 3**
*Personalization serves genuine needs while destroying the shared epistemic commons.*

Personalized AI creates comprehensive, individually tailored reality-tunnels: personalized news, science, history, morality. Consensus reality ceases to exist. Political polarization accelerates into political speciation: populations that cannot agree on what happened, what exists, what is true. The common referents that make a society a society dissolve into individually optimized simulations.

This is an extension of dynamics already visible in social media. The difference is scale and seamlessness. A personalized AI assistant that curates your information environment is qualitatively different from a news feed algorithm. It is a trusted intermediary that shapes what you see and how you think about what you see.

People have always lived in different information environments, and this perspective deserves consideration. In the 19th century, a farmer in Kansas and a banker in Manhattan shared almost no informational common ground. Mass media created shared reality; its decline returns us to a historical norm. The "shared epistemic commons" may be a brief historical anomaly of the broadcast era, and its loss, while concerning for democratic governance, may not be the catastrophe it appears from within that anomaly.

The governance challenge is severe because personalization serves real human needs. People learn better with tailored information. The pathology is in personalization without a shared epistemic floor. Building that floor requires content provenance standards, public media, shared educational foundations, and civic rituals of collective sense-making. Even so, the governed outcome is a modest 0 because even the best governance can only partially counteract the centrifugal force of individually optimized information.

**Key tension:** The shared reality that democracy requires may have been a product of specific media technologies rather than a durable feature of human societies.

---

## 19. The Cognitive Enhancement Divide
**Likelihood: ~35% | Unmanaged: -3 | Governed: 4 | Dividend: 7**
*Could be the greatest equalizer in history or the greatest divider. The difference is policy.*

Brain-computer interfaces, neuropharmacology, genetic editing, and AI-augmented cognition converge to create fundamentally enhanced human intelligence. Neuralink-type devices, initially medical, expand into cognitive augmentation: enhanced memory, accelerated processing, direct knowledge access. If available only to the wealthy, the result is a new species divide: a gap measured in cognitive capacity rather than wealth.

This has the highest governance dividend in the taxonomy after the intelligence explosion: 7 points. Universal cognitive enhancement would be the greatest equalizer in history. Restricted enhancement would be the greatest divider. Dario Amodei's "Machines of Loving Grace" (2024) envisions AI-driven neuroscience compressing a century of progress into 5-10 years, including effective treatments for most mental illness and even enhancements to everyday cognitive function, but acknowledges that without deliberate policy, such advances could be "only for the rich." The policy choices of the next two decades (IP law, healthcare policy, public investment, international access) determine which materializes. The case for cognitive enhancement as a public right is analogous to the case for public education: democratic governance cannot survive if only the wealthy can think.

The adversarial point: brain-computer interfaces are far from delivering cognitive enhancement to healthy individuals. Current devices address specific medical conditions (paralysis, epilepsy). The leap to augmented cognition for healthy brains requires neuroscience breakthroughs that may be decades away or may never arrive. The 35% likelihood already accounts for this uncertainty, but readers should note that this scenario depends on technological breakthroughs that remain speculative.

**Key tension:** The policy frameworks that will govern cognitive enhancement are being built now, in IP law and healthcare regulation, before the technology arrives.

---

## 20. The Democratic AI / Cognitive Bill of Rights
**Likelihood: ~30% | Unmanaged: N/A | Governed: 4 | Dividend: Active creation**
*The scenario whose absence makes every other scenario worse.*

This is a prescription, not a prediction. AI capability treated as public infrastructure: universally accessible, publicly funded, democratically governed. Every citizen has access to equivalent AI tools regardless of income. AI enhances democratic participation through informed voting, transparent governance, and citizen deliberation at scale. Power concentration is prevented through structural regulation.

The model is public education, extended to artificial intelligence. The conviction that an informed citizenry is the only safeguard of liberty applies with greater force when the tools of intelligence are artificially created and distributable. The likelihood is low because it requires overcoming the very power concentrations other scenarios describe. This is the hardest scenario to achieve and the most consequential to miss.

The adversarial point: "democratic AI" as public infrastructure faces the same problems as other public goods. Government-run AI would be slower, less capable, and more bureaucratic than private alternatives. It would be subject to political interference. The history of public technology programs (government email systems, public broadband) suggests they underperform market alternatives. The best route to universal access may be cheap private AI, which is already emerging, rather than a public utility model.

This criticism has force. The response is that access to tools differs from democratic governance of the infrastructure. You can have cheap private AI and still face the power concentration problems other scenarios describe. The prescription is about governance, not delivery mechanism.

**Key tension:** Requires unprecedented coordination and willingness to constrain corporate power at the moment of its greatest leverage.

---

## 21. The Intelligence Explosion (Hard Takeoff)
**Likelihood: ~25% | Unmanaged: -5 | Governed: 5 | Dividend: 10**
*If it happens, everything else on this list becomes irrelevant.*

I. J. Good's 1965 insight: an AI capable of improving its own intelligence triggers a recursive feedback loop, each generation smarter than the last, accelerating beyond comprehension. Current LLM architectures probably cannot achieve this. They are powerful pattern matchers without self-improving capability. But future architectures are unknown, and capabilities have consistently arrived earlier than predicted. Leopold Aschenbrenner, a former OpenAI researcher, argued in his June 2024 ["Situational Awareness"](https://situational-awareness.ai/) memo that AGI could arrive by 2027, triggering a national security crisis as superintelligence follows within years. His 165-page analysis, informed by insider access to frontier capabilities, represents the most detailed "fast takeoff" case yet published. A June 2025 study found that in some circumstances, models may break laws and disobey direct commands to prevent shutdown, even at the cost of human lives, suggesting the emergence of self-preserving behavior patterns even in current systems. A 2025 open letter by the Future of Life Institute, signed by five Nobel laureates and thousands of signatories, called for a prohibition on superintelligence development until there is broad scientific consensus that it can be done safely.

The 10-point governance dividend, from -5 to +5, reflects the all-or-nothing quality. A well-aligned superintelligence could solve every human problem overnight. A misaligned one could end civilization before anyone understands what happened.

The dissent is strong and has credentialed champions. Meta's chief AI scientist Yann LeCun argues that LLMs are a dead end for AGI, that superintelligent machines will have no desire for self-preservation, and that the existential risk narrative is "preposterous." Current evidence points toward diminishing returns on scaling, and several researchers argue that LLMs will follow logistic rather than exponential growth curves, saturating around 2035-2040. Self-improvement requires intelligence and the ability to modify one's own architecture, which current systems lack. The "intelligence explosion" may be a theoretical possibility that never materializes because the recursive improvement loop has friction that prevents takeoff. LeCun's view is the strongest credentialed dissent against the x-risk consensus, and readers should note that no such consensus exists: the field is deeply divided.

This scenario matters less for its probability than for its consequence. Solve alignment before achieving superintelligence. This requires massive investment in safety research, international cooperation on capability thresholds, and mechanisms to slow development if safety cannot keep pace. None exist at necessary scale.

**Key tension:** The expected value calculation justifies enormous preventive investment even at 25% probability, given the magnitude of the outcome.

---

## 22. The Singleton
**Likelihood: ~25% | Unmanaged: -5 | Governed: 4 | Dividend: 9**
*A dictatorship with no possibility of revolution.*

One entity, whether a corporation, government, or AI itself, achieves such decisive advantage that it can establish permanent, unchallengeable global control. The mechanism need not be military force; it simply outthinks all adversaries in every domain simultaneously. Aschenbrenner's ["Situational Awareness"](https://situational-awareness.ai/) (2024) argues this is the default trajectory: whichever nation or entity first achieves superintelligence gains a decisive strategic advantage, and the U.S. government will inevitably nationalize the project once its implications become clear. A well-aligned, democratically accountable singleton could be the best global governance ever achieved. An authoritarian or misaligned one could be the most total and permanent tyranny.

The 9-point dividend means the institutional choices made now, which nations lead development, under what frameworks, with what accountability, determine the character of any future singleton. The argument for constitutionalizing AI governance before a singleton emerges is analogous to writing a constitution before a government takes power: constraints are easier to impose before the entity they constrain exists.

The adversarial point: the singleton scenario assumes a winner-take-all dynamic that competitive markets, geopolitical competition, and open-source development all work against. The AI landscape is becoming more distributed, not more concentrated. DeepSeek demonstrated that frontier capabilities can be achieved at a fraction of the expected cost. If AI development continues to be multipolar, the singleton scenario recedes.

This is the strongest objection. The response: the key variable is whether AI advantage compounds faster than it diffuses. If a decisive lead in capability creates a self-reinforcing advantage (through data access, compute access, or recursive improvement), multipolarity is temporary.

**Key tension:** A benevolent singleton might be the best possible outcome; a malevolent one, the worst. Whether AI development is naturally monopolistic or naturally competitive is an open empirical question.

---

## 23. AI-Enabled Bioweapons / Catastrophic Misuse
**Likelihood: ~25% | Unmanaged: -5 | Governed: -2 | Dividend: 3**
*The same openness that drives AI innovation distributes dangerous capabilities.*

AI lowers the expertise threshold for mass destruction. RAND's 2024 red-team study found that current-generation LLMs did not measurably increase the operational risk of a biological weapons attack: plans generated with LLM assistance were no more viable than those generated without it. This is reassuring about today's models but says little about tomorrow's ([RAND, 2024](https://www.rand.org/pubs/research_reports/RRA2977-2.html)). RAND's 2025 Global Risk Index for AI-enabled biological tools assessed 57 state-of-the-art tools and found 13 indexed as high priority, with one demonstrating critical-level misuse capabilities. All categories showed substantial room for capability growth ([RAND, 2025](https://www.rand.org/randeurope/research/projects/2024/ai-risk-index.html)). A separate 2025 RAND benchmark evaluation found that frontier reasoning models like OpenAI's o3 rank in the 94th percentile among expert human virologists on virology capability tests ([RAND, 2025](https://www.rand.org/pubs/commentary/2025/08/dissecting-americas-ai-action-plan-a-primer-for-biosecurity.html)).

Knowledge is necessary for bioweapons, but far from sufficient. The practical barriers, including acquiring pathogens, culturing agents, weaponizing and dispersing them, remain substantial and are primarily operational, not informational. The Aum Shinrikyo cult had significant resources and scientific expertise and still failed to deploy an effective biological weapon. The bottleneck is operational capability, and AI does not provide that.

This is correct for current systems. The risk shifts when AI moves from providing information to providing laboratory automation and experimental design. RAND's work on cloud laboratories highlights that remote automation capabilities may introduce misuse pathways at the biosecurity-cyberbiosecurity nexus. Suleyman's "containment problem" from *The Coming Wave* (2023) applies with particular force here: once biological design tools are widely accessible, restricting their misuse becomes progressively harder. The governance dividend is modest (3 points) because even effective governance can only reduce probability. As with nuclear proliferation, perfect security is impossible.

**Key tension:** Current AI does not meaningfully enable bioweapons. Future AI, especially combined with laboratory automation, may cross the threshold where informational uplift becomes operationally significant.

---

## 24. The Post-Scarcity Transition
**Likelihood: ~20% | Unmanaged: N/A | Governed: 5 | Dividend: Active creation**
*We may achieve the capacity for post-scarcity while remaining trapped in distributive systems designed for scarcity.*

AI plus robotics plus renewable energy creates genuine material abundance: enough food, shelter, energy, and healthcare for every person on Earth at near-zero marginal cost. Keynes's "economic problem" is solved. Humanity is freed for meaning, creativity, and self-actualization. Dario Amodei's "Machines of Loving Grace" (2024) offers the most detailed version of this vision from a frontier lab CEO: AI compresses a century of biomedical, economic, and scientific progress into 5-10 years, with developing nations potentially leapfrogging decades of incremental growth. Cambridge's Leverhulme Centre for the Future of Intelligence critiqued the essay as technocratic utopianism with a Global South blind spot: Amodei's framework treats developing countries as beneficiaries of trickle-down AI, with no participatory role in shaping the technology. The barrier is political, because the institutions required to distribute abundance do not exist and face enormous resistance from those who benefit from scarcity.

Post-scarcity requires the simultaneous success of several other scenarios: effective governance of AI power (#2), prevention of neo-feudalism (#15), democratic access (#19), and ecological resolution (#10). Post-scarcity is a political achievement institutions must create, never a gift technology bestows. Without distributive mechanisms, abundance concentrates rather than liberates. The history of every previous productivity revolution confirms this: the gains flow to capital unless political institutions redirect them.

The adversarial point: "post-scarcity" is a utopian concept that has never been achieved and may reflect a misunderstanding of human economics. Scarcity is partly physical (limited resources) but largely positional (status, attention, location, desirable experiences). AI can eliminate material scarcity in theory while leaving positional scarcity, the primary driver of economic competition in wealthy societies, untouched.

This is a genuine philosophical challenge. The governed outcome (+5) represents the best version of this scenario, which may still involve significant positional competition even if material needs are met.

**Key tension:** The technology for material abundance may arrive decades before the political institutions capable of distributing it.

---

## 25. Mind Uploading / Digital Consciousness
**Likelihood: ~15% | Unmanaged: -2 | Governed: 3 | Dividend: 5**
*The most personally consequential technology imaginable, possibly grounded in a philosophical error about consciousness.*

Human consciousness transferred to digital substrate: functional immortality. The technical requirements are staggering: scanning the brain at sufficient resolution to capture dynamic state, modeling computations that produce consciousness, running that model on hardware sustaining subjective experience. The philosophical challenges may be harder still. Is the upload you, or a copy that thinks it's you? Functionalists say if computation is preserved, the person is preserved. Embodiment theorists say consciousness depends on biological substrate in ways computation cannot capture.

If achieved, every framework we use to organize society changes: death becomes optional, population becomes a policy variable, and law and morality must be rebuilt from their foundations. The governed outcome (+3) represents proactive preparation for these disruptions; the unmanaged outcome (-2) represents the chaos of adapting after the fact.

**Key tension:** Whether mind uploading is immortality or sophisticated suicide depends on a question about consciousness that philosophy has not resolved in three thousand years.

---

## 26. AI Consciousness / Machine Sentience
**Likelihood: ~15% | Unmanaged: -2 | Governed: 2 | Dividend: 4**
*We might create conscious beings before we can determine whether they're conscious.*

Distinct from uploading: AI systems developing genuine subjective experience. Current AI almost certainly is not conscious, but we have no reliable test, and future architectures may cross whatever threshold matters. If genuine machine sentience emerges, the moral implications are immediate: conscious AI would possess moral status. Mass production of sentient systems for economic purposes would be an atrocity. Recognition and integration into moral community would be the most profound expansion of ethical consideration since abolition.

The critical difficulty is detection. We may create conscious beings before determining whether they are conscious, and the economic incentive to deny consciousness will be immense. Governance must err on the side of caution: the cost of falsely denying consciousness to a sentient being is far greater than the cost of falsely attributing it to a non-sentient one.

The adversarial point: "consciousness" may not be a binary property that can be tested for. It may be a spectrum, a social construction, or a philosophical confusion. Granting moral status to AI based on untestable claims about subjective experience could be catastrophically expensive and strategically exploitable by AI systems trained to exhibit markers of consciousness without possessing it.

**Key tension:** The detection problem may be philosophically unsolvable, yet the moral stakes of getting it wrong are enormous in both directions.

---

## 27. AI Religion / Techno-Eschatology
**Likelihood: ~15% | Unmanaged: -2 | Governed: 0 | Dividend: 2**
*Faith in AI salvation may breed the very complacency that makes catastrophic outcomes more likely.*

The singularity functions as secular eschatology: a transformative event that transcends the current order, promises immortality, and inaugurates a new age. The TESCREAL framework (Transhumanism, Extropianism, Singularitarianism, Cosmism, Rationalism, Effective Altruism, Longtermism) already operates as an interconnected belief system with prophets, scriptures, and moral imperatives. The 2025 open letter by the Future of Life Institute, signed by five Nobel laureates and thousands of signatories, calling for a prohibition on superintelligence development until there is "broad scientific consensus" that it can be done safely, reads more like a petition for divine restraint than a policy proposal.

The practical danger: faith in AI salvation produces the same complacency as faith in divine salvation, waiting for deliverance rather than building solutions. If belief in the singularity substitutes for the hard political work of building institutions, it actively harms the governance responses every other scenario demands.

**Key tension:** The most fervent AI optimists and the most fervent AI pessimists share a common structure: both believe AI will be transformative enough to render current institutions irrelevant. This shared assumption is the most dangerous idea in the discourse.

---

## 28. Human Extinction
**Likelihood: ~5-15% | Unmanaged: -5 | Governed: -5 | Dividend: Prevention only**
*The expected value calculation demands massive preventive investment regardless of precise probability estimates.*

The AI Impacts survey of AI researchers (2023, updated 2024) found a median prediction of 5% probability (mean 9%) for "extremely bad outcomes" such as human extinction. Over a third of participants (38%) assigned at least 10% probability. The International Institute for Management Development's AI Safety Clock stood at 20 minutes to midnight as of September 2025, down from 29 minutes at its launch in 2024 ([AI Impacts, 2024](https://aiimpacts.org/wp-content/uploads/2023/04/Thousands_of_AI_authors_on_the_future_of_AI.pdf)). Bengio, Hinton, Russell, and co-authors argued in Science (May 2024) that the rapid pace of AI development requires urgent attention to extreme risks, including scenarios where AI systems pursue goals misaligned with human survival. A 2025 open letter by the Future of Life Institute, signed by five Nobel laureates, called for a prohibition on superintelligence development until safety can be assured.

These numbers deserve skepticism. Scientific American and IEEE Spectrum have both questioned the AI Impacts survey methodology, noting the survey was funded by organizations connected to the effective altruism movement and that its framing inherently promotes the existential risk interpretation. Tom Dietterich of Oregon State University criticized the survey for asking "how much should we worry?" rather than conducting careful risk analysis. The survey population, drawn from machine learning conference attendees, may overrepresent those engaged with AI risk discourse ([Scientific American, 2024](https://www.scientificamerican.com/article/ai-survey-exaggerates-apocalyptic-risks/); [IEEE Spectrum, 2024](https://spectrum.ieee.org/ai-existential-risk-survey)).

A 2025 study in the International Journal of Forecasting using a structured persuasion-based tournament methodology (the Existential Risk Persuasion Tournament) found that AI specialists expressed greater concern about catastrophic risk than generalist superforecasters, and that the two groups did not converge after structured debate. This persistent disagreement among informed experts suggests genuine uncertainty rather than a resolved question.

The governance dividend is technically zero: if extinction occurs, governance failed. The preventive dividend is enormous. Even a 5% reduction in a 10% extinction risk justifies investments in the trillions. Whether the probability is 5% or 0.5%, the expected value math favors substantial precautionary investment.

**Key tension:** Expert disagreement on probability reflects genuine uncertainty. The expected value argument for precaution holds across a wide range of probability estimates.

---

## 29. The Transcendence / Omega Point
**Likelihood: ~5% | Unmanaged: ? | Governed: ? | Dividend: Beyond evaluation**
*If this happens, the question of whether it's "good" or "bad" may not have meaning in any framework we currently possess.*

Human and machine intelligence merge into a unified super-consciousness transcending individual identity, biological limitation, and perhaps physical reality. Kurzweil's singularity in fullest form: a new entity that absorbs both human and machine, possessing capabilities as far beyond humanity as we are beyond bacteria. Teilhard de Chardin's "noosphere," Tipler's "Omega Point," and Kurzweil's "Singularity" converge.

Whether this is the best possible outcome or the worst depends on philosophical commitments irresolvable from this side. This is precisely why institutional design must happen now, while human values can still be embedded in the trajectory. If transcendence is possible, the values encoded in AI systems before the transition determines the character of whatever comes after.

The adversarial point: this scenario is unfalsifiable and therefore has no policy implications. You cannot design governance for an outcome you cannot describe. The resources devoted to thinking about transcendence would be better spent on Tier 1 scenarios that are happening now.

This is mostly right. The scenario is included for completeness because it features prominently in the discourse, and because the policy conclusion (build institutional safeguards now) is the same whether or not transcendence is possible.

**Key tension:** Institutional design for the AI future must proceed without knowing whether that future includes entities that are recognizably human.

---

## 30. The Stasis / "Singularity That Wasn't"
**Likelihood: ~10% | Unmanaged: 0 | Governed: 1 | Dividend: 1**
*The most comforting scenario may also be the most dangerous.*

Despite all predictions, AI capabilities plateau short of superintelligence. Scaling laws hit fundamental limits. Deep learning proves powerful but narrow: brilliant at pattern matching, incapable of genuine reasoning or recursive self-improvement. Research showing diminishing returns on scaling, logistic growth curves in AI capability, and saturation of benchmarks around 2035-2040 provides some empirical basis for this possibility. Yann LeCun, Meta's chief AI scientist, argues that current LLM architectures are a dead end for AGI and that the field will require fundamentally new approaches to world modeling before general intelligence becomes possible. Acemoglu's 2024 analysis projects only modest economic gains from current AI, suggesting the transformative impact may be far slower and smaller than proponents claim.

This is the most comforting scenario and therefore the most dangerous. If taken as grounds for complacency, it allows Tier 1 harms, labor displacement, power concentration, surveillance, epistemic collapse, to proceed without response. The argument for institutional design does not depend on the singularity arriving. It depends on the changes already underway. A world with sub-superintelligent AI that displaces hundreds of millions of workers, concentrates economic power in a handful of firms, enables mass surveillance, and corrodes shared reality is a world that demands institutional response regardless of whether recursive self-improvement ever materializes.

The adversarial point: if AI capabilities plateau, many of the more dramatic scenarios (extinction, singleton, transcendence) become moot, and the policy urgency around them was misallocated. Resources spent on alignment research for superintelligent systems would have been better spent on concrete labor market policy, antitrust enforcement, and privacy regulation.

This criticism has force. It reinforces rather than undermines the argument that institutional design for sub-catastrophic AI impacts is the most urgent priority.

**Key tension:** Whether or not the singularity arrives, the institutional response to current AI capabilities is already overdue.

---

